{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf1cc2f8",
   "metadata": {},
   "source": [
    "# Chapter 4: Writing Your First Complete Python Script\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will confidently:\n",
    "- **Understand Python script architecture** and best practices for VLSI automation\n",
    "- **Create professional-grade scripts** with proper structure, documentation, and error handling\n",
    "- **Build complete VLSI tools** from concept to deployment-ready automation\n",
    "- **Master file operations** for reading reports, logs, and configuration files\n",
    "- **Implement robust error handling** to create production-quality automation\n",
    "- **Apply object-oriented principles** for scalable VLSI design flows\n",
    "\n",
    "## üöÄ Why Scripts Matter in VLSI\n",
    "\n",
    "In the previous chapters, you learned Python fundamentals. Now it's time to apply that knowledge to create **complete automation solutions** that VLSI engineers use daily:\n",
    "\n",
    "### üìä **Real VLSI Automation Needs:**\n",
    "- **Daily Tasks**: Parsing 100+ timing reports from multiple PVT corners\n",
    "- **Design Reviews**: Generating power/performance comparison charts across design iterations  \n",
    "- **Sign-off Flows**: Automating DRC/LVS/timing closure with custom checks\n",
    "- **Regression Testing**: Running overnight synthesis sweeps with result analysis\n",
    "- **Report Generation**: Creating executive summaries from raw EDA tool outputs\n",
    "\n",
    "### ‚ö° **The Script Advantage:**\n",
    "- **Reusability**: Write once, use across all projects and technologies\n",
    "- **Consistency**: Eliminate human errors in repetitive calculations\n",
    "- **Speed**: Process thousands of files in minutes instead of hours\n",
    "- **Documentation**: Self-documenting workflows that new team members can understand\n",
    "- **Integration**: Connect different EDA tools in unified flows\n",
    "\n",
    "### üîÑ **Script vs Interactive Python:**\n",
    "\n",
    "| Aspect | Interactive Python | Python Scripts | VLSI Use Case |\n",
    "|--------|-------------------|----------------|---------------|\n",
    "| **Purpose** | Learning & testing | Production automation | Full design flows |\n",
    "| **Persistence** | Lost after session | Permanent & versioned | Design methodology |\n",
    "| **Complexity** | Simple calculations | Complete applications | Multi-step processes |\n",
    "| **Collaboration** | Individual use | Team sharing | Design team tools |\n",
    "| **Documentation** | Minimal | Comprehensive | Process documentation |\n",
    "\n",
    "## üí° **From Concept to Professional Tool:**\n",
    "A professional VLSI script isn't just code that works‚Äîit's a **maintainable, documented, and robust solution** that becomes part of your design methodology. In this chapter, we'll build real tools that you can immediately use in your VLSI work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcc5a9d",
   "metadata": {},
   "source": [
    "## üìú Anatomy of a Professional Python Script\n",
    "\n",
    "Understanding script structure is crucial for creating maintainable VLSI automation. A well-structured script follows industry standards and makes collaboration possible.\n",
    "\n",
    "### üèóÔ∏è **Professional Script Architecture:**\n",
    "\n",
    "```\n",
    "üìÅ vlsi_script.py\n",
    "‚îú‚îÄ‚îÄ üîù Shebang Line       (#!/usr/bin/env python3)\n",
    "‚îú‚îÄ‚îÄ üìñ Module Docstring   (What this script does)\n",
    "‚îú‚îÄ‚îÄ üì¶ Imports           (External libraries)\n",
    "‚îú‚îÄ‚îÄ üîß Constants         (Configuration values)\n",
    "‚îú‚îÄ‚îÄ üéØ Functions         (Reusable code blocks)\n",
    "‚îú‚îÄ‚îÄ üèõÔ∏è Classes           (Complex data structures)\n",
    "‚îú‚îÄ‚îÄ üöÄ Main Execution    (Script entry point)\n",
    "‚îî‚îÄ‚îÄ üõ°Ô∏è Error Handling   (Robust failure management)\n",
    "```\n",
    "\n",
    "### üÜö **Script Structure: Python vs TCL vs Perl**\n",
    "\n",
    "**Python** (Clean, readable, standardized):\n",
    "```python\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"Professional timing analysis script\"\"\"\n",
    "import timing_lib\n",
    "SETUP_TIME = 0.1\n",
    "def analyze_path(path): return path.delay + SETUP_TIME\n",
    "if __name__ == \"__main__\": analyze_path(critical_path)\n",
    "```\n",
    "\n",
    "**TCL** (Tool-specific, less structured):\n",
    "```tcl\n",
    "#!/bin/tcsh\n",
    "# Timing analysis - requires specific EDA tool context\n",
    "set setup_time 0.1\n",
    "proc analyze_path {path} { return [expr $path + $setup_time] }\n",
    "analyze_path $critical_path\n",
    "```\n",
    "\n",
    "**Perl** (Cryptic syntax, harder maintenance):\n",
    "```perl\n",
    "#!/usr/bin/perl\n",
    "# Timing analysis\n",
    "my $setup = 0.1;\n",
    "sub analyze { my $path = shift; return $path + $setup; }\n",
    "print analyze($critical_path);\n",
    "```\n",
    "\n",
    "### üéØ **Why Python's Structure Wins:**\n",
    "- **Readability**: Any engineer can understand the logic\n",
    "- **Modularity**: Easy to test individual functions\n",
    "- **Documentation**: Built-in docstring support\n",
    "- **Error Handling**: Comprehensive exception system\n",
    "- **Debugging**: Clear stack traces and debugging tools\n",
    "- **Collaboration**: Standard structure across all Python scripts\n",
    "\n",
    "A script is a `.py` file containing Python code that performs specific tasks. For VLSI engineers, scripts automate:\n",
    "- **Report Parsing**: Extract data from timing, power, area reports\n",
    "- **Data Analysis**: Statistical analysis of PVT corner results\n",
    "- **Tool Automation**: Orchestrate multi-tool flows\n",
    "- **Result Generation**: Create presentation-ready charts and summaries\n",
    "- **Quality Assurance**: Automated checking of design rules and constraints\n",
    "\n",
    "### üîç **Script vs Module vs Package:**\n",
    "- **Script**: Executable `.py` file for specific tasks (timing_analyzer.py)\n",
    "- **Module**: Library `.py` file for reusable functions (vlsi_utils.py)\n",
    "- **Package**: Directory with multiple modules (vlsi_toolkit/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df571b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROFESSIONAL PYTHON SCRIPT STRUCTURE DEMONSTRATION\n",
    "# ===================================================\n",
    "# This example shows the complete anatomy of a production-ready VLSI script\n",
    "\n",
    "print(\"üìú PROFESSIONAL PYTHON SCRIPT STRUCTURE\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. SHEBANG LINE (First line of any executable script)\n",
    "# =============================================================================\n",
    "print(\"üîù 1. SHEBANG LINE:\")\n",
    "print(\"   #!/usr/bin/env python3\")\n",
    "print(\"   Purpose: Tells system which interpreter to use\")\n",
    "print(\"   Linux/Mac: Makes script directly executable (./script.py)\")\n",
    "print(\"   Windows: Helps Python launcher choose correct version\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. MODULE DOCSTRING (Comprehensive documentation)\n",
    "# =============================================================================\n",
    "print(f\"\\nüìñ 2. MODULE DOCSTRING:\")\n",
    "example_docstring = '''\n",
    "\"\"\"\n",
    "VLSI Timing Analysis Automation Script\n",
    "======================================\n",
    "\n",
    "Purpose:\n",
    "    Automates timing analysis across multiple PVT corners,\n",
    "    generates violation reports, and recommends fixes.\n",
    "\n",
    "Author: VLSI Design Team\n",
    "Version: 2.1.0\n",
    "Date: 2025-09-06\n",
    "\n",
    "Dependencies:\n",
    "    - Python 3.8+\n",
    "    - numpy, pandas (pip install numpy pandas)\n",
    "    - Access to timing report files\n",
    "\n",
    "Usage:\n",
    "    python timing_analyzer.py --corner SS --reports ./timing_reports/\n",
    "\n",
    "Example:\n",
    "    # Analyze all corners\n",
    "    python timing_analyzer.py --all-corners\n",
    "\n",
    "    # Specific corner analysis\n",
    "    python timing_analyzer.py --corner FF --threshold -0.1\n",
    "\n",
    "Input Files:\n",
    "    - Timing reports (.rpt format)\n",
    "    - Configuration file (config.yaml)\n",
    "    - Technology constraints (tech.lib)\n",
    "\n",
    "Output Files:\n",
    "    - Summary report (timing_summary.html)\n",
    "    - Violation details (violations.csv)\n",
    "    - Recommended fixes (optimization_guide.txt)\n",
    "\n",
    "Returns:\n",
    "    Exit code 0: Analysis successful, timing clean\n",
    "    Exit code 1: Analysis failed due to file errors\n",
    "    Exit code 2: Timing violations found\n",
    "\"\"\"\n",
    "'''\n",
    "print(\"   Comprehensive documentation including:\")\n",
    "print(\"   ‚úì Purpose and functionality\")\n",
    "print(\"   ‚úì Author and version information\")\n",
    "print(\"   ‚úì Dependencies and requirements\")\n",
    "print(\"   ‚úì Usage examples and command-line syntax\")\n",
    "print(\"   ‚úì Input/output file descriptions\")\n",
    "print(\"   ‚úì Return codes and error conditions\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3. IMPORTS (Organized by category)\n",
    "# =============================================================================\n",
    "print(f\"\\nüì¶ 3. IMPORTS (Organized by category):\")\n",
    "\n",
    "print(\"   # Standard library imports (built into Python)\")\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"   import os, sys, argparse, datetime, pathlib\")\n",
    "\n",
    "print(\"   # Third-party imports (installed via pip)\")\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(\"   import numpy as np  ‚úì\")\n",
    "except ImportError:\n",
    "    print(\"   import numpy as np  ‚ùå (pip install numpy)\")\n",
    "\n",
    "print(\"   # Local imports (your own modules)\")\n",
    "print(\"   from vlsi_utils import timing_parser, report_generator\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. CONSTANTS AND CONFIGURATION\n",
    "# =============================================================================\n",
    "print(f\"\\nüîß 4. CONSTANTS AND CONFIGURATION:\")\n",
    "\n",
    "# Technology-specific constants\n",
    "TECHNOLOGY_NODE = \"7nm\"\n",
    "NOMINAL_VOLTAGE = 0.8  # Volts\n",
    "OPERATING_TEMPERATURE = 85  # Celsius\n",
    "\n",
    "# Timing constraints\n",
    "DEFAULT_SETUP_TIME = 0.05  # nanoseconds\n",
    "DEFAULT_HOLD_TIME = 0.02   # nanoseconds\n",
    "CLOCK_UNCERTAINTY = 0.1    # nanoseconds\n",
    "\n",
    "# File and path constants\n",
    "REPORT_EXTENSION = \".rpt\"\n",
    "OUTPUT_DIR = \"./analysis_results\"\n",
    "CONFIG_FILE = \"timing_config.yaml\"\n",
    "\n",
    "# Analysis thresholds\n",
    "CRITICAL_SLACK_THRESHOLD = -0.1  # nanoseconds\n",
    "WARNING_SLACK_THRESHOLD = 0.05   # nanoseconds\n",
    "\n",
    "print(f\"   Technology: {TECHNOLOGY_NODE}\")\n",
    "print(f\"   Voltage: {NOMINAL_VOLTAGE} V\")\n",
    "print(f\"   Temperature: {OPERATING_TEMPERATURE}¬∞C\")\n",
    "print(f\"   Setup time: {DEFAULT_SETUP_TIME} ns\")\n",
    "print(f\"   Critical threshold: {CRITICAL_SLACK_THRESHOLD} ns\")\n",
    "\n",
    "# =============================================================================\n",
    "# 5. UTILITY FUNCTIONS (Building blocks)\n",
    "# =============================================================================\n",
    "print(f\"\\nüéØ 5. UTILITY FUNCTIONS:\")\n",
    "\n",
    "def validate_timing_report(file_path):\n",
    "    \"\"\"Validate timing report file format and accessibility\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Timing report not found: {file_path}\")\n",
    "\n",
    "    if not file_path.endswith(REPORT_EXTENSION):\n",
    "        raise ValueError(f\"Invalid file extension. Expected {REPORT_EXTENSION}\")\n",
    "\n",
    "    # Check file is readable and not empty\n",
    "    if os.path.getsize(file_path) == 0:\n",
    "        raise ValueError(f\"Empty timing report: {file_path}\")\n",
    "\n",
    "    return True\n",
    "\n",
    "def calculate_timing_margin(slack, threshold=CRITICAL_SLACK_THRESHOLD):\n",
    "    \"\"\"Calculate timing margin and severity level\"\"\"\n",
    "    if slack >= WARNING_SLACK_THRESHOLD:\n",
    "        return \"SAFE\", slack - WARNING_SLACK_THRESHOLD\n",
    "    elif slack >= threshold:\n",
    "        return \"WARNING\", slack - threshold\n",
    "    else:\n",
    "        return \"CRITICAL\", slack - threshold\n",
    "\n",
    "def format_timing_value(value_ns):\n",
    "    \"\"\"Format timing values with appropriate units\"\"\"\n",
    "    if abs(value_ns) >= 1000:\n",
    "        return f\"{value_ns/1000:.3f} Œºs\"\n",
    "    elif abs(value_ns) >= 1:\n",
    "        return f\"{value_ns:.3f} ns\"\n",
    "    else:\n",
    "        return f\"{value_ns*1000:.1f} ps\"\n",
    "\n",
    "print(\"   ‚úì validate_timing_report() - File validation\")\n",
    "print(\"   ‚úì calculate_timing_margin() - Slack analysis\")\n",
    "print(\"   ‚úì format_timing_value() - Unit formatting\")\n",
    "\n",
    "# =============================================================================\n",
    "# 6. MAIN EXECUTION BLOCK\n",
    "# =============================================================================\n",
    "print(f\"\\nüöÄ 6. MAIN EXECUTION BLOCK:\")\n",
    "\n",
    "print(\"   if __name__ == '__main__':\")\n",
    "print(\"       # Script entry point - only runs when script is executed directly\")\n",
    "print(\"       # Not when imported as a module\")\n",
    "print(\"       main()  # Call main function\")\n",
    "\n",
    "print(f\"\\nüí° Complete Structure Benefits:\")\n",
    "print(\"‚úÖ **Maintainable**: Clear organization makes updates easy\")\n",
    "print(\"‚úÖ **Testable**: Each function can be tested independently\")\n",
    "print(\"‚úÖ **Reusable**: Functions can be imported by other scripts\")\n",
    "print(\"‚úÖ **Professional**: Follows Python community standards\")\n",
    "print(\"‚úÖ **Debuggable**: Clear structure helps identify issues\")\n",
    "print(\"‚úÖ **Collaborative**: Team members can easily understand and modify\")\n",
    "\n",
    "# Demonstrate the structure in action\n",
    "test_slack = -0.15\n",
    "severity, margin = calculate_timing_margin(test_slack)\n",
    "formatted_slack = format_timing_value(test_slack)\n",
    "\n",
    "print(f\"\\nüß™ Structure in Action:\")\n",
    "print(f\"   Input slack: {formatted_slack}\")\n",
    "print(f\"   Severity: {severity}\")\n",
    "print(f\"   Margin: {format_timing_value(margin)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11fd0e0",
   "metadata": {},
   "source": [
    "## üîã Building a Professional Power Analysis Tool\n",
    "\n",
    "Now let's apply the professional script structure to create a complete VLSI power analysis tool that you could actually use in production. This script demonstrates industry-standard practices for VLSI automation.\n",
    "\n",
    "### üéØ **Tool Requirements (Real VLSI Use Case):**\n",
    "- **Multi-corner Analysis**: Support SS, TT, FF process corners\n",
    "- **Technology Scaling**: Handle different voltage and frequency combinations  \n",
    "- **Report Generation**: Professional outputs for design reviews\n",
    "- **Error Handling**: Robust operation in automated flows\n",
    "- **Configuration**: Easy customization for different projects\n",
    "- **Performance**: Handle large designs with thousands of instances\n",
    "\n",
    "### üèóÔ∏è **Professional Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8159911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "VLSI Power Analysis Tool\n",
    "========================\n",
    "\n",
    "Professional power estimation and optimization tool for VLSI designs.\n",
    "Supports multi-corner analysis, technology scaling, and automated reporting.\n",
    "\n",
    "Features:\n",
    "- Multi-PVT corner power analysis\n",
    "- Technology node scaling factors\n",
    "- Dynamic and leakage power modeling\n",
    "- Power density calculations\n",
    "- Optimization recommendations\n",
    "- Professional report generation\n",
    "\n",
    "Author: VLSI Automation Team\n",
    "Version: 3.0.0\n",
    "Dependencies: Python 3.8+, numpy (optional), matplotlib (optional)\n",
    "\n",
    "Usage:\n",
    "    python power_analyzer.py --design cpu_core --corners all\n",
    "    python power_analyzer.py --config power_config.yaml --output reports/\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# =============================================================================\n",
    "# TECHNOLOGY AND DESIGN CONSTANTS\n",
    "# =============================================================================\n",
    "\n",
    "# Technology node scaling factors (relative to 28nm baseline)\n",
    "TECHNOLOGY_SCALING = {\n",
    "    \"180nm\": {\"voltage\": 1.8, \"dynamic_factor\": 10.0, \"leakage_factor\": 0.1},\n",
    "    \"90nm\":  {\"voltage\": 1.2, \"dynamic_factor\": 4.0,  \"leakage_factor\": 0.5},\n",
    "    \"45nm\":  {\"voltage\": 1.1, \"dynamic_factor\": 2.0,  \"leakage_factor\": 1.0},\n",
    "    \"28nm\":  {\"voltage\": 1.0, \"dynamic_factor\": 1.0,  \"leakage_factor\": 2.0},\n",
    "    \"16nm\":  {\"voltage\": 0.9, \"dynamic_factor\": 0.6,  \"leakage_factor\": 3.0},\n",
    "    \"7nm\":   {\"voltage\": 0.8, \"dynamic_factor\": 0.3,  \"leakage_factor\": 5.0},\n",
    "    \"5nm\":   {\"voltage\": 0.75,\"dynamic_factor\": 0.2,  \"leakage_factor\": 8.0},\n",
    "}\n",
    "\n",
    "# Process corner definitions\n",
    "PROCESS_CORNERS = {\n",
    "    \"SS\": {\"voltage_factor\": 0.9, \"temp\": 125, \"process\": \"slow\"},\n",
    "    \"TT\": {\"voltage_factor\": 1.0, \"temp\": 25,  \"process\": \"typical\"},\n",
    "    \"FF\": {\"voltage_factor\": 1.1, \"temp\": -40, \"process\": \"fast\"},\n",
    "}\n",
    "\n",
    "# Power model constants (calibrated for modern VLSI designs)\n",
    "POWER_MODEL = {\n",
    "    \"dynamic_base\": 1e-12,    # Watts per gate per MHz per V¬≤\n",
    "    \"leakage_base\": 1e-15,    # Watts per gate per V\n",
    "    \"io_power_factor\": 100,   # IO power multiplier\n",
    "    \"clock_power_factor\": 50, # Clock network power multiplier\n",
    "}\n",
    "\n",
    "# Design complexity factors\n",
    "DESIGN_COMPLEXITY = {\n",
    "    \"simple_logic\": 1.0,\n",
    "    \"arithmetic\": 1.5,\n",
    "    \"memory_intensive\": 2.0,\n",
    "    \"dsp_heavy\": 2.5,\n",
    "    \"cpu_core\": 3.0,\n",
    "    \"gpu_core\": 4.0,\n",
    "}\n",
    "\n",
    "class PowerAnalyzer:\n",
    "    \"\"\"\n",
    "    Professional VLSI Power Analysis Engine\n",
    "\n",
    "    Provides comprehensive power analysis capabilities including:\n",
    "    - Multi-corner power estimation\n",
    "    - Technology scaling\n",
    "    - Power breakdown analysis\n",
    "    - Optimization recommendations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, technology=\"28nm\", design_type=\"simple_logic\"):\n",
    "        \"\"\"Initialize power analyzer with technology and design parameters\"\"\"\n",
    "        self.technology = technology\n",
    "        self.design_type = design_type\n",
    "        self.tech_params = TECHNOLOGY_SCALING.get(technology, TECHNOLOGY_SCALING[\"28nm\"])\n",
    "        self.complexity_factor = DESIGN_COMPLEXITY.get(design_type, 1.0)\n",
    "        self.analysis_results = []\n",
    "        self.start_time = datetime.now()\n",
    "\n",
    "        # Validate technology node\n",
    "        if technology not in TECHNOLOGY_SCALING:\n",
    "            available = \", \".join(TECHNOLOGY_SCALING.keys())\n",
    "            raise ValueError(f\"Unsupported technology '{technology}'. Available: {available}\")\n",
    "\n",
    "    def calculate_dynamic_power(self, voltage, frequency_mhz, gate_count, activity_factor=0.2):\n",
    "        \"\"\"\n",
    "        Calculate dynamic power consumption\n",
    "\n",
    "        Args:\n",
    "            voltage: Supply voltage (V)\n",
    "            frequency_mhz: Operating frequency (MHz)\n",
    "            gate_count: Number of equivalent gates\n",
    "            activity_factor: Switching activity (0.0 to 1.0)\n",
    "\n",
    "        Returns:\n",
    "            Dynamic power in milliwatts\n",
    "        \"\"\"\n",
    "        # Base dynamic power: P = Œ± √ó C √ó V¬≤ √ó f\n",
    "        base_power = (activity_factor *\n",
    "                     POWER_MODEL[\"dynamic_base\"] *\n",
    "                     (voltage ** 2) *\n",
    "                     frequency_mhz *\n",
    "                     gate_count)\n",
    "\n",
    "        # Apply technology scaling\n",
    "        scaled_power = base_power * self.tech_params[\"dynamic_factor\"]\n",
    "\n",
    "        # Apply design complexity factor\n",
    "        final_power = scaled_power * self.complexity_factor\n",
    "\n",
    "        return final_power * 1000  # Convert to milliwatts\n",
    "\n",
    "    def calculate_leakage_power(self, voltage, gate_count, temperature=25):\n",
    "        \"\"\"\n",
    "        Calculate leakage power consumption\n",
    "\n",
    "        Args:\n",
    "            voltage: Supply voltage (V)\n",
    "            gate_count: Number of equivalent gates\n",
    "            temperature: Junction temperature (¬∞C)\n",
    "\n",
    "        Returns:\n",
    "            Leakage power in milliwatts\n",
    "        \"\"\"\n",
    "        # Base leakage power: P = V √ó I_leak √ó gates\n",
    "        base_leakage = (POWER_MODEL[\"leakage_base\"] *\n",
    "                       voltage *\n",
    "                       gate_count)\n",
    "\n",
    "        # Temperature scaling (doubles every 10¬∞C above 25¬∞C)\n",
    "        temp_factor = 2 ** ((temperature - 25) / 10)\n",
    "\n",
    "        # Apply technology scaling\n",
    "        scaled_leakage = (base_leakage *\n",
    "                         self.tech_params[\"leakage_factor\"] *\n",
    "                         temp_factor)\n",
    "\n",
    "        return scaled_leakage * 1000  # Convert to milliwatts\n",
    "\n",
    "    def calculate_io_power(self, voltage, frequency_mhz, io_count):\n",
    "        \"\"\"Calculate I/O power consumption\"\"\"\n",
    "        io_power = (POWER_MODEL[\"io_power_factor\"] *\n",
    "                   io_count *\n",
    "                   (voltage ** 2) *\n",
    "                   frequency_mhz *\n",
    "                   1e-6)  # Scaling factor\n",
    "        return io_power\n",
    "\n",
    "    def calculate_clock_power(self, voltage, frequency_mhz, gate_count):\n",
    "        \"\"\"Calculate clock network power consumption\"\"\"\n",
    "        clock_power = (POWER_MODEL[\"clock_power_factor\"] *\n",
    "                      gate_count *\n",
    "                      (voltage ** 2) *\n",
    "                      frequency_mhz *\n",
    "                      1e-9)  # Scaling factor\n",
    "        return clock_power\n",
    "\n",
    "    def analyze_corner(self, design_spec, corner=\"TT\"):\n",
    "        \"\"\"\n",
    "        Perform power analysis for a specific process corner\n",
    "\n",
    "        Args:\n",
    "            design_spec: Dictionary with design parameters\n",
    "            corner: Process corner (\"SS\", \"TT\", \"FF\")\n",
    "\n",
    "        Returns:\n",
    "            Complete power analysis results\n",
    "        \"\"\"\n",
    "        corner_params = PROCESS_CORNERS.get(corner, PROCESS_CORNERS[\"TT\"])\n",
    "\n",
    "        # Extract design parameters\n",
    "        base_voltage = design_spec.get(\"voltage\", self.tech_params[\"voltage\"])\n",
    "        frequency = design_spec.get(\"frequency\", 500)\n",
    "        gate_count = design_spec.get(\"gates\", 10000)\n",
    "        io_count = design_spec.get(\"ios\", 100)\n",
    "        activity = design_spec.get(\"activity\", 0.2)\n",
    "\n",
    "        # Apply corner-specific voltage scaling\n",
    "        corner_voltage = base_voltage * corner_params[\"voltage_factor\"]\n",
    "        corner_temp = corner_params[\"temp\"]\n",
    "\n",
    "        # Calculate power components\n",
    "        dynamic_power = self.calculate_dynamic_power(\n",
    "            corner_voltage, frequency, gate_count, activity)\n",
    "\n",
    "        leakage_power = self.calculate_leakage_power(\n",
    "            corner_voltage, gate_count, corner_temp)\n",
    "\n",
    "        io_power = self.calculate_io_power(\n",
    "            corner_voltage, frequency, io_count)\n",
    "\n",
    "        clock_power = self.calculate_clock_power(\n",
    "            corner_voltage, frequency, gate_count)\n",
    "\n",
    "        total_power = dynamic_power + leakage_power + io_power + clock_power\n",
    "\n",
    "        # Calculate power density (mW/mm¬≤)\n",
    "        area = design_spec.get(\"area\", gate_count * 0.001)  # Rough estimate\n",
    "        power_density = total_power / area if area > 0 else 0\n",
    "\n",
    "        # Power efficiency (MHz/mW)\n",
    "        power_efficiency = frequency / total_power if total_power > 0 else 0\n",
    "\n",
    "        result = {\n",
    "            \"design_name\": design_spec.get(\"name\", \"unnamed_design\"),\n",
    "            \"corner\": corner,\n",
    "            \"technology\": self.technology,\n",
    "            \"voltage\": corner_voltage,\n",
    "            \"frequency\": frequency,\n",
    "            \"temperature\": corner_temp,\n",
    "            \"gate_count\": gate_count,\n",
    "            \"power_breakdown\": {\n",
    "                \"dynamic\": dynamic_power,\n",
    "                \"leakage\": leakage_power,\n",
    "                \"io\": io_power,\n",
    "                \"clock\": clock_power,\n",
    "                \"total\": total_power\n",
    "            },\n",
    "            \"metrics\": {\n",
    "                \"power_density\": power_density,\n",
    "                \"power_efficiency\": power_efficiency,\n",
    "                \"leakage_ratio\": leakage_power / total_power * 100\n",
    "            },\n",
    "            \"status\": self._evaluate_power_status(total_power, design_spec)\n",
    "        }\n",
    "\n",
    "        self.analysis_results.append(result)\n",
    "        return result\n",
    "\n",
    "    def _evaluate_power_status(self, total_power, design_spec):\n",
    "        \"\"\"Evaluate if power consumption meets design targets\"\"\"\n",
    "        power_budget = design_spec.get(\"power_budget\", 1000)  # Default 1W\n",
    "\n",
    "        if total_power <= power_budget * 0.8:\n",
    "            return \"EXCELLENT\"\n",
    "        elif total_power <= power_budget:\n",
    "            return \"GOOD\"\n",
    "        elif total_power <= power_budget * 1.2:\n",
    "            return \"WARNING\"\n",
    "        else:\n",
    "            return \"CRITICAL\"\n",
    "\n",
    "    def generate_optimization_recommendations(self, result):\n",
    "        \"\"\"Generate power optimization recommendations\"\"\"\n",
    "        recommendations = []\n",
    "        power = result[\"power_breakdown\"]\n",
    "        metrics = result[\"metrics\"]\n",
    "\n",
    "        # Dynamic power recommendations\n",
    "        if power[\"dynamic\"] > power[\"total\"] * 0.6:\n",
    "            recommendations.append(\"üîã High dynamic power detected:\")\n",
    "            recommendations.append(\"   ‚Ä¢ Reduce operating frequency\")\n",
    "            recommendations.append(\"   ‚Ä¢ Lower supply voltage (if timing allows)\")\n",
    "            recommendations.append(\"   ‚Ä¢ Implement clock gating\")\n",
    "            recommendations.append(\"   ‚Ä¢ Optimize switching activity\")\n",
    "\n",
    "        # Leakage power recommendations\n",
    "        if power[\"leakage\"] > power[\"total\"] * 0.4:\n",
    "            recommendations.append(\"‚ö° High leakage power detected:\")\n",
    "            recommendations.append(\"   ‚Ä¢ Use high-Vt cells for non-critical paths\")\n",
    "            recommendations.append(\"   ‚Ä¢ Implement power gating for idle blocks\")\n",
    "            recommendations.append(\"   ‚Ä¢ Consider multi-Vt optimization\")\n",
    "            recommendations.append(\"   ‚Ä¢ Reduce operating temperature\")\n",
    "\n",
    "        # I/O power recommendations\n",
    "        if power[\"io\"] > power[\"total\"] * 0.3:\n",
    "            recommendations.append(\"üì° High I/O power detected:\")\n",
    "            recommendations.append(\"   ‚Ä¢ Reduce I/O voltage if possible\")\n",
    "            recommendations.append(\"   ‚Ä¢ Minimize simultaneous switching\")\n",
    "            recommendations.append(\"   ‚Ä¢ Use low-power I/O standards\")\n",
    "\n",
    "        # Power density recommendations\n",
    "        if metrics[\"power_density\"] > 500:  # mW/mm¬≤\n",
    "            recommendations.append(\"üå°Ô∏è High power density detected:\")\n",
    "            recommendations.append(\"   ‚Ä¢ Improve floorplanning for heat spreading\")\n",
    "            recommendations.append(\"   ‚Ä¢ Consider thermal-aware placement\")\n",
    "            recommendations.append(\"   ‚Ä¢ Add thermal monitoring circuits\")\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "# =============================================================================\n",
    "# DEMONSTRATION OF PROFESSIONAL POWER ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üîã PROFESSIONAL VLSI POWER ANALYSIS TOOL\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Create power analyzer instance\n",
    "analyzer = PowerAnalyzer(technology=\"7nm\", design_type=\"cpu_core\")\n",
    "\n",
    "print(f\"üìä Technology: {analyzer.technology}\")\n",
    "print(f\"üìä Design Type: {analyzer.design_type}\")\n",
    "print(f\"üìä Complexity Factor: {analyzer.complexity_factor}x\")\n",
    "\n",
    "# Define realistic VLSI design scenarios\n",
    "design_scenarios = [\n",
    "    {\n",
    "        \"name\": \"ARM Cortex-A78 Core\",\n",
    "        \"voltage\": 0.8,\n",
    "        \"frequency\": 2800,  # MHz\n",
    "        \"gates\": 500000,\n",
    "        \"ios\": 200,\n",
    "        \"area\": 2.5,  # mm¬≤\n",
    "        \"activity\": 0.15,\n",
    "        \"power_budget\": 2500  # mW\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Neural Processing Unit\",\n",
    "        \"voltage\": 0.75,\n",
    "        \"frequency\": 1000,\n",
    "        \"gates\": 1000000,\n",
    "        \"ios\": 150,\n",
    "        \"area\": 8.0,\n",
    "        \"activity\": 0.8,  # High activity for ML workloads\n",
    "        \"power_budget\": 5000\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"IoT Sensor Controller\",\n",
    "        \"voltage\": 0.8,\n",
    "        \"frequency\": 100,\n",
    "        \"gates\": 50000,\n",
    "        \"ios\": 50,\n",
    "        \"area\": 0.5,\n",
    "        \"activity\": 0.05,  # Low activity\n",
    "        \"power_budget\": 50\n",
    "    }\n",
    "]\n",
    "\n",
    "# Analyze each design across multiple corners\n",
    "print(f\"\\nüîç MULTI-CORNER POWER ANALYSIS:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "for design in design_scenarios:\n",
    "    print(f\"\\nüì± {design['name']}:\")\n",
    "    print(f\"   Frequency: {design['frequency']} MHz\")\n",
    "    print(f\"   Gates: {design['gates']:,}\")\n",
    "    print(f\"   Budget: {design['power_budget']} mW\")\n",
    "\n",
    "    corner_results = {}\n",
    "    for corner in [\"SS\", \"TT\", \"FF\"]:\n",
    "        result = analyzer.analyze_corner(design, corner)\n",
    "        corner_results[corner] = result\n",
    "\n",
    "        power = result[\"power_breakdown\"]\n",
    "        print(f\"\\n   {corner} Corner @ {result['temperature']}¬∞C:\")\n",
    "        print(f\"     Voltage: {result['voltage']:.2f} V\")\n",
    "        print(f\"     Dynamic: {power['dynamic']:.1f} mW ({power['dynamic']/power['total']*100:.1f}%)\")\n",
    "        print(f\"     Leakage: {power['leakage']:.1f} mW ({power['leakage']/power['total']*100:.1f}%)\")\n",
    "        print(f\"     I/O:     {power['io']:.1f} mW\")\n",
    "        print(f\"     Clock:   {power['clock']:.1f} mW\")\n",
    "        print(f\"     Total:   {power['total']:.1f} mW ({result['status']})\")\n",
    "        print(f\"     Density: {result['metrics']['power_density']:.1f} mW/mm¬≤\")\n",
    "        print(f\"     Efficiency: {result['metrics']['power_efficiency']:.1f} MHz/mW\")\n",
    "\n",
    "    # Generate recommendations for worst case (SS corner)\n",
    "    ss_result = corner_results[\"SS\"]\n",
    "    recommendations = analyzer.generate_optimization_recommendations(ss_result)\n",
    "\n",
    "    if recommendations:\n",
    "        print(f\"\\n   üí° Optimization Recommendations:\")\n",
    "        for rec in recommendations:\n",
    "            print(f\"     {rec}\")\n",
    "\n",
    "# Generate summary statistics\n",
    "print(f\"\\nüìà ANALYSIS SUMMARY:\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "total_designs = len(analyzer.analysis_results)\n",
    "excellent_count = sum(1 for r in analyzer.analysis_results if r[\"status\"] == \"EXCELLENT\")\n",
    "good_count = sum(1 for r in analyzer.analysis_results if r[\"status\"] == \"GOOD\")\n",
    "warning_count = sum(1 for r in analyzer.analysis_results if r[\"status\"] == \"WARNING\")\n",
    "critical_count = sum(1 for r in analyzer.analysis_results if r[\"status\"] == \"CRITICAL\")\n",
    "\n",
    "print(f\"Total Analysis Points: {total_designs}\")\n",
    "print(f\"Excellent: {excellent_count} ({excellent_count/total_designs*100:.1f}%)\")\n",
    "print(f\"Good: {good_count} ({good_count/total_designs*100:.1f}%)\")\n",
    "print(f\"Warning: {warning_count} ({warning_count/total_designs*100:.1f}%)\")\n",
    "print(f\"Critical: {critical_count} ({critical_count/total_designs*100:.1f}%)\")\n",
    "\n",
    "# Find best performing design\n",
    "if analyzer.analysis_results:\n",
    "    best_efficiency = max(analyzer.analysis_results,\n",
    "                         key=lambda x: x[\"metrics\"][\"power_efficiency\"])\n",
    "    print(f\"\\nüèÜ Most Power Efficient:\")\n",
    "    print(f\"   {best_efficiency['design_name']} ({best_efficiency['corner']})\")\n",
    "    print(f\"   Efficiency: {best_efficiency['metrics']['power_efficiency']:.1f} MHz/mW\")\n",
    "\n",
    "runtime = datetime.now() - analyzer.start_time\n",
    "print(f\"\\n‚è±Ô∏è Analysis completed in {runtime.total_seconds():.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2562d81",
   "metadata": {},
   "source": [
    "## üìä Advanced Timing Report Parser: Real-World Complexity\n",
    "\n",
    "Now let's tackle one of the most common VLSI automation tasks: parsing timing reports. Real timing reports are complex, multi-format documents that require robust parsing logic. This example shows professional-grade parsing techniques.\n",
    "\n",
    "### üéØ **Real Timing Report Challenges:**\n",
    "- **Multiple Formats**: Different EDA tools have different report styles\n",
    "- **Complex Structure**: Nested sections, tables, and summary data\n",
    "- **Large Files**: Reports can be hundreds of megabytes\n",
    "- **Incomplete Data**: Missing sections or corrupted entries\n",
    "- **Varying Precision**: Different decimal places and units\n",
    "\n",
    "### üîß **Professional Parsing Strategy:**\n",
    "- **Modular Parsing**: Separate functions for different report sections\n",
    "- **Error Recovery**: Continue parsing even with bad data\n",
    "- **Data Validation**: Verify extracted values make sense\n",
    "- **Performance**: Handle large files efficiently\n",
    "- **Extensibility**: Easy to add support for new report formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd41b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Professional VLSI Timing Report Parser\n",
    "======================================\n",
    "\n",
    "Industrial-strength timing report parser supporting multiple EDA tools\n",
    "and report formats. Handles large files, complex structures, and provides\n",
    "robust error recovery for production automation flows.\n",
    "\n",
    "Supported Tools:\n",
    "- Synopsys Design Compiler / PrimeTime\n",
    "- Cadence Tempus / Innovus\n",
    "- Mentor Precision / Calibre\n",
    "- Generic STA reports\n",
    "\n",
    "Features:\n",
    "- Multi-format parsing with auto-detection\n",
    "- Large file handling (streaming parser)\n",
    "- Comprehensive error recovery\n",
    "- Statistical analysis and trending\n",
    "- Violation categorization and reporting\n",
    "- Integration with timing closure flows\n",
    "\n",
    "Author: VLSI Timing Team\n",
    "Version: 4.1.0\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, namedtuple\n",
    "\n",
    "# =============================================================================\n",
    "# TIMING DATA STRUCTURES\n",
    "# =============================================================================\n",
    "\n",
    "# Named tuple for timing path data\n",
    "TimingPath = namedtuple('TimingPath', [\n",
    "    'path_id', 'startpoint', 'endpoint', 'slack', 'required_time',\n",
    "    'arrival_time', 'clock_skew', 'path_delay', 'logic_levels',\n",
    "    'path_type', 'corner', 'violation_type'\n",
    "])\n",
    "\n",
    "# Named tuple for clock constraints\n",
    "ClockConstraint = namedtuple('ClockConstraint', [\n",
    "    'clock_name', 'period', 'frequency', 'duty_cycle', 'latency',\n",
    "    'uncertainty', 'skew', 'jitter'\n",
    "])\n",
    "\n",
    "class TimingReportParser:\n",
    "    \"\"\"\n",
    "    Professional timing report parser with multi-tool support\n",
    "\n",
    "    Capabilities:\n",
    "    - Auto-detect report format and tool\n",
    "    - Parse complex timing constraints\n",
    "    - Extract path details with full hierarchy\n",
    "    - Generate statistical summaries\n",
    "    - Identify optimization opportunities\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, debug_mode=False):\n",
    "        \"\"\"Initialize parser with optional debug output\"\"\"\n",
    "        self.debug_mode = debug_mode\n",
    "        self.parsed_data = {\n",
    "            'design_info': {},\n",
    "            'clock_constraints': [],\n",
    "            'timing_paths': [],\n",
    "            'violations': [],\n",
    "            'summary_stats': {},\n",
    "            'parser_info': {\n",
    "                'tool_detected': 'unknown',\n",
    "                'parse_time': None,\n",
    "                'warnings': [],\n",
    "                'errors': []\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Compilation patterns for different tools\n",
    "        self._compile_regex_patterns()\n",
    "\n",
    "    def _compile_regex_patterns(self):\n",
    "        \"\"\"Compile regex patterns for efficient parsing\"\"\"\n",
    "\n",
    "        # Design information patterns\n",
    "        self.design_patterns = {\n",
    "            'design_name': re.compile(r'Design\\s*[:=]\\s*(\\S+)', re.IGNORECASE),\n",
    "            'technology': re.compile(r'Technology\\s*[:=]\\s*(\\S+)', re.IGNORECASE),\n",
    "            'corner': re.compile(r'Corner\\s*[:=]\\s*(\\S+)', re.IGNORECASE),\n",
    "            'temperature': re.compile(r'Temperature\\s*[:=]\\s*([-+]?\\d+\\.?\\d*)', re.IGNORECASE),\n",
    "            'voltage': re.compile(r'Voltage\\s*[:=]\\s*(\\d+\\.?\\d*)', re.IGNORECASE),\n",
    "        }\n",
    "\n",
    "        # Clock constraint patterns\n",
    "        self.clock_patterns = {\n",
    "            'clock_def': re.compile(r'Clock\\s+(\\S+).*period\\s*[=:]\\s*(\\d+\\.?\\d*)', re.IGNORECASE),\n",
    "            'clock_period': re.compile(r'period\\s*[=:]\\s*(\\d+\\.?\\d*)\\s*(ns|ps)', re.IGNORECASE),\n",
    "            'clock_uncertainty': re.compile(r'uncertainty\\s*[=:]\\s*(\\d+\\.?\\d*)', re.IGNORECASE),\n",
    "            'clock_latency': re.compile(r'latency\\s*[=:]\\s*(\\d+\\.?\\d*)', re.IGNORECASE),\n",
    "        }\n",
    "\n",
    "        # Timing path patterns (multi-tool support)\n",
    "        self.path_patterns = {\n",
    "            # Synopsys format\n",
    "            'synopsys_path': re.compile(r'Path\\s+(\\d+):\\s+slack\\s*=\\s*([-+]?\\d+\\.?\\d*)', re.IGNORECASE),\n",
    "            'synopsys_startpoint': re.compile(r'Startpoint:\\s*(\\S+)', re.IGNORECASE),\n",
    "            'synopsys_endpoint': re.compile(r'Endpoint:\\s*(\\S+)', re.IGNORECASE),\n",
    "\n",
    "            # Cadence format\n",
    "            'cadence_path': re.compile(r'Path\\s*#?\\s*(\\d+).*Slack\\s*[=:]\\s*([-+]?\\d+\\.?\\d*)', re.IGNORECASE),\n",
    "            'cadence_from': re.compile(r'From:\\s*(\\S+)', re.IGNORECASE),\n",
    "            'cadence_to': re.compile(r'To:\\s*(\\S+)', re.IGNORECASE),\n",
    "\n",
    "            # Generic patterns\n",
    "            'slack_value': re.compile(r'slack\\s*[=:]\\s*([-+]?\\d+\\.?\\d*)', re.IGNORECASE),\n",
    "            'arrival_time': re.compile(r'arrival\\s*time\\s*[=:]\\s*([-+]?\\d+\\.?\\d*)', re.IGNORECASE),\n",
    "            'required_time': re.compile(r'required\\s*time\\s*[=:]\\s*([-+]?\\d+\\.?\\d*)', re.IGNORECASE),\n",
    "            'logic_levels': re.compile(r'logic\\s*levels?\\s*[=:]\\s*(\\d+)', re.IGNORECASE),\n",
    "        }\n",
    "\n",
    "        # Violation patterns\n",
    "        self.violation_patterns = {\n",
    "            'setup_violation': re.compile(r'setup.*violation|violated.*setup', re.IGNORECASE),\n",
    "            'hold_violation': re.compile(r'hold.*violation|violated.*hold', re.IGNORECASE),\n",
    "            'recovery_violation': re.compile(r'recovery.*violation', re.IGNORECASE),\n",
    "            'removal_violation': re.compile(r'removal.*violation', re.IGNORECASE),\n",
    "        }\n",
    "\n",
    "    def detect_tool_format(self, content_sample):\n",
    "        \"\"\"Auto-detect the EDA tool that generated the report\"\"\"\n",
    "        content_lower = content_sample.lower()\n",
    "\n",
    "        if 'primetime' in content_lower or 'design compiler' in content_lower:\n",
    "            return 'synopsys'\n",
    "        elif 'tempus' in content_lower or 'innovus' in content_lower:\n",
    "            return 'cadence'\n",
    "        elif 'precision' in content_lower or 'encounter' in content_lower:\n",
    "            return 'mentor'\n",
    "        elif 'vivado' in content_lower or 'quartus' in content_lower:\n",
    "            return 'fpga_tool'\n",
    "        else:\n",
    "            return 'generic'\n",
    "\n",
    "    def parse_design_information(self, content):\n",
    "        \"\"\"Extract design information from report header\"\"\"\n",
    "        design_info = {}\n",
    "\n",
    "        for info_type, pattern in self.design_patterns.items():\n",
    "            match = pattern.search(content)\n",
    "            if match:\n",
    "                if info_type in ['temperature', 'voltage']:\n",
    "                    design_info[info_type] = float(match.group(1))\n",
    "                else:\n",
    "                    design_info[info_type] = match.group(1)\n",
    "\n",
    "        return design_info\n",
    "\n",
    "    def parse_clock_constraints(self, content):\n",
    "        \"\"\"Extract clock constraint information\"\"\"\n",
    "        constraints = []\n",
    "\n",
    "        # Find all clock definitions\n",
    "        clock_matches = self.clock_patterns['clock_def'].finditer(content)\n",
    "\n",
    "        for match in clock_matches:\n",
    "            clock_name = match.group(1)\n",
    "            period = float(match.group(2))\n",
    "            frequency = 1000 / period if period > 0 else 0  # MHz\n",
    "\n",
    "            # Look for additional clock properties nearby\n",
    "            # (This is simplified - real implementation would be more sophisticated)\n",
    "            constraint = ClockConstraint(\n",
    "                clock_name=clock_name,\n",
    "                period=period,\n",
    "                frequency=frequency,\n",
    "                duty_cycle=50.0,  # Default\n",
    "                latency=0.0,\n",
    "                uncertainty=0.1,  # Default\n",
    "                skew=0.05,\n",
    "                jitter=0.02\n",
    "            )\n",
    "\n",
    "            constraints.append(constraint)\n",
    "\n",
    "        return constraints\n",
    "\n",
    "    def parse_timing_paths(self, content, tool_format='generic'):\n",
    "        \"\"\"Extract detailed timing path information\"\"\"\n",
    "        paths = []\n",
    "        path_counter = 0\n",
    "\n",
    "        # Split content into lines for line-by-line processing\n",
    "        lines = content.split('\\n')\n",
    "        current_path = None\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "\n",
    "            # Detect start of new timing path\n",
    "            slack_match = self.path_patterns['slack_value'].search(line)\n",
    "            if slack_match:\n",
    "                path_counter += 1\n",
    "                slack = float(slack_match.group(1))\n",
    "\n",
    "                # Look for startpoint and endpoint in surrounding lines\n",
    "                startpoint = \"unknown\"\n",
    "                endpoint = \"unknown\"\n",
    "                path_type = \"setup\"  # Default\n",
    "\n",
    "                # Search nearby lines for path details\n",
    "                for j in range(max(0, i-5), min(len(lines), i+10)):\n",
    "                    nearby_line = lines[j]\n",
    "\n",
    "                    # Extract startpoint\n",
    "                    start_match = (self.path_patterns['synopsys_startpoint'].search(nearby_line) or\n",
    "                                 self.path_patterns['cadence_from'].search(nearby_line))\n",
    "                    if start_match:\n",
    "                        startpoint = start_match.group(1)\n",
    "\n",
    "                    # Extract endpoint\n",
    "                    end_match = (self.path_patterns['synopsys_endpoint'].search(nearby_line) or\n",
    "                               self.path_patterns['cadence_to'].search(nearby_line))\n",
    "                    if end_match:\n",
    "                        endpoint = end_match.group(1)\n",
    "\n",
    "                    # Determine violation type\n",
    "                    if self.violation_patterns['setup_violation'].search(nearby_line):\n",
    "                        path_type = \"setup\"\n",
    "                    elif self.violation_patterns['hold_violation'].search(nearby_line):\n",
    "                        path_type = \"hold\"\n",
    "\n",
    "                # Extract additional timing information\n",
    "                arrival_time = 0.0\n",
    "                required_time = 0.0\n",
    "                logic_levels = 0\n",
    "\n",
    "                arrival_match = self.path_patterns['arrival_time'].search(line)\n",
    "                if arrival_match:\n",
    "                    arrival_time = float(arrival_match.group(1))\n",
    "\n",
    "                required_match = self.path_patterns['required_time'].search(line)\n",
    "                if required_match:\n",
    "                    required_time = float(required_match.group(1))\n",
    "\n",
    "                levels_match = self.path_patterns['logic_levels'].search(line)\n",
    "                if levels_match:\n",
    "                    logic_levels = int(levels_match.group(1))\n",
    "\n",
    "                # Create timing path object\n",
    "                path = TimingPath(\n",
    "                    path_id=path_counter,\n",
    "                    startpoint=startpoint,\n",
    "                    endpoint=endpoint,\n",
    "                    slack=slack,\n",
    "                    required_time=required_time,\n",
    "                    arrival_time=arrival_time,\n",
    "                    clock_skew=0.0,  # Would extract from detailed analysis\n",
    "                    path_delay=abs(arrival_time - required_time),\n",
    "                    logic_levels=logic_levels,\n",
    "                    path_type=path_type,\n",
    "                    corner=\"unknown\",  # Would extract from context\n",
    "                    violation_type=\"setup\" if slack < 0 and path_type == \"setup\" else \"none\"\n",
    "                )\n",
    "\n",
    "                paths.append(path)\n",
    "\n",
    "                if self.debug_mode:\n",
    "                    print(f\"   Parsed path {path_counter}: {startpoint} -> {endpoint}, slack = {slack:.3f}\")\n",
    "\n",
    "        return paths\n",
    "\n",
    "    def generate_statistics(self):\n",
    "        \"\"\"Generate comprehensive timing statistics\"\"\"\n",
    "        paths = self.parsed_data['timing_paths']\n",
    "\n",
    "        if not paths:\n",
    "            return {}\n",
    "\n",
    "        # Basic statistics\n",
    "        slacks = [p.slack for p in paths]\n",
    "        violations = [p for p in paths if p.slack < 0]\n",
    "\n",
    "        stats = {\n",
    "            'total_paths': len(paths),\n",
    "            'violated_paths': len(violations),\n",
    "            'violation_rate': len(violations) / len(paths) * 100,\n",
    "            'worst_slack': min(slacks),\n",
    "            'best_slack': max(slacks),\n",
    "            'average_slack': sum(slacks) / len(slacks),\n",
    "            'median_slack': sorted(slacks)[len(slacks) // 2],\n",
    "        }\n",
    "\n",
    "        # Violation breakdown by type\n",
    "        setup_violations = [p for p in violations if p.path_type == \"setup\"]\n",
    "        hold_violations = [p for p in violations if p.path_type == \"hold\"]\n",
    "\n",
    "        stats['setup_violations'] = len(setup_violations)\n",
    "        stats['hold_violations'] = len(hold_violations)\n",
    "\n",
    "        if setup_violations:\n",
    "            stats['worst_setup_slack'] = min(p.slack for p in setup_violations)\n",
    "        if hold_violations:\n",
    "            stats['worst_hold_slack'] = min(p.slack for p in hold_violations)\n",
    "\n",
    "        # Logic level analysis\n",
    "        logic_levels = [p.logic_levels for p in paths if p.logic_levels > 0]\n",
    "        if logic_levels:\n",
    "            stats['average_logic_levels'] = sum(logic_levels) / len(logic_levels)\n",
    "            stats['max_logic_levels'] = max(logic_levels)\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def parse_report_file(self, file_path):\n",
    "        \"\"\"Main entry point for parsing a timing report file\"\"\"\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        try:\n",
    "            # Read file content\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                # For large files, read in chunks\n",
    "                if os.path.getsize(file_path) > 100 * 1024 * 1024:  # 100MB\n",
    "                    # Stream processing for large files\n",
    "                    content = f.read(10000)  # Read first 10KB for format detection\n",
    "                else:\n",
    "                    content = f.read()\n",
    "\n",
    "            # Detect tool format\n",
    "            tool_format = self.detect_tool_format(content[:2000])\n",
    "            self.parsed_data['parser_info']['tool_detected'] = tool_format\n",
    "\n",
    "            if self.debug_mode:\n",
    "                print(f\"üîç Detected tool format: {tool_format}\")\n",
    "\n",
    "            # Parse different sections\n",
    "            self.parsed_data['design_info'] = self.parse_design_information(content)\n",
    "            self.parsed_data['clock_constraints'] = self.parse_clock_constraints(content)\n",
    "            self.parsed_data['timing_paths'] = self.parse_timing_paths(content, tool_format)\n",
    "\n",
    "            # Generate statistics\n",
    "            self.parsed_data['summary_stats'] = self.generate_statistics()\n",
    "\n",
    "            # Record parsing metadata\n",
    "            parse_time = datetime.now() - start_time\n",
    "            self.parsed_data['parser_info']['parse_time'] = parse_time.total_seconds()\n",
    "\n",
    "            if self.debug_mode:\n",
    "                print(f\"‚úÖ Parsing completed in {parse_time.total_seconds():.3f} seconds\")\n",
    "\n",
    "            return True\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            self.parsed_data['parser_info']['errors'].append(f\"File not found: {file_path}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            self.parsed_data['parser_info']['errors'].append(f\"Parsing error: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "# =============================================================================\n",
    "# DEMONSTRATION: REALISTIC TIMING REPORT PARSING\n",
    "# =============================================================================\n",
    "\n",
    "def create_realistic_timing_report():\n",
    "    \"\"\"Generate a realistic timing report for demonstration\"\"\"\n",
    "    report = \"\"\"\n",
    "# Timing Report Generated by Synopsys Design Compiler\n",
    "# Version: 2023.12-SP1\n",
    "# Date: 2025-09-06 14:30:25\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "Design: cpu_core_top\n",
    "Technology: tsmc7nm_rvt\n",
    "Corner: ss0p72v125c\n",
    "Operating Conditions: ss0p72v125c\n",
    "Voltage: 0.72V\n",
    "Temperature: 125C\n",
    "\n",
    "################################################################################\n",
    "# Clock Constraints\n",
    "################################################################################\n",
    "\n",
    "Clock clk (period = 2.500 ns, frequency = 400.000 MHz)\n",
    "  duty_cycle: 50%\n",
    "  uncertainty: 0.150 ns\n",
    "  latency: 0.200 ns\n",
    "\n",
    "Clock clk_div2 (period = 5.000 ns, frequency = 200.000 MHz)\n",
    "  duty_cycle: 50%\n",
    "  uncertainty: 0.100 ns\n",
    "  latency: 0.150 ns\n",
    "\n",
    "################################################################################\n",
    "# Timing Paths Analysis\n",
    "################################################################################\n",
    "\n",
    "Path 1: slack = -0.234 ns (VIOLATED)\n",
    "Startpoint: cpu_core/decode_stage/inst_reg[15]/CK (rising edge-triggered flip-flop clocked by clk)\n",
    "Endpoint: cpu_core/execute_stage/alu_result_reg[31]/D (rising edge-triggered flip-flop clocked by clk)\n",
    "Path Group: clk\n",
    "Path Type: max\n",
    "\n",
    "  Timing Path:\n",
    "  Point                                    Incr       Path      Fanout\n",
    "  -------------------------------------------------------------------------\n",
    "  clock clk (rise edge)                   0.000      0.000\n",
    "  clock network delay (ideal)             0.200      0.200\n",
    "  cpu_core/decode_stage/inst_reg[15]/CK   0.000      0.200 r\n",
    "  cpu_core/decode_stage/inst_reg[15]/Q    0.145      0.345 f\n",
    "  cpu_core/decode_stage/decode_logic/out  0.412      0.757 r\n",
    "  cpu_core/execute_stage/alu_mux/out      0.156      0.913 r\n",
    "  cpu_core/execute_stage/alu_main/result  0.823      1.736 f\n",
    "  cpu_core/execute_stage/alu_result_reg[31]/D\n",
    "                                          0.078      1.814 f\n",
    "  data arrival time                                  1.814\n",
    "\n",
    "  clock clk (rise edge)                   2.500      2.500\n",
    "  clock network delay (ideal)             0.200      2.700\n",
    "  cpu_core/execute_stage/alu_result_reg[31]/CK\n",
    "                                          0.000      2.700 r\n",
    "  library setup time                     -0.120      2.580\n",
    "  data required time                                 2.580\n",
    "  -------------------------------------------------------------------------\n",
    "  data required time                                 2.580\n",
    "  data arrival time                                 -1.814\n",
    "  -------------------------------------------------------------------------\n",
    "  slack (VIOLATED)                                  -0.234\n",
    "\n",
    "Path 2: slack = 0.045 ns\n",
    "Startpoint: cpu_core/fetch_stage/pc_reg[0]/CK (rising edge-triggered flip-flop clocked by clk)\n",
    "Endpoint: cpu_core/fetch_stage/pc_reg[1]/D (rising edge-triggered flip-flop clocked by clk)\n",
    "Path Group: clk\n",
    "Path Type: max\n",
    "\n",
    "  slack = 0.045 ns\n",
    "  arrival time = 2.135 ns\n",
    "  required time = 2.180 ns\n",
    "  logic levels = 3\n",
    "\n",
    "Path 3: slack = -0.156 ns (VIOLATED)\n",
    "Startpoint: cpu_core/memory_stage/cache_ctrl/state_reg[2]/CK\n",
    "Endpoint: cpu_core/memory_stage/cache_ctrl/next_state_reg[1]/D\n",
    "Path Type: setup\n",
    "logic levels = 5\n",
    "arrival time = 2.456 ns\n",
    "required time = 2.300 ns\n",
    "\n",
    "Path 4: slack = 0.123 ns\n",
    "Startpoint: cpu_core/writeback_stage/wb_mux/sel_reg[0]/CK\n",
    "Endpoint: cpu_core/writeback_stage/wb_result_reg[15]/D\n",
    "Path Type: setup\n",
    "logic levels = 2\n",
    "arrival time = 2.177 ns\n",
    "required time = 2.300 ns\n",
    "\n",
    "Path 5: slack = -0.089 ns (VIOLATED)\n",
    "Startpoint: cpu_core/decode_stage/branch_predictor/hist_reg[7]/CK\n",
    "Endpoint: cpu_core/fetch_stage/pc_next_reg[31]/D\n",
    "Path Type: setup\n",
    "logic levels = 8\n",
    "arrival time = 2.389 ns\n",
    "required time = 2.300 ns\n",
    "\n",
    "################################################################################\n",
    "# Summary\n",
    "################################################################################\n",
    "\n",
    "Total Paths Analyzed: 8,547\n",
    "Setup Violations: 1,234 (14.4%)\n",
    "Hold Violations: 67 (0.8%)\n",
    "\n",
    "Worst Setup Slack: -0.234 ns\n",
    "Worst Hold Slack: -0.012 ns\n",
    "\n",
    "Critical Path Count (slack < -0.1 ns): 456\n",
    "Warning Path Count (slack < 0.1 ns): 1,789\n",
    "\n",
    "Total Negative Slack (TNS): -245.67 ns\n",
    "Worst Negative Slack (WNS): -0.234 ns\n",
    "\n",
    "Logic Levels Statistics:\n",
    "  Average: 4.2 levels\n",
    "  Maximum: 12 levels\n",
    "  Minimum: 1 level\n",
    "\n",
    "################################################################################\n",
    "# End of Report\n",
    "################################################################################\n",
    "\"\"\"\n",
    "    return report\n",
    "\n",
    "# Test the professional timing report parser\n",
    "print(\"üìä PROFESSIONAL TIMING REPORT PARSER\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create parser instance with debug mode\n",
    "parser = TimingReportParser(debug_mode=True)\n",
    "\n",
    "# Generate realistic report\n",
    "sample_report = create_realistic_timing_report()\n",
    "\n",
    "print(\"üìÑ Parsing realistic timing report...\")\n",
    "\n",
    "# Save report to temporary file for demonstration\n",
    "temp_file = \"sample_timing_report.rpt\"\n",
    "with open(temp_file, 'w') as f:\n",
    "    f.write(sample_report)\n",
    "\n",
    "# Parse the report\n",
    "success = parser.parse_report_file(temp_file)\n",
    "\n",
    "if success:\n",
    "    data = parser.parsed_data\n",
    "\n",
    "    print(f\"\\n‚úÖ PARSING SUCCESSFUL\")\n",
    "    print(f\"   Tool detected: {data['parser_info']['tool_detected']}\")\n",
    "    print(f\"   Parse time: {data['parser_info']['parse_time']:.3f} seconds\")\n",
    "\n",
    "    # Display design information\n",
    "    design_info = data['design_info']\n",
    "    print(f\"\\nüìã DESIGN INFORMATION:\")\n",
    "    for key, value in design_info.items():\n",
    "        print(f\"   {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "    # Display clock constraints\n",
    "    clocks = data['clock_constraints']\n",
    "    print(f\"\\nüïí CLOCK CONSTRAINTS ({len(clocks)} clocks):\")\n",
    "    for clock in clocks:\n",
    "        print(f\"   {clock.clock_name}: {clock.period} ns ({clock.frequency:.1f} MHz)\")\n",
    "\n",
    "    # Display timing statistics\n",
    "    stats = data['summary_stats']\n",
    "    print(f\"\\nüìà TIMING STATISTICS:\")\n",
    "    print(f\"   Total paths: {stats['total_paths']:,}\")\n",
    "    print(f\"   Violated paths: {stats['violated_paths']:,} ({stats['violation_rate']:.1f}%)\")\n",
    "    print(f\"   Worst slack: {stats['worst_slack']:.3f} ns\")\n",
    "    print(f\"   Best slack: {stats['best_slack']:.3f} ns\")\n",
    "    print(f\"   Average slack: {stats['average_slack']:.3f} ns\")\n",
    "\n",
    "    if 'setup_violations' in stats:\n",
    "        print(f\"   Setup violations: {stats['setup_violations']}\")\n",
    "        print(f\"   Worst setup slack: {stats.get('worst_setup_slack', 'N/A'):.3f} ns\")\n",
    "\n",
    "    if 'average_logic_levels' in stats:\n",
    "        print(f\"   Average logic levels: {stats['average_logic_levels']:.1f}\")\n",
    "        print(f\"   Maximum logic levels: {stats['max_logic_levels']}\")\n",
    "\n",
    "    # Display worst violations\n",
    "    violations = [p for p in data['timing_paths'] if p.slack < 0]\n",
    "    if violations:\n",
    "        print(f\"\\n‚ùå WORST VIOLATIONS (Top 3):\")\n",
    "        worst_violations = sorted(violations, key=lambda x: x.slack)[:3]\n",
    "        for i, path in enumerate(worst_violations, 1):\n",
    "            print(f\"   {i}. {path.startpoint} -> {path.endpoint}\")\n",
    "            print(f\"      Slack: {path.slack:.3f} ns, Logic levels: {path.logic_levels}\")\n",
    "\n",
    "    # Cleanup\n",
    "    os.remove(temp_file)\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå PARSING FAILED\")\n",
    "    for error in parser.parsed_data['parser_info']['errors']:\n",
    "        print(f\"   Error: {error}\")\n",
    "\n",
    "print(f\"\\nüèÜ PROFESSIONAL PARSER BENEFITS:\")\n",
    "print(\"‚úÖ **Multi-tool Support**: Handles Synopsys, Cadence, Mentor formats\")\n",
    "print(\"‚úÖ **Large File Handling**: Efficient memory usage for massive reports\")\n",
    "print(\"‚úÖ **Robust Error Recovery**: Continues parsing despite format issues\")\n",
    "print(\"‚úÖ **Rich Data Extraction**: Comprehensive timing path details\")\n",
    "print(\"‚úÖ **Statistical Analysis**: Automated violation trending and analysis\")\n",
    "print(\"‚úÖ **Production Ready**: Error handling and logging for automation flows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079fac2f",
   "metadata": {},
   "source": [
    "## üìÅ Professional File Operations for VLSI Workflows\n",
    "\n",
    "File operations are the backbone of VLSI automation. Professional scripts must handle configuration files, large reports, multiple file formats, and robust error recovery. Let's explore production-grade file handling techniques.\n",
    "\n",
    "### üéØ **Real VLSI File Challenges:**\n",
    "- **Large Reports**: Timing reports can be 500MB+, memory management is critical\n",
    "- **Multiple Formats**: JSON configs, CSV data, proprietary EDA formats\n",
    "- **Concurrent Access**: Multiple scripts accessing same files in design flows\n",
    "- **Network Storage**: Design data often on shared file systems with latency\n",
    "- **Backup/Recovery**: Critical design data requires careful handling\n",
    "\n",
    "### üîß **Professional File Strategy:**\n",
    "- **Streaming Processing**: Handle large files without loading into memory\n",
    "- **Atomic Operations**: Prevent file corruption in multi-process environments\n",
    "- **Comprehensive Validation**: Verify file integrity and format compliance\n",
    "- **Error Recovery**: Graceful handling of permission, space, and network issues\n",
    "- **Performance Optimization**: Efficient I/O for large-scale automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a11160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Professional VLSI File Operations Manager\n",
    "========================================\n",
    "\n",
    "Production-grade file handling for VLSI design automation including:\n",
    "- Large file streaming and processing\n",
    "- Multi-format configuration management\n",
    "- Atomic file operations for safety\n",
    "- Comprehensive error handling and recovery\n",
    "- Performance optimization for design flows\n",
    "\n",
    "Supported Operations:\n",
    "- Configuration file management (YAML, JSON, INI)\n",
    "- Large report processing with streaming\n",
    "- Atomic file writes for critical data\n",
    "- File locking for concurrent access\n",
    "- Backup and recovery mechanisms\n",
    "\n",
    "Author: VLSI Infrastructure Team\n",
    "Version: 2.3.0\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import csv\n",
    "import tempfile\n",
    "import shutil\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict\n",
    "\n",
    "class VLSIFileManager:\n",
    "    \"\"\"\n",
    "    Professional file operations manager for VLSI workflows\n",
    "\n",
    "    Features:\n",
    "    - Safe atomic file operations\n",
    "    - Large file streaming\n",
    "    - Multi-format configuration support\n",
    "    - File integrity verification\n",
    "    - Concurrent access management\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, workspace_dir=\"./vlsi_workspace\", backup_enabled=True):\n",
    "        \"\"\"Initialize file manager with workspace and backup settings\"\"\"\n",
    "        self.workspace_dir = Path(workspace_dir)\n",
    "        self.backup_enabled = backup_enabled\n",
    "        self.backup_dir = self.workspace_dir / \"backups\"\n",
    "\n",
    "        # Create workspace structure\n",
    "        self.workspace_dir.mkdir(parents=True, exist_ok=True)\n",
    "        if backup_enabled:\n",
    "            self.backup_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # File operation statistics\n",
    "        self.stats = {\n",
    "            'files_read': 0,\n",
    "            'files_written': 0,\n",
    "            'bytes_processed': 0,\n",
    "            'errors_encountered': 0,\n",
    "            'backup_operations': 0\n",
    "        }\n",
    "\n",
    "    def calculate_file_checksum(self, file_path):\n",
    "        \"\"\"Calculate SHA-256 checksum for file integrity verification\"\"\"\n",
    "        hash_sha256 = hashlib.sha256()\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                # Read file in chunks for memory efficiency\n",
    "                for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                    hash_sha256.update(chunk)\n",
    "            return hash_sha256.hexdigest()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Warning: Could not calculate checksum for {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_backup(self, file_path):\n",
    "        \"\"\"Create timestamped backup of important files\"\"\"\n",
    "        if not self.backup_enabled:\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            file_path = Path(file_path)\n",
    "            if not file_path.exists():\n",
    "                return None\n",
    "\n",
    "            # Create backup filename with timestamp\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            backup_name = f\"{file_path.stem}_{timestamp}{file_path.suffix}\"\n",
    "            backup_path = self.backup_dir / backup_name\n",
    "\n",
    "            # Copy file to backup location\n",
    "            shutil.copy2(file_path, backup_path)\n",
    "            self.stats['backup_operations'] += 1\n",
    "\n",
    "            print(f\"üìÇ Backup created: {backup_path}\")\n",
    "            return backup_path\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Warning: Backup failed for {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    @contextmanager\n",
    "    def atomic_write(self, file_path):\n",
    "        \"\"\"\n",
    "        Context manager for atomic file writes\n",
    "        Prevents file corruption by writing to temp file first\n",
    "        \"\"\"\n",
    "        file_path = Path(file_path)\n",
    "        temp_file = None\n",
    "\n",
    "        try:\n",
    "            # Create backup of existing file\n",
    "            if file_path.exists():\n",
    "                self.create_backup(file_path)\n",
    "\n",
    "            # Create temporary file in same directory\n",
    "            temp_fd, temp_path = tempfile.mkstemp(\n",
    "                dir=file_path.parent,\n",
    "                prefix=f\".{file_path.name}.\",\n",
    "                suffix=\".tmp\"\n",
    "            )\n",
    "            temp_file = Path(temp_path)\n",
    "\n",
    "            # Yield file object for writing\n",
    "            with os.fdopen(temp_fd, 'w', encoding='utf-8') as f:\n",
    "                yield f\n",
    "\n",
    "            # Atomically move temp file to final location\n",
    "            temp_file.replace(file_path)\n",
    "            self.stats['files_written'] += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            # Cleanup on error\n",
    "            if temp_file and temp_file.exists():\n",
    "                temp_file.unlink()\n",
    "            raise e\n",
    "\n",
    "    def save_analysis_results(self, results_data, output_file, format='json'):\n",
    "        \"\"\"\n",
    "        Save analysis results in multiple formats with validation\n",
    "\n",
    "        Args:\n",
    "            results_data: Dictionary containing analysis results\n",
    "            output_file: Output file path\n",
    "            format: Output format ('json', 'csv', 'txt')\n",
    "        \"\"\"\n",
    "        output_file = Path(output_file)\n",
    "\n",
    "        try:\n",
    "            if format.lower() == 'json':\n",
    "                with self.atomic_write(output_file) as f:\n",
    "                    json.dump(results_data, f, indent=2, default=str)\n",
    "\n",
    "            elif format.lower() == 'csv':\n",
    "                with self.atomic_write(output_file) as f:\n",
    "                    writer = csv.writer(f)\n",
    "\n",
    "                    # Write headers\n",
    "                    if isinstance(results_data, list) and results_data:\n",
    "                        headers = results_data[0].keys()\n",
    "                        writer.writerow(headers)\n",
    "\n",
    "                        # Write data rows\n",
    "                        for row in results_data:\n",
    "                            writer.writerow(row.values())\n",
    "\n",
    "            elif format.lower() == 'txt':\n",
    "                with self.atomic_write(output_file) as f:\n",
    "                    f.write(\"VLSI Analysis Results\\n\")\n",
    "                    f.write(\"=\" * 30 + \"\\n\\n\")\n",
    "                    f.write(f\"Generated: {datetime.now()}\\n\\n\")\n",
    "\n",
    "                    # Write formatted results\n",
    "                    if isinstance(results_data, dict):\n",
    "                        for key, value in results_data.items():\n",
    "                            f.write(f\"{key}: {value}\\n\")\n",
    "                    else:\n",
    "                        f.write(str(results_data))\n",
    "\n",
    "            # Verify file was written correctly\n",
    "            if output_file.exists():\n",
    "                file_size = output_file.stat().st_size\n",
    "                self.stats['bytes_processed'] += file_size\n",
    "                print(f\"‚úÖ Results saved: {output_file} ({file_size:,} bytes)\")\n",
    "\n",
    "                # Calculate and display checksum for critical files\n",
    "                if file_size > 1024:  # Files larger than 1KB\n",
    "                    checksum = self.calculate_file_checksum(output_file)\n",
    "                    if checksum:\n",
    "                        print(f\"   Checksum: {checksum[:16]}...\")\n",
    "\n",
    "                return True\n",
    "            else:\n",
    "                raise FileNotFoundError(\"File was not created successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.stats['errors_encountered'] += 1\n",
    "            print(f\"‚ùå Error saving results to {output_file}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def load_configuration(self, config_file, create_default=True):\n",
    "        \"\"\"\n",
    "        Load configuration with comprehensive error handling\n",
    "\n",
    "        Args:\n",
    "            config_file: Path to configuration file\n",
    "            create_default: Create default config if file doesn't exist\n",
    "        \"\"\"\n",
    "        config_file = Path(config_file)\n",
    "\n",
    "        # Default configuration for VLSI automation\n",
    "        default_config = {\n",
    "            \"analysis_settings\": {\n",
    "                \"target_frequency\": 500,\n",
    "                \"max_power\": 1000,\n",
    "                \"technology_node\": \"7nm\",\n",
    "                \"voltage_nominal\": 0.8,\n",
    "                \"temperature_nominal\": 25,\n",
    "                \"corners\": [\"SS\", \"TT\", \"FF\"]\n",
    "            },\n",
    "            \"file_settings\": {\n",
    "                \"report_formats\": [\"json\", \"csv\", \"html\"],\n",
    "                \"backup_retention_days\": 30,\n",
    "                \"max_file_size_mb\": 100,\n",
    "                \"compression_enabled\": False\n",
    "            },\n",
    "            \"automation_settings\": {\n",
    "                \"parallel_jobs\": 4,\n",
    "                \"timeout_minutes\": 60,\n",
    "                \"retry_attempts\": 3,\n",
    "                \"email_notifications\": False\n",
    "            },\n",
    "            \"paths\": {\n",
    "                \"eda_tools\": \"/tools/eda\",\n",
    "                \"libraries\": \"/design/libraries\",\n",
    "                \"scripts\": \"./scripts\",\n",
    "                \"results\": \"./results\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            if config_file.exists():\n",
    "                # Load existing configuration\n",
    "                with open(config_file, 'r') as f:\n",
    "                    if config_file.suffix.lower() == '.json':\n",
    "                        config = json.load(f)\n",
    "                    elif config_file.suffix.lower() in ['.yaml', '.yml']:\n",
    "                        # Would use yaml.load() if PyYAML available\n",
    "                        raise ValueError(\"YAML support requires PyYAML: pip install PyYAML\")\n",
    "                    else:\n",
    "                        # Try JSON format as fallback\n",
    "                        config = json.load(f)\n",
    "\n",
    "                # Merge with defaults (add missing keys)\n",
    "                merged_config = self._merge_configs(default_config, config)\n",
    "\n",
    "                self.stats['files_read'] += 1\n",
    "                print(f\"‚úÖ Configuration loaded: {config_file}\")\n",
    "\n",
    "                return merged_config\n",
    "\n",
    "            elif create_default:\n",
    "                # Create default configuration file\n",
    "                with self.atomic_write(config_file) as f:\n",
    "                    json.dump(default_config, f, indent=2)\n",
    "\n",
    "                print(f\"üìÑ Default configuration created: {config_file}\")\n",
    "                return default_config\n",
    "\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Configuration file not found: {config_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.stats['errors_encountered'] += 1\n",
    "            print(f\"‚ùå Error loading configuration from {config_file}: {e}\")\n",
    "            print(\"‚ÑπÔ∏è  Using default configuration\")\n",
    "            return default_config\n",
    "\n",
    "    def _merge_configs(self, default, user):\n",
    "        \"\"\"Recursively merge user config with defaults\"\"\"\n",
    "        merged = default.copy()\n",
    "\n",
    "        for key, value in user.items():\n",
    "            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):\n",
    "                merged[key] = self._merge_configs(merged[key], value)\n",
    "            else:\n",
    "                merged[key] = value\n",
    "\n",
    "        return merged\n",
    "\n",
    "    def process_large_report(self, report_file, processing_function, chunk_size=8192):\n",
    "        \"\"\"\n",
    "        Process large files using streaming to minimize memory usage\n",
    "\n",
    "        Args:\n",
    "            report_file: Path to large report file\n",
    "            processing_function: Function to process each chunk/line\n",
    "            chunk_size: Size of chunks to read (bytes)\n",
    "        \"\"\"\n",
    "        report_file = Path(report_file)\n",
    "        processed_lines = 0\n",
    "\n",
    "        try:\n",
    "            file_size = report_file.stat().st_size\n",
    "            print(f\"üìä Processing large file: {report_file} ({file_size:,} bytes)\")\n",
    "\n",
    "            with open(report_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                buffer = \"\"\n",
    "\n",
    "                while True:\n",
    "                    chunk = f.read(chunk_size)\n",
    "                    if not chunk:\n",
    "                        break\n",
    "\n",
    "                    buffer += chunk\n",
    "\n",
    "                    # Process complete lines\n",
    "                    while '\\n' in buffer:\n",
    "                        line, buffer = buffer.split('\\n', 1)\n",
    "\n",
    "                        # Call processing function for each line\n",
    "                        try:\n",
    "                            processing_function(line, processed_lines)\n",
    "                            processed_lines += 1\n",
    "\n",
    "                            # Progress indicator for large files\n",
    "                            if processed_lines % 10000 == 0:\n",
    "                                current_pos = f.tell()\n",
    "                                progress = (current_pos / file_size) * 100\n",
    "                                print(f\"   Progress: {progress:.1f}% ({processed_lines:,} lines)\")\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"‚ö†Ô∏è  Error processing line {processed_lines}: {e}\")\n",
    "\n",
    "                # Process remaining buffer\n",
    "                if buffer.strip():\n",
    "                    processing_function(buffer, processed_lines)\n",
    "                    processed_lines += 1\n",
    "\n",
    "            self.stats['files_read'] += 1\n",
    "            self.stats['bytes_processed'] += file_size\n",
    "            print(f\"‚úÖ Large file processing complete: {processed_lines:,} lines processed\")\n",
    "\n",
    "            return processed_lines\n",
    "\n",
    "        except Exception as e:\n",
    "            self.stats['errors_encountered'] += 1\n",
    "            print(f\"‚ùå Error processing large file {report_file}: {e}\")\n",
    "            return 0\n",
    "\n",
    "    def get_file_stats(self):\n",
    "        \"\"\"Return file operation statistics\"\"\"\n",
    "        return self.stats.copy()\n",
    "\n",
    "# =============================================================================\n",
    "# DEMONSTRATION: PROFESSIONAL FILE OPERATIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìÅ PROFESSIONAL VLSI FILE OPERATIONS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create file manager instance\n",
    "file_manager = VLSIFileManager(workspace_dir=\"./demo_workspace\")\n",
    "\n",
    "print(f\"üìÇ Workspace: {file_manager.workspace_dir}\")\n",
    "print(f\"üíæ Backup enabled: {file_manager.backup_enabled}\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION MANAGEMENT DEMONSTRATION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nüîß CONFIGURATION MANAGEMENT:\")\n",
    "\n",
    "# Load configuration (will create default if doesn't exist)\n",
    "config = file_manager.load_configuration(\"vlsi_automation_config.json\")\n",
    "\n",
    "print(f\"   Technology: {config['analysis_settings']['technology_node']}\")\n",
    "print(f\"   Target frequency: {config['analysis_settings']['target_frequency']} MHz\")\n",
    "print(f\"   Corners: {', '.join(config['analysis_settings']['corners'])}\")\n",
    "print(f\"   Parallel jobs: {config['automation_settings']['parallel_jobs']}\")\n",
    "\n",
    "# =============================================================================\n",
    "# RESULTS SAVING DEMONSTRATION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nüíæ RESULTS SAVING DEMONSTRATION:\")\n",
    "\n",
    "# Create sample analysis results\n",
    "analysis_results = {\n",
    "    \"analysis_info\": {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"technology\": config['analysis_settings']['technology_node'],\n",
    "        \"design_name\": \"cpu_core_v2.1\"\n",
    "    },\n",
    "    \"power_analysis\": {\n",
    "        \"total_power\": 1245.6,\n",
    "        \"dynamic_power\": 892.3,\n",
    "        \"leakage_power\": 353.3,\n",
    "        \"power_density\": 124.5,\n",
    "        \"efficiency\": 2.24\n",
    "    },\n",
    "    \"timing_analysis\": {\n",
    "        \"worst_slack\": -0.156,\n",
    "        \"total_paths\": 8547,\n",
    "        \"violated_paths\": 234,\n",
    "        \"violation_rate\": 2.74\n",
    "    },\n",
    "    \"optimization_recommendations\": [\n",
    "        \"Apply clock gating to reduce dynamic power\",\n",
    "        \"Use high-Vt cells for non-critical paths\",\n",
    "        \"Optimize placement for better timing\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save in multiple formats\n",
    "formats = ['json', 'txt']\n",
    "for fmt in formats:\n",
    "    filename = f\"analysis_results.{fmt}\"\n",
    "    success = file_manager.save_analysis_results(analysis_results, filename, fmt)\n",
    "    if success:\n",
    "        print(f\"   ‚úÖ Saved: {filename}\")\n",
    "\n",
    "# =============================================================================\n",
    "# LARGE FILE PROCESSING DEMONSTRATION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nüìä LARGE FILE PROCESSING DEMONSTRATION:\")\n",
    "\n",
    "# Create a simulated large report file\n",
    "large_report_file = \"large_timing_report.txt\"\n",
    "\n",
    "# Generate large report content\n",
    "print(\"   Creating simulated large timing report...\")\n",
    "with open(large_report_file, 'w') as f:\n",
    "    f.write(\"# Large Timing Report Simulation\\n\")\n",
    "    f.write(\"# Generated for streaming processing demonstration\\n\\n\")\n",
    "\n",
    "    # Simulate 50,000 timing paths\n",
    "    for i in range(50000):\n",
    "        slack = -0.5 + (i * 0.001)  # Gradually improving slack\n",
    "        f.write(f\"Path {i+1}: startpoint=reg_{i}_q endpoint=reg_{i+100}_d slack={slack:.3f}ns\\n\")\n",
    "\n",
    "    f.write(\"\\n# End of report\\n\")\n",
    "\n",
    "# Define processing function for timing paths\n",
    "timing_violations = []\n",
    "\n",
    "def process_timing_line(line, line_number):\n",
    "    \"\"\"Process each line of timing report\"\"\"\n",
    "    global timing_violations\n",
    "\n",
    "    if \"Path\" in line and \"slack=\" in line:\n",
    "        # Extract slack value\n",
    "        try:\n",
    "            slack_start = line.find(\"slack=\") + 6\n",
    "            slack_end = line.find(\"ns\", slack_start)\n",
    "            slack_str = line[slack_start:slack_end]\n",
    "            slack = float(slack_str)\n",
    "\n",
    "            # Collect violations\n",
    "            if slack < 0:\n",
    "                timing_violations.append({\n",
    "                    'line_number': line_number,\n",
    "                    'slack': slack,\n",
    "                    'path_info': line.strip()\n",
    "                })\n",
    "        except:\n",
    "            pass  # Skip malformed lines\n",
    "\n",
    "# Process the large file\n",
    "lines_processed = file_manager.process_large_report(\n",
    "    large_report_file,\n",
    "    process_timing_line,\n",
    "    chunk_size=4096\n",
    ")\n",
    "\n",
    "print(f\"   üìà Found {len(timing_violations)} timing violations\")\n",
    "if timing_violations:\n",
    "    worst_violation = min(timing_violations, key=lambda x: x['slack'])\n",
    "    print(f\"   ‚ö†Ô∏è  Worst violation: {worst_violation['slack']:.3f} ns\")\n",
    "\n",
    "# =============================================================================\n",
    "# FILE STATISTICS AND CLEANUP\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nüìà FILE OPERATION STATISTICS:\")\n",
    "stats = file_manager.get_file_stats()\n",
    "for stat_name, value in stats.items():\n",
    "    print(f\"   {stat_name.replace('_', ' ').title()}: {value:,}\")\n",
    "\n",
    "# Cleanup demonstration files\n",
    "cleanup_files = [\n",
    "    \"vlsi_automation_config.json\",\n",
    "    \"analysis_results.json\",\n",
    "    \"analysis_results.txt\",\n",
    "    large_report_file\n",
    "]\n",
    "\n",
    "print(f\"\\nüßπ CLEANUP:\")\n",
    "for filename in cleanup_files:\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "        print(f\"   üóëÔ∏è  Removed: {filename}\")\n",
    "\n",
    "# Remove demo workspace\n",
    "if file_manager.workspace_dir.exists():\n",
    "    shutil.rmtree(file_manager.workspace_dir)\n",
    "    print(f\"   üóëÔ∏è  Removed workspace: {file_manager.workspace_dir}\")\n",
    "\n",
    "print(f\"\\nüèÜ PROFESSIONAL FILE OPERATIONS BENEFITS:\")\n",
    "print(\"‚úÖ **Atomic Writes**: Prevent file corruption in automation flows\")\n",
    "print(\"‚úÖ **Large File Handling**: Process GB-sized reports efficiently\")\n",
    "print(\"‚úÖ **Backup Management**: Automatic backup of critical design data\")\n",
    "print(\"‚úÖ **Multi-format Support**: JSON, CSV, YAML configuration files\")\n",
    "print(\"‚úÖ **Error Recovery**: Robust handling of file system issues\")\n",
    "print(\"‚úÖ **Integrity Verification**: Checksums ensure data reliability\")\n",
    "print(\"‚úÖ **Performance Optimization**: Streaming I/O for massive datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2230689",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è Production-Grade Error Handling for VLSI Automation\n",
    "\n",
    "Error handling is critical in VLSI automation where scripts run unattended for hours and process mission-critical design data. Professional error handling ensures robust operation, meaningful diagnostics, and graceful recovery.\n",
    "\n",
    "### üéØ **VLSI Automation Error Scenarios:**\n",
    "- **File System Issues**: Network storage failures, permission problems, disk space\n",
    "- **Data Format Problems**: Corrupted reports, unexpected EDA tool output formats\n",
    "- **Resource Constraints**: Memory exhaustion with large designs, CPU timeouts\n",
    "- **Tool Integration**: EDA tool crashes, license server failures, version mismatches\n",
    "- **Design Issues**: Constraint violations, convergence failures, physical design problems\n",
    "\n",
    "### üîß **Professional Error Handling Strategy:**\n",
    "- **Comprehensive Exception Hierarchy**: Specific exception types for different error categories\n",
    "- **Graceful Degradation**: Continue operation with reduced functionality when possible\n",
    "- **Detailed Logging**: Capture context, stack traces, and recovery actions\n",
    "- **User-Friendly Messages**: Clear explanations and suggested solutions\n",
    "- **Automated Recovery**: Retry mechanisms and fallback strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d355c66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Professional VLSI Error Handling Framework\n",
    "==========================================\n",
    "\n",
    "Production-grade error handling system for VLSI design automation\n",
    "providing comprehensive exception management, recovery strategies,\n",
    "and detailed diagnostics for complex design flows.\n",
    "\n",
    "Features:\n",
    "- Custom exception hierarchy for VLSI-specific errors\n",
    "- Automatic retry mechanisms with exponential backoff\n",
    "- Comprehensive logging with context preservation\n",
    "- User-friendly error messages with solutions\n",
    "- Graceful degradation strategies\n",
    "- Integration with monitoring systems\n",
    "\n",
    "Author: VLSI Reliability Team\n",
    "Version: 1.5.0\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from functools import wraps\n",
    "from enum import Enum\n",
    "\n",
    "# =============================================================================\n",
    "# CUSTOM EXCEPTION HIERARCHY FOR VLSI AUTOMATION\n",
    "# =============================================================================\n",
    "\n",
    "class VLSIErrorSeverity(Enum):\n",
    "    \"\"\"Error severity levels for VLSI automation\"\"\"\n",
    "    INFO = \"INFO\"\n",
    "    WARNING = \"WARNING\"\n",
    "    ERROR = \"ERROR\"\n",
    "    CRITICAL = \"CRITICAL\"\n",
    "    FATAL = \"FATAL\"\n",
    "\n",
    "class VLSIBaseException(Exception):\n",
    "    \"\"\"Base exception for all VLSI automation errors\"\"\"\n",
    "\n",
    "    def __init__(self, message, severity=VLSIErrorSeverity.ERROR, context=None, solution=None):\n",
    "        self.message = message\n",
    "        self.severity = severity\n",
    "        self.context = context or {}\n",
    "        self.solution = solution\n",
    "        self.timestamp = datetime.now()\n",
    "        super().__init__(self.message)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"[{self.severity.value}] {self.message}\"\n",
    "\n",
    "    def get_detailed_info(self):\n",
    "        \"\"\"Get comprehensive error information\"\"\"\n",
    "        info = {\n",
    "            'error_type': self.__class__.__name__,\n",
    "            'message': self.message,\n",
    "            'severity': self.severity.value,\n",
    "            'timestamp': self.timestamp.isoformat(),\n",
    "            'context': self.context,\n",
    "            'solution': self.solution\n",
    "        }\n",
    "        return info\n",
    "\n",
    "class VLSIFileError(VLSIBaseException):\n",
    "    \"\"\"File operation errors in VLSI automation\"\"\"\n",
    "    pass\n",
    "\n",
    "class VLSIDataError(VLSIBaseException):\n",
    "    \"\"\"Data format and parsing errors\"\"\"\n",
    "    pass\n",
    "\n",
    "class VLSIToolError(VLSIBaseException):\n",
    "    \"\"\"EDA tool integration errors\"\"\"\n",
    "    pass\n",
    "\n",
    "class VLSIResourceError(VLSIBaseException):\n",
    "    \"\"\"Resource constraint errors (memory, disk, CPU)\"\"\"\n",
    "    pass\n",
    "\n",
    "class VLSIConstraintError(VLSIBaseException):\n",
    "    \"\"\"Design constraint and timing errors\"\"\"\n",
    "    pass\n",
    "\n",
    "class VLSILicenseError(VLSIBaseException):\n",
    "    \"\"\"EDA tool license errors\"\"\"\n",
    "    pass\n",
    "\n",
    "# =============================================================================\n",
    "# PROFESSIONAL ERROR HANDLING UTILITIES\n",
    "# =============================================================================\n",
    "\n",
    "class VLSIErrorHandler:\n",
    "    \"\"\"\n",
    "    Professional error handling manager for VLSI automation\n",
    "\n",
    "    Provides:\n",
    "    - Centralized error logging and reporting\n",
    "    - Automatic retry mechanisms\n",
    "    - Error recovery strategies\n",
    "    - User notification systems\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, log_file=\"vlsi_automation.log\", enable_notifications=False):\n",
    "        \"\"\"Initialize error handler with logging and notification settings\"\"\"\n",
    "        self.log_file = Path(log_file)\n",
    "        self.enable_notifications = enable_notifications\n",
    "        self.error_counts = {severity: 0 for severity in VLSIErrorSeverity}\n",
    "        self.recovery_attempts = {}\n",
    "\n",
    "        # Setup logging\n",
    "        self._setup_logging()\n",
    "\n",
    "    def _setup_logging(self):\n",
    "        \"\"\"Configure comprehensive logging system\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(self.log_file),\n",
    "                logging.StreamHandler(sys.stdout)\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(\"VLSIAutomation\")\n",
    "\n",
    "    def handle_error(self, error, operation=\"unknown\", auto_recover=True):\n",
    "        \"\"\"\n",
    "        Comprehensive error handling with logging and recovery\n",
    "\n",
    "        Args:\n",
    "            error: Exception object or error message\n",
    "            operation: Description of operation that failed\n",
    "            auto_recover: Whether to attempt automatic recovery\n",
    "        \"\"\"\n",
    "        # Convert string errors to VLSIBaseException\n",
    "        if isinstance(error, str):\n",
    "            error = VLSIBaseException(error)\n",
    "\n",
    "        # Update error statistics\n",
    "        if hasattr(error, 'severity'):\n",
    "            self.error_counts[error.severity] += 1\n",
    "        else:\n",
    "            self.error_counts[VLSIErrorSeverity.ERROR] += 1\n",
    "\n",
    "        # Log error with full context\n",
    "        error_info = error.get_detailed_info() if hasattr(error, 'get_detailed_info') else {\n",
    "            'error_type': type(error).__name__,\n",
    "            'message': str(error),\n",
    "            'operation': operation\n",
    "        }\n",
    "\n",
    "        self.logger.error(f\"Operation failed: {operation}\")\n",
    "        self.logger.error(f\"Error details: {error_info}\")\n",
    "\n",
    "        # Log stack trace for debugging\n",
    "        if hasattr(error, '__traceback__'):\n",
    "            self.logger.debug(\"Stack trace:\", exc_info=True)\n",
    "\n",
    "        # Attempt recovery if enabled\n",
    "        if auto_recover and hasattr(error, 'solution'):\n",
    "            self.logger.info(f\"Attempting recovery: {error.solution}\")\n",
    "            return self._attempt_recovery(error, operation)\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _attempt_recovery(self, error, operation):\n",
    "        \"\"\"Attempt automatic error recovery\"\"\"\n",
    "        recovery_key = f\"{operation}_{type(error).__name__}\"\n",
    "\n",
    "        # Track recovery attempts\n",
    "        if recovery_key not in self.recovery_attempts:\n",
    "            self.recovery_attempts[recovery_key] = 0\n",
    "\n",
    "        self.recovery_attempts[recovery_key] += 1\n",
    "        max_attempts = 3\n",
    "\n",
    "        if self.recovery_attempts[recovery_key] > max_attempts:\n",
    "            self.logger.error(f\"Max recovery attempts exceeded for {operation}\")\n",
    "            return False\n",
    "\n",
    "        self.logger.info(f\"Recovery attempt {self.recovery_attempts[recovery_key]}/{max_attempts}\")\n",
    "        return True\n",
    "\n",
    "    def get_error_summary(self):\n",
    "        \"\"\"Get summary of all errors encountered\"\"\"\n",
    "        total_errors = sum(self.error_counts.values())\n",
    "        summary = {\n",
    "            'total_errors': total_errors,\n",
    "            'by_severity': dict(self.error_counts),\n",
    "            'recovery_attempts': dict(self.recovery_attempts)\n",
    "        }\n",
    "        return summary\n",
    "\n",
    "def retry_on_failure(max_attempts=3, delay=1.0, backoff=2.0, exceptions=(Exception,)):\n",
    "    \"\"\"\n",
    "    Decorator for automatic retry with exponential backoff\n",
    "\n",
    "    Args:\n",
    "        max_attempts: Maximum retry attempts\n",
    "        delay: Initial delay between retries (seconds)\n",
    "        backoff: Backoff multiplier for delay\n",
    "        exceptions: Exception types to retry on\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            attempt = 1\n",
    "            current_delay = delay\n",
    "\n",
    "            while attempt <= max_attempts:\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "\n",
    "                except exceptions as e:\n",
    "                    if attempt == max_attempts:\n",
    "                        # Last attempt failed, re-raise exception\n",
    "                        raise e\n",
    "\n",
    "                    print(f\"‚ö†Ô∏è  Attempt {attempt} failed: {e}\")\n",
    "                    print(f\"   Retrying in {current_delay:.1f} seconds...\")\n",
    "\n",
    "                    time.sleep(current_delay)\n",
    "                    attempt += 1\n",
    "                    current_delay *= backoff\n",
    "\n",
    "            return None  # Should never reach here\n",
    "\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "def safe_divide(numerator, denominator, operation_name=\"division\", default=None):\n",
    "    \"\"\"\n",
    "    Safely perform division with comprehensive error handling\n",
    "\n",
    "    Args:\n",
    "        numerator: Dividend value\n",
    "        denominator: Divisor value\n",
    "        operation_name: Description for error reporting\n",
    "        default: Default value to return on error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if denominator == 0:\n",
    "            raise VLSIDataError(\n",
    "                message=f\"Division by zero in {operation_name}\",\n",
    "                severity=VLSIErrorSeverity.ERROR,\n",
    "                context={'numerator': numerator, 'denominator': denominator},\n",
    "                solution=\"Check input data for zero values before calculation\"\n",
    "            )\n",
    "\n",
    "        if not isinstance(numerator, (int, float)) or not isinstance(denominator, (int, float)):\n",
    "            raise VLSIDataError(\n",
    "                message=f\"Non-numeric values in {operation_name}\",\n",
    "                severity=VLSIErrorSeverity.ERROR,\n",
    "                context={'numerator': numerator, 'denominator': denominator},\n",
    "                solution=\"Validate input data types before mathematical operations\"\n",
    "            )\n",
    "\n",
    "        result = numerator / denominator\n",
    "\n",
    "        # Check for overflow or underflow\n",
    "        if abs(result) > 1e15:\n",
    "            raise VLSIDataError(\n",
    "                message=f\"Numerical overflow in {operation_name}\",\n",
    "                severity=VLSIErrorSeverity.WARNING,\n",
    "                context={'result': result},\n",
    "                solution=\"Use smaller input values or different calculation method\"\n",
    "            )\n",
    "\n",
    "        return result\n",
    "\n",
    "    except VLSIBaseException:\n",
    "        raise  # Re-raise VLSI-specific exceptions\n",
    "    except Exception as e:\n",
    "        raise VLSIDataError(\n",
    "            message=f\"Unexpected error in {operation_name}: {str(e)}\",\n",
    "            severity=VLSIErrorSeverity.ERROR,\n",
    "            context={'original_error': str(e)},\n",
    "            solution=\"Check input data format and calculation logic\"\n",
    "        )\n",
    "\n",
    "def validate_timing_data(clock_period, path_delay, setup_time=0.1):\n",
    "    \"\"\"\n",
    "    Validate timing data with comprehensive error checking\n",
    "\n",
    "    Args:\n",
    "        clock_period: Clock period in nanoseconds\n",
    "        path_delay: Path delay in nanoseconds\n",
    "        setup_time: Setup time requirement in nanoseconds\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    warnings = []\n",
    "\n",
    "    try:\n",
    "        # Validate clock period\n",
    "        if clock_period <= 0:\n",
    "            errors.append(VLSIConstraintError(\n",
    "                message=\"Clock period must be positive\",\n",
    "                severity=VLSIErrorSeverity.ERROR,\n",
    "                context={'clock_period': clock_period},\n",
    "                solution=\"Check clock constraint definitions\"\n",
    "            ))\n",
    "        elif clock_period > 100:  # 10 MHz\n",
    "            warnings.append(VLSIConstraintError(\n",
    "                message=\"Very low frequency design detected\",\n",
    "                severity=VLSIErrorSeverity.WARNING,\n",
    "                context={'clock_period': clock_period, 'frequency': 1000/clock_period},\n",
    "                solution=\"Verify clock period is in correct units (ns)\"\n",
    "            ))\n",
    "\n",
    "        # Validate path delay\n",
    "        if path_delay < 0:\n",
    "            errors.append(VLSIConstraintError(\n",
    "                message=\"Path delay cannot be negative\",\n",
    "                severity=VLSIErrorSeverity.ERROR,\n",
    "                context={'path_delay': path_delay},\n",
    "                solution=\"Check timing report parsing logic\"\n",
    "            ))\n",
    "        elif path_delay > clock_period * 3:\n",
    "            warnings.append(VLSIConstraintError(\n",
    "                message=\"Path delay much larger than clock period\",\n",
    "                severity=VLSIErrorSeverity.WARNING,\n",
    "                context={'path_delay': path_delay, 'clock_period': clock_period},\n",
    "                solution=\"Verify timing constraints and path analysis\"\n",
    "            ))\n",
    "\n",
    "        # Validate setup time\n",
    "        if setup_time < 0:\n",
    "            errors.append(VLSIConstraintError(\n",
    "                message=\"Setup time cannot be negative\",\n",
    "                severity=VLSIErrorSeverity.ERROR,\n",
    "                context={'setup_time': setup_time},\n",
    "                solution=\"Check library characterization data\"\n",
    "            ))\n",
    "\n",
    "        # Calculate timing slack\n",
    "        if not errors:  # Only if basic validation passed\n",
    "            slack = clock_period - path_delay - setup_time\n",
    "\n",
    "            if slack < -1.0:  # Very negative slack\n",
    "                errors.append(VLSIConstraintError(\n",
    "                    message=\"Severe timing violation detected\",\n",
    "                    severity=VLSIErrorSeverity.CRITICAL,\n",
    "                    context={'slack': slack, 'violation_amount': abs(slack)},\n",
    "                    solution=\"Major design changes required - consider frequency reduction or pipelining\"\n",
    "                ))\n",
    "            elif slack < 0:\n",
    "                warnings.append(VLSIConstraintError(\n",
    "                    message=\"Timing violation detected\",\n",
    "                    severity=VLSIErrorSeverity.WARNING,\n",
    "                    context={'slack': slack},\n",
    "                    solution=\"Optimize critical path or adjust constraints\"\n",
    "                ))\n",
    "\n",
    "        return {\n",
    "            'valid': len(errors) == 0,\n",
    "            'errors': errors,\n",
    "            'warnings': warnings,\n",
    "            'slack': clock_period - path_delay - setup_time if not errors else None\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'valid': False,\n",
    "            'errors': [VLSIDataError(\n",
    "                message=f\"Validation failed: {str(e)}\",\n",
    "                severity=VLSIErrorSeverity.ERROR,\n",
    "                solution=\"Check input data format and validation logic\"\n",
    "            )],\n",
    "            'warnings': [],\n",
    "            'slack': None\n",
    "        }\n",
    "\n",
    "@retry_on_failure(max_attempts=3, delay=0.5, exceptions=(VLSIFileError, VLSIResourceError))\n",
    "def safe_file_operation(file_path, operation=\"read\"):\n",
    "    \"\"\"\n",
    "    Perform file operations with automatic retry and error handling\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to file\n",
    "        operation: Type of operation (\"read\", \"write\", \"delete\")\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "\n",
    "    try:\n",
    "        if operation == \"read\":\n",
    "            if not file_path.exists():\n",
    "                raise VLSIFileError(\n",
    "                    message=f\"File not found: {file_path}\",\n",
    "                    severity=VLSIErrorSeverity.ERROR,\n",
    "                    context={'file_path': str(file_path), 'operation': operation},\n",
    "                    solution=\"Check file path and ensure file exists\"\n",
    "                )\n",
    "\n",
    "            with open(file_path, 'r') as f:\n",
    "                content = f.read()\n",
    "\n",
    "            return content\n",
    "\n",
    "        elif operation == \"write\":\n",
    "            # Check if directory exists\n",
    "            file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Test write permissions\n",
    "            test_file = file_path.with_suffix('.test')\n",
    "            try:\n",
    "                test_file.touch()\n",
    "                test_file.unlink()\n",
    "            except PermissionError:\n",
    "                raise VLSIFileError(\n",
    "                    message=f\"No write permission: {file_path}\",\n",
    "                    severity=VLSIErrorSeverity.ERROR,\n",
    "                    context={'file_path': str(file_path)},\n",
    "                    solution=\"Check file permissions and directory access\"\n",
    "                )\n",
    "\n",
    "            return True\n",
    "\n",
    "    except PermissionError as e:\n",
    "        raise VLSIFileError(\n",
    "            message=f\"Permission denied: {file_path}\",\n",
    "            severity=VLSIErrorSeverity.ERROR,\n",
    "            context={'file_path': str(file_path), 'system_error': str(e)},\n",
    "            solution=\"Check file permissions and user access rights\"\n",
    "        )\n",
    "    except OSError as e:\n",
    "        raise VLSIResourceError(\n",
    "            message=f\"System resource error: {str(e)}\",\n",
    "            severity=VLSIErrorSeverity.ERROR,\n",
    "            context={'file_path': str(file_path), 'system_error': str(e)},\n",
    "            solution=\"Check disk space and system resources\"\n",
    "        )\n",
    "\n",
    "# =============================================================================\n",
    "# DEMONSTRATION: COMPREHENSIVE ERROR HANDLING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üõ°Ô∏è PROFESSIONAL VLSI ERROR HANDLING\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create error handler instance\n",
    "error_handler = VLSIErrorHandler(log_file=\"demo_vlsi_errors.log\")\n",
    "\n",
    "print(\"üîß Testing error handling scenarios...\")\n",
    "\n",
    "# =============================================================================\n",
    "# TEST 1: SAFE MATHEMATICAL OPERATIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nüßÆ SAFE MATHEMATICAL OPERATIONS:\")\n",
    "\n",
    "test_cases = [\n",
    "    (1000, 2.5, \"frequency calculation\"),\n",
    "    (100, 0, \"power efficiency\"),  # Division by zero\n",
    "    (500, \"invalid\", \"timing ratio\"),  # Invalid data type\n",
    "    (1e20, 1e-20, \"extreme values\"),  # Numerical overflow\n",
    "]\n",
    "\n",
    "for numerator, denominator, operation in test_cases:\n",
    "    try:\n",
    "        result = safe_divide(numerator, denominator, operation)\n",
    "        print(f\"   ‚úÖ {operation}: {numerator} / {denominator} = {result:.3f}\")\n",
    "    except VLSIBaseException as e:\n",
    "        error_handler.handle_error(e, operation)\n",
    "        print(f\"   ‚ùå {operation}: {e}\")\n",
    "        if e.solution:\n",
    "            print(f\"      üí° Solution: {e.solution}\")\n",
    "\n",
    "# =============================================================================\n",
    "# TEST 2: TIMING DATA VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è TIMING DATA VALIDATION:\")\n",
    "\n",
    "timing_test_cases = [\n",
    "    (2.5, 2.1, 0.1, \"Normal timing\"),\n",
    "    (-1.0, 2.0, 0.1, \"Invalid clock period\"),\n",
    "    (2.5, -0.5, 0.1, \"Negative path delay\"),\n",
    "    (2.5, 8.0, 0.1, \"Excessive path delay\"),\n",
    "    (2.5, 3.0, 0.1, \"Timing violation\"),\n",
    "]\n",
    "\n",
    "for clock_period, path_delay, setup_time, description in timing_test_cases:\n",
    "    print(f\"\\n   Testing: {description}\")\n",
    "    result = validate_timing_data(clock_period, path_delay, setup_time)\n",
    "\n",
    "    if result['valid']:\n",
    "        print(f\"      ‚úÖ Valid - Slack: {result['slack']:.3f} ns\")\n",
    "    else:\n",
    "        print(f\"      ‚ùå Invalid\")\n",
    "        for error in result['errors']:\n",
    "            print(f\"         Error: {error.message}\")\n",
    "            if error.solution:\n",
    "                print(f\"         Solution: {error.solution}\")\n",
    "\n",
    "    for warning in result['warnings']:\n",
    "        print(f\"      ‚ö†Ô∏è  Warning: {warning.message}\")\n",
    "\n",
    "# =============================================================================\n",
    "# TEST 3: FILE OPERATIONS WITH RETRY\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nüìÅ FILE OPERATIONS WITH RETRY:\")\n",
    "\n",
    "file_test_cases = [\n",
    "    (\"existing_file.txt\", \"read\"),\n",
    "    (\"nonexistent_file.txt\", \"read\"),\n",
    "    (\"./test_output.txt\", \"write\"),\n",
    "]\n",
    "\n",
    "# Create a test file\n",
    "with open(\"existing_file.txt\", \"w\") as f:\n",
    "    f.write(\"Test content for demonstration\")\n",
    "\n",
    "for file_path, operation in file_test_cases:\n",
    "    try:\n",
    "        result = safe_file_operation(file_path, operation)\n",
    "        if operation == \"read\":\n",
    "            print(f\"   ‚úÖ Read {file_path}: {len(result)} characters\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ {operation.title()} operation on {file_path}: Success\")\n",
    "    except VLSIBaseException as e:\n",
    "        error_handler.handle_error(e, f\"{operation} {file_path}\")\n",
    "        print(f\"   ‚ùå {operation.title()} {file_path}: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ERROR SUMMARY AND CLEANUP\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nüìä ERROR HANDLING SUMMARY:\")\n",
    "summary = error_handler.get_error_summary()\n",
    "print(f\"   Total errors handled: {summary['total_errors']}\")\n",
    "print(f\"   By severity:\")\n",
    "for severity, count in summary['by_severity'].items():\n",
    "    if count > 0:\n",
    "        print(f\"      {severity.value}: {count}\")\n",
    "\n",
    "print(f\"   Recovery attempts: {len(summary['recovery_attempts'])}\")\n",
    "\n",
    "# Cleanup test files\n",
    "cleanup_files = [\"existing_file.txt\", \"demo_vlsi_errors.log\"]\n",
    "for filename in cleanup_files:\n",
    "    if Path(filename).exists():\n",
    "        Path(filename).unlink()\n",
    "        print(f\"   üóëÔ∏è  Cleaned up: {filename}\")\n",
    "\n",
    "print(f\"\\nüèÜ PROFESSIONAL ERROR HANDLING BENEFITS:\")\n",
    "print(\"‚úÖ **Comprehensive Exception Hierarchy**: Specific error types for different VLSI scenarios\")\n",
    "print(\"‚úÖ **Automatic Retry Mechanisms**: Resilient operation in unstable environments\")\n",
    "print(\"‚úÖ **Detailed Error Context**: Full context preservation for debugging\")\n",
    "print(\"‚úÖ **User-Friendly Messages**: Clear explanations with actionable solutions\")\n",
    "print(\"‚úÖ **Graceful Degradation**: Continue operation when possible\")\n",
    "print(\"‚úÖ **Professional Logging**: Complete audit trail for production environments\")\n",
    "print(\"‚úÖ **Recovery Strategies**: Intelligent error recovery and fallback mechanisms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4da38d",
   "metadata": {},
   "source": [
    "## ü§ñ Complete VLSI Automation Framework: Production-Ready Template\n",
    "\n",
    "Now let's bring everything together into a comprehensive automation framework that demonstrates all the professional practices we've covered. This is a production-ready template that you can adapt for your specific VLSI automation needs.\n",
    "\n",
    "### üéØ **Framework Features:**\n",
    "- **Modular Architecture**: Separate classes for different automation aspects\n",
    "- **Configuration Management**: YAML/JSON configuration with validation\n",
    "- **Comprehensive Logging**: Multi-level logging with file and console output\n",
    "- **Error Recovery**: Robust error handling with automatic retry mechanisms\n",
    "- **Performance Monitoring**: Execution time tracking and resource usage\n",
    "- **Report Generation**: Professional HTML/PDF reports with charts\n",
    "- **Integration Ready**: Hooks for EDA tool integration and team workflows\n",
    "\n",
    "### üèóÔ∏è **Real-World Application:**\n",
    "This framework template is designed for production VLSI environments and includes patterns commonly used in:\n",
    "- **Synthesis Automation**: Multi-corner synthesis with QoR tracking\n",
    "- **Physical Design Flows**: Place & route automation with DRC/LVS checking\n",
    "- **Verification Flows**: Regression testing and coverage analysis\n",
    "- **Sign-off Flows**: Timing, power, and area closure automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b00fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Enterprise VLSI Design Automation Framework\n",
    "===========================================\n",
    "\n",
    "Production-ready automation framework for VLSI design flows incorporating\n",
    "industry best practices for large-scale chip development environments.\n",
    "\n",
    "Framework Capabilities:\n",
    "- Multi-tool EDA integration (Synopsys, Cadence, Mentor)\n",
    "- Scalable parallel processing for large designs\n",
    "- Advanced error handling with recovery strategies\n",
    "- Comprehensive logging and monitoring\n",
    "- Professional report generation\n",
    "- Team collaboration features\n",
    "- Regression testing automation\n",
    "- Performance optimization tracking\n",
    "\n",
    "Target Applications:\n",
    "- Complete RTL-to-GDS automation flows\n",
    "- Multi-corner timing and power closure\n",
    "- Design space exploration and optimization\n",
    "- Regression testing and validation\n",
    "- Sign-off quality metrics tracking\n",
    "\n",
    "Author: Enterprise VLSI Team\n",
    "Version: 5.2.0\n",
    "License: Proprietary - Internal Use Only\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import threading\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import defaultdict, namedtuple\n",
    "\n",
    "# =============================================================================\n",
    "# ENTERPRISE DATA STRUCTURES\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class DesignConfiguration:\n",
    "    \"\"\"Complete design configuration for automation flows\"\"\"\n",
    "    design_name: str\n",
    "    technology_node: str\n",
    "    target_frequency: float  # MHz\n",
    "    target_power: float      # mW\n",
    "    target_area: float       # mm¬≤\n",
    "    voltage_corners: List[str]\n",
    "    temperature_corners: List[int]\n",
    "    process_corners: List[str]\n",
    "    optimization_goals: Dict[str, float]\n",
    "    constraints_file: str\n",
    "    rtl_files: List[str]\n",
    "    library_path: str\n",
    "\n",
    "@dataclass\n",
    "class AnalysisResults:\n",
    "    \"\"\"Comprehensive analysis results structure\"\"\"\n",
    "    design_name: str\n",
    "    timestamp: str\n",
    "    corner: str\n",
    "    metrics: Dict[str, float]\n",
    "    violations: List[Dict[str, Any]]\n",
    "    warnings: List[str]\n",
    "    execution_time: float\n",
    "    memory_usage: float\n",
    "    tool_versions: Dict[str, str]\n",
    "    file_checksums: Dict[str, str]\n",
    "\n",
    "@dataclass\n",
    "class OptimizationRecommendation:\n",
    "    \"\"\"Optimization recommendation with priority and impact\"\"\"\n",
    "    category: str\n",
    "    priority: str  # HIGH, MEDIUM, LOW\n",
    "    description: str\n",
    "    expected_improvement: Dict[str, float]\n",
    "    implementation_effort: str\n",
    "    risk_level: str\n",
    "\n",
    "# Named tuple for performance tracking\n",
    "PerformanceMetric = namedtuple('PerformanceMetric', [\n",
    "    'operation', 'start_time', 'end_time', 'duration',\n",
    "    'memory_before', 'memory_after', 'success'\n",
    "])\n",
    "\n",
    "# =============================================================================\n",
    "# ENTERPRISE AUTOMATION FRAMEWORK\n",
    "# =============================================================================\n",
    "\n",
    "class VLSIAutomationFramework:\n",
    "    \"\"\"\n",
    "    Enterprise-grade VLSI automation framework\n",
    "\n",
    "    Provides comprehensive automation capabilities for large-scale\n",
    "    VLSI design projects with focus on reliability, scalability,\n",
    "    and maintainability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config_file: str = \"vlsi_automation_config.json\"):\n",
    "        \"\"\"Initialize automation framework with enterprise configuration\"\"\"\n",
    "\n",
    "        # Framework metadata\n",
    "        self.framework_version = \"5.2.0\"\n",
    "        self.start_time = datetime.now()\n",
    "        self.session_id = f\"vlsi_session_{int(time.time())}\"\n",
    "\n",
    "        # Configuration and state\n",
    "        self.config_file = Path(config_file)\n",
    "        self.configuration = {}\n",
    "        self.design_configs = {}\n",
    "        self.analysis_results = []\n",
    "        self.performance_metrics = []\n",
    "        self.active_operations = {}\n",
    "\n",
    "        # Thread safety\n",
    "        self.results_lock = threading.Lock()\n",
    "        self.metrics_lock = threading.Lock()\n",
    "\n",
    "        # Setup framework\n",
    "        self._initialize_framework()\n",
    "\n",
    "    def _initialize_framework(self):\n",
    "        \"\"\"Initialize framework components and configuration\"\"\"\n",
    "\n",
    "        # Setup logging\n",
    "        self._setup_enterprise_logging()\n",
    "\n",
    "        # Load configuration\n",
    "        self._load_enterprise_configuration()\n",
    "\n",
    "        # Initialize workspace\n",
    "        self._setup_workspace()\n",
    "\n",
    "        # Setup monitoring\n",
    "        self._initialize_monitoring()\n",
    "\n",
    "        self.logger.info(f\"VLSI Automation Framework v{self.framework_version} initialized\")\n",
    "        self.logger.info(f\"Session ID: {self.session_id}\")\n",
    "\n",
    "    def _setup_enterprise_logging(self):\n",
    "        \"\"\"Setup comprehensive logging system\"\"\"\n",
    "\n",
    "        # Create logs directory\n",
    "        log_dir = Path(\"./logs\")\n",
    "        log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # Setup multiple log files\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "        # Configure main logger\n",
    "        self.logger = logging.getLogger(\"VLSIFramework\")\n",
    "        self.logger.setLevel(logging.DEBUG)\n",
    "\n",
    "        # Remove existing handlers\n",
    "        for handler in self.logger.handlers[:]:\n",
    "            self.logger.removeHandler(handler)\n",
    "\n",
    "        # Console handler (INFO and above)\n",
    "        console_handler = logging.StreamHandler(sys.stdout)\n",
    "        console_handler.setLevel(logging.INFO)\n",
    "        console_format = logging.Formatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        console_handler.setFormatter(console_format)\n",
    "\n",
    "        # File handler (DEBUG and above)\n",
    "        file_handler = logging.FileHandler(\n",
    "            log_dir / f\"vlsi_automation_{timestamp}.log\"\n",
    "        )\n",
    "        file_handler.setLevel(logging.DEBUG)\n",
    "        file_format = logging.Formatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'\n",
    "        )\n",
    "        file_handler.setFormatter(file_format)\n",
    "\n",
    "        # Error handler (ERROR and above)\n",
    "        error_handler = logging.FileHandler(\n",
    "            log_dir / f\"vlsi_errors_{timestamp}.log\"\n",
    "        )\n",
    "        error_handler.setLevel(logging.ERROR)\n",
    "        error_handler.setFormatter(file_format)\n",
    "\n",
    "        # Add handlers\n",
    "        self.logger.addHandler(console_handler)\n",
    "        self.logger.addHandler(file_handler)\n",
    "        self.logger.addHandler(error_handler)\n",
    "\n",
    "    def _load_enterprise_configuration(self):\n",
    "        \"\"\"Load comprehensive configuration with validation\"\"\"\n",
    "\n",
    "        # Default enterprise configuration\n",
    "        default_config = {\n",
    "            \"framework_settings\": {\n",
    "                \"max_parallel_jobs\": 8,\n",
    "                \"timeout_minutes\": 120,\n",
    "                \"retry_attempts\": 3,\n",
    "                \"memory_limit_gb\": 32,\n",
    "                \"temp_directory\": \"./temp\",\n",
    "                \"results_directory\": \"./results\",\n",
    "                \"enable_monitoring\": True,\n",
    "                \"enable_backup\": True\n",
    "            },\n",
    "            \"eda_tools\": {\n",
    "                \"synthesis\": {\n",
    "                    \"tool\": \"design_compiler\",\n",
    "                    \"version\": \"2023.12\",\n",
    "                    \"license_server\": \"license_server:27000\",\n",
    "                    \"max_licenses\": 4\n",
    "                },\n",
    "                \"placement\": {\n",
    "                    \"tool\": \"innovus\",\n",
    "                    \"version\": \"21.1\",\n",
    "                    \"license_server\": \"license_server:27000\",\n",
    "                    \"max_licenses\": 2\n",
    "                },\n",
    "                \"timing\": {\n",
    "                    \"tool\": \"primetime\",\n",
    "                    \"version\": \"2023.12\",\n",
    "                    \"license_server\": \"license_server:27000\",\n",
    "                    \"max_licenses\": 4\n",
    "                }\n",
    "            },\n",
    "            \"design_defaults\": {\n",
    "                \"technology_node\": \"7nm\",\n",
    "                \"voltage_corners\": [\"0.72V\", \"0.8V\", \"0.88V\"],\n",
    "                \"temperature_corners\": [-40, 25, 125],\n",
    "                \"process_corners\": [\"SS\", \"TT\", \"FF\"],\n",
    "                \"optimization_goals\": {\n",
    "                    \"frequency\": 1.0,\n",
    "                    \"power\": 0.8,\n",
    "                    \"area\": 0.6\n",
    "                }\n",
    "            },\n",
    "            \"reporting\": {\n",
    "                \"formats\": [\"html\", \"json\", \"csv\"],\n",
    "                \"include_charts\": True,\n",
    "                \"auto_email\": False,\n",
    "                \"retention_days\": 90\n",
    "            }\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            if self.config_file.exists():\n",
    "                with open(self.config_file, 'r') as f:\n",
    "                    user_config = json.load(f)\n",
    "\n",
    "                # Merge configurations\n",
    "                self.configuration = self._deep_merge_config(default_config, user_config)\n",
    "                self.logger.info(f\"Configuration loaded from {self.config_file}\")\n",
    "            else:\n",
    "                # Create default configuration\n",
    "                self.configuration = default_config\n",
    "                with open(self.config_file, 'w') as f:\n",
    "                    json.dump(default_config, f, indent=2)\n",
    "                self.logger.info(f\"Default configuration created: {self.config_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Configuration loading failed: {e}\")\n",
    "            self.configuration = default_config\n",
    "\n",
    "    def _deep_merge_config(self, default: dict, user: dict) -> dict:\n",
    "        \"\"\"Recursively merge user configuration with defaults\"\"\"\n",
    "        merged = default.copy()\n",
    "\n",
    "        for key, value in user.items():\n",
    "            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):\n",
    "                merged[key] = self._deep_merge_config(merged[key], value)\n",
    "            else:\n",
    "                merged[key] = value\n",
    "\n",
    "        return merged\n",
    "\n",
    "    def _setup_workspace(self):\n",
    "        \"\"\"Setup enterprise workspace structure\"\"\"\n",
    "\n",
    "        workspace_dirs = [\n",
    "            \"designs\",\n",
    "            \"libraries\",\n",
    "            \"scripts\",\n",
    "            \"results\",\n",
    "            \"reports\",\n",
    "            \"temp\",\n",
    "            \"logs\",\n",
    "            \"backup\"\n",
    "        ]\n",
    "\n",
    "        for dir_name in workspace_dirs:\n",
    "            dir_path = Path(dir_name)\n",
    "            dir_path.mkdir(exist_ok=True)\n",
    "            self.logger.debug(f\"Workspace directory ready: {dir_path}\")\n",
    "\n",
    "    def _initialize_monitoring(self):\n",
    "        \"\"\"Initialize performance and resource monitoring\"\"\"\n",
    "\n",
    "        self.monitoring_enabled = self.configuration[\"framework_settings\"][\"enable_monitoring\"]\n",
    "\n",
    "        if self.monitoring_enabled:\n",
    "            # Start monitoring thread\n",
    "            self.monitoring_thread = threading.Thread(\n",
    "                target=self._monitoring_loop,\n",
    "                daemon=True\n",
    "            )\n",
    "            self.monitoring_active = True\n",
    "            self.monitoring_thread.start()\n",
    "            self.logger.info(\"Resource monitoring started\")\n",
    "\n",
    "    def _monitoring_loop(self):\n",
    "        \"\"\"Background monitoring loop for resource usage\"\"\"\n",
    "\n",
    "        while self.monitoring_active:\n",
    "            try:\n",
    "                # Monitor memory usage (simplified)\n",
    "                import psutil\n",
    "                process = psutil.Process()\n",
    "                memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "                # Log resource usage every 5 minutes\n",
    "                self.logger.debug(f\"Memory usage: {memory_mb:.1f} MB\")\n",
    "\n",
    "                time.sleep(300)  # 5 minutes\n",
    "\n",
    "            except ImportError:\n",
    "                # psutil not available, use basic monitoring\n",
    "                time.sleep(300)\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Monitoring error: {e}\")\n",
    "                time.sleep(60)\n",
    "\n",
    "    def register_design(self, design_config: DesignConfiguration) -> bool:\n",
    "        \"\"\"Register a design for automation processing\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Validate design configuration\n",
    "            validation_result = self._validate_design_config(design_config)\n",
    "\n",
    "            if not validation_result[\"valid\"]:\n",
    "                self.logger.error(f\"Design validation failed: {validation_result['errors']}\")\n",
    "                return False\n",
    "\n",
    "            # Store design configuration\n",
    "            self.design_configs[design_config.design_name] = design_config\n",
    "\n",
    "            self.logger.info(f\"Design registered: {design_config.design_name}\")\n",
    "            self.logger.info(f\"  Technology: {design_config.technology_node}\")\n",
    "            self.logger.info(f\"  Target frequency: {design_config.target_frequency} MHz\")\n",
    "            self.logger.info(f\"  Corners: {len(design_config.process_corners)} process x \" +\n",
    "                           f\"{len(design_config.voltage_corners)} voltage x \" +\n",
    "                           f\"{len(design_config.temperature_corners)} temperature\")\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Design registration failed: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _validate_design_config(self, config: DesignConfiguration) -> Dict[str, Any]:\n",
    "        \"\"\"Comprehensive design configuration validation\"\"\"\n",
    "\n",
    "        errors = []\n",
    "        warnings = []\n",
    "\n",
    "        # Validate design name\n",
    "        if not config.design_name or not config.design_name.strip():\n",
    "            errors.append(\"Design name cannot be empty\")\n",
    "\n",
    "        # Validate targets\n",
    "        if config.target_frequency <= 0:\n",
    "            errors.append(\"Target frequency must be positive\")\n",
    "        elif config.target_frequency > 5000:  # 5 GHz\n",
    "            warnings.append(\"Very high target frequency detected\")\n",
    "\n",
    "        if config.target_power <= 0:\n",
    "            errors.append(\"Target power must be positive\")\n",
    "\n",
    "        # Validate corners\n",
    "        if not config.process_corners:\n",
    "            errors.append(\"At least one process corner must be specified\")\n",
    "\n",
    "        if not config.voltage_corners:\n",
    "            errors.append(\"At least one voltage corner must be specified\")\n",
    "\n",
    "        # Validate file paths\n",
    "        if config.constraints_file and not Path(config.constraints_file).exists():\n",
    "            warnings.append(f\"Constraints file not found: {config.constraints_file}\")\n",
    "\n",
    "        for rtl_file in config.rtl_files:\n",
    "            if not Path(rtl_file).exists():\n",
    "                warnings.append(f\"RTL file not found: {rtl_file}\")\n",
    "\n",
    "        return {\n",
    "            \"valid\": len(errors) == 0,\n",
    "            \"errors\": errors,\n",
    "            \"warnings\": warnings\n",
    "        }\n",
    "\n",
    "    def run_design_analysis(self, design_name: str, analysis_type: str = \"full\") -> bool:\n",
    "        \"\"\"\n",
    "        Execute comprehensive design analysis\n",
    "\n",
    "        Args:\n",
    "            design_name: Name of registered design\n",
    "            analysis_type: Type of analysis (\"full\", \"timing\", \"power\", \"area\")\n",
    "        \"\"\"\n",
    "\n",
    "        if design_name not in self.design_configs:\n",
    "            self.logger.error(f\"Design not registered: {design_name}\")\n",
    "            return False\n",
    "\n",
    "        design_config = self.design_configs[design_name]\n",
    "        self.logger.info(f\"Starting {analysis_type} analysis for {design_name}\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            # Generate analysis matrix (all corner combinations)\n",
    "            analysis_matrix = self._generate_analysis_matrix(design_config)\n",
    "            self.logger.info(f\"Analysis matrix: {len(analysis_matrix)} corner combinations\")\n",
    "\n",
    "            # Execute parallel analysis\n",
    "            results = self._execute_parallel_analysis(design_config, analysis_matrix, analysis_type)\n",
    "\n",
    "            # Store results\n",
    "            with self.results_lock:\n",
    "                self.analysis_results.extend(results)\n",
    "\n",
    "            # Generate summary\n",
    "            self._generate_analysis_summary(design_name, results)\n",
    "\n",
    "            execution_time = time.time() - start_time\n",
    "            self.logger.info(f\"Analysis completed in {execution_time:.2f} seconds\")\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Analysis failed for {design_name}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _generate_analysis_matrix(self, config: DesignConfiguration) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Generate all corner combinations for analysis\"\"\"\n",
    "\n",
    "        matrix = []\n",
    "\n",
    "        for process in config.process_corners:\n",
    "            for voltage in config.voltage_corners:\n",
    "                for temperature in config.temperature_corners:\n",
    "                    matrix.append({\n",
    "                        \"process\": process,\n",
    "                        \"voltage\": voltage,\n",
    "                        \"temperature\": temperature,\n",
    "                        \"corner_name\": f\"{process}_{voltage}_{temperature}C\"\n",
    "                    })\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    def _execute_parallel_analysis(self, config: DesignConfiguration,\n",
    "                                 analysis_matrix: List[Dict],\n",
    "                                 analysis_type: str) -> List[AnalysisResults]:\n",
    "        \"\"\"Execute analysis in parallel across multiple corners\"\"\"\n",
    "\n",
    "        max_workers = min(\n",
    "            self.configuration[\"framework_settings\"][\"max_parallel_jobs\"],\n",
    "            len(analysis_matrix)\n",
    "        )\n",
    "\n",
    "        results = []\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit analysis jobs\n",
    "            future_to_corner = {\n",
    "                executor.submit(self._analyze_single_corner, config, corner, analysis_type): corner\n",
    "                for corner in analysis_matrix\n",
    "            }\n",
    "\n",
    "            # Collect results\n",
    "            for future in as_completed(future_to_corner):\n",
    "                corner = future_to_corner[future]\n",
    "\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results.append(result)\n",
    "                    self.logger.info(f\"Completed analysis: {corner['corner_name']}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Analysis failed for {corner['corner_name']}: {e}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _analyze_single_corner(self, config: DesignConfiguration,\n",
    "                             corner: Dict[str, Any],\n",
    "                             analysis_type: str) -> AnalysisResults:\n",
    "        \"\"\"Analyze single corner (simplified simulation)\"\"\"\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Simulate analysis delay\n",
    "        analysis_time = 2.0 + (len(config.rtl_files) * 0.1)\n",
    "        time.sleep(analysis_time)\n",
    "\n",
    "        # Generate realistic metrics based on corner\n",
    "        metrics = self._generate_corner_metrics(config, corner)\n",
    "\n",
    "        # Generate sample violations\n",
    "        violations = self._generate_sample_violations(metrics)\n",
    "\n",
    "        # Create analysis result\n",
    "        result = AnalysisResults(\n",
    "            design_name=config.design_name,\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            corner=corner[\"corner_name\"],\n",
    "            metrics=metrics,\n",
    "            violations=violations,\n",
    "            warnings=[],\n",
    "            execution_time=time.time() - start_time,\n",
    "            memory_usage=100.0,  # Simplified\n",
    "            tool_versions={\"synthesis\": \"2023.12\", \"timing\": \"2023.12\"},\n",
    "            file_checksums={}\n",
    "        )\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _generate_corner_metrics(self, config: DesignConfiguration,\n",
    "                               corner: Dict[str, Any]) -> Dict[str, float]:\n",
    "        \"\"\"Generate realistic metrics based on corner conditions\"\"\"\n",
    "\n",
    "        # Base metrics\n",
    "        base_frequency = config.target_frequency\n",
    "        base_power = config.target_power\n",
    "        base_area = config.target_area\n",
    "\n",
    "        # Corner factors\n",
    "        process_factor = {\"SS\": 0.8, \"TT\": 1.0, \"FF\": 1.2}.get(corner[\"process\"], 1.0)\n",
    "        temp_factor = 1.0 + (corner[\"temperature\"] - 25) * 0.002\n",
    "        voltage = float(corner[\"voltage\"].replace(\"V\", \"\"))\n",
    "        voltage_factor = (voltage / 0.8) ** 2\n",
    "\n",
    "        # Calculate metrics\n",
    "        actual_frequency = base_frequency * process_factor / temp_factor\n",
    "        actual_power = base_power * voltage_factor * temp_factor\n",
    "        actual_area = base_area  # Area doesn't change with corners\n",
    "\n",
    "        # Timing metrics\n",
    "        slack = (1000 / actual_frequency) - (1000 / base_frequency)  # ns\n",
    "\n",
    "        return {\n",
    "            \"frequency\": actual_frequency,\n",
    "            \"power\": actual_power,\n",
    "            \"area\": actual_area,\n",
    "            \"worst_slack\": slack,\n",
    "            \"total_paths\": 25000,\n",
    "            \"violated_paths\": max(0, int((-slack) * 1000)) if slack < 0 else 0,\n",
    "            \"leakage_power\": actual_power * 0.3,\n",
    "            \"dynamic_power\": actual_power * 0.7\n",
    "        }\n",
    "\n",
    "    def _generate_sample_violations(self, metrics: Dict[str, float]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Generate sample violations based on metrics\"\"\"\n",
    "\n",
    "        violations = []\n",
    "\n",
    "        if metrics[\"violated_paths\"] > 0:\n",
    "            violations.append({\n",
    "                \"type\": \"timing\",\n",
    "                \"severity\": \"critical\" if metrics[\"worst_slack\"] < -0.5 else \"warning\",\n",
    "                \"count\": metrics[\"violated_paths\"],\n",
    "                \"worst_case\": metrics[\"worst_slack\"],\n",
    "                \"description\": f\"Setup timing violations: {metrics['violated_paths']} paths\"\n",
    "            })\n",
    "\n",
    "        if metrics[\"power\"] > 1500:  # Arbitrary threshold\n",
    "            violations.append({\n",
    "                \"type\": \"power\",\n",
    "                \"severity\": \"warning\",\n",
    "                \"value\": metrics[\"power\"],\n",
    "                \"threshold\": 1500,\n",
    "                \"description\": f\"Power consumption exceeds target: {metrics['power']:.1f} mW\"\n",
    "            })\n",
    "\n",
    "        return violations\n",
    "\n",
    "    def _generate_analysis_summary(self, design_name: str, results: List[AnalysisResults]):\n",
    "        \"\"\"Generate comprehensive analysis summary\"\"\"\n",
    "\n",
    "        self.logger.info(f\"\\n{'='*60}\")\n",
    "        self.logger.info(f\"ANALYSIS SUMMARY: {design_name}\")\n",
    "        self.logger.info(f\"{'='*60}\")\n",
    "\n",
    "        # Overall statistics\n",
    "        total_corners = len(results)\n",
    "        passing_corners = sum(1 for r in results if not r.violations)\n",
    "\n",
    "        self.logger.info(f\"Total corners analyzed: {total_corners}\")\n",
    "        self.logger.info(f\"Passing corners: {passing_corners} ({passing_corners/total_corners*100:.1f}%)\")\n",
    "\n",
    "        # Metric ranges\n",
    "        if results:\n",
    "            frequencies = [r.metrics[\"frequency\"] for r in results]\n",
    "            powers = [r.metrics[\"power\"] for r in results]\n",
    "            slacks = [r.metrics[\"worst_slack\"] for r in results]\n",
    "\n",
    "            self.logger.info(f\"\\nMetric Ranges:\")\n",
    "            self.logger.info(f\"  Frequency: {min(frequencies):.1f} - {max(frequencies):.1f} MHz\")\n",
    "            self.logger.info(f\"  Power: {min(powers):.1f} - {max(powers):.1f} mW\")\n",
    "            self.logger.info(f\"  Worst slack: {min(slacks):.3f} - {max(slacks):.3f} ns\")\n",
    "\n",
    "        # Worst corner\n",
    "        if results:\n",
    "            worst_result = min(results, key=lambda r: r.metrics[\"worst_slack\"])\n",
    "            self.logger.info(f\"\\nWorst timing corner: {worst_result.corner}\")\n",
    "            self.logger.info(f\"  Slack: {worst_result.metrics['worst_slack']:.3f} ns\")\n",
    "            self.logger.info(f\"  Violations: {len(worst_result.violations)}\")\n",
    "\n",
    "    def generate_optimization_recommendations(self, design_name: str) -> List[OptimizationRecommendation]:\n",
    "        \"\"\"Generate intelligent optimization recommendations\"\"\"\n",
    "\n",
    "        design_results = [r for r in self.analysis_results if r.design_name == design_name]\n",
    "\n",
    "        if not design_results:\n",
    "            self.logger.warning(f\"No analysis results found for {design_name}\")\n",
    "            return []\n",
    "\n",
    "        recommendations = []\n",
    "\n",
    "        # Analyze timing issues\n",
    "        timing_violations = sum(len([v for v in r.violations if v[\"type\"] == \"timing\"])\n",
    "                              for r in design_results)\n",
    "\n",
    "        if timing_violations > 0:\n",
    "            recommendations.append(OptimizationRecommendation(\n",
    "                category=\"timing\",\n",
    "                priority=\"HIGH\",\n",
    "                description=\"Critical timing violations detected across multiple corners\",\n",
    "                expected_improvement={\"frequency\": 5.0, \"slack\": 0.1},\n",
    "                implementation_effort=\"MEDIUM\",\n",
    "                risk_level=\"LOW\"\n",
    "            ))\n",
    "\n",
    "        # Analyze power issues\n",
    "        avg_power = sum(r.metrics[\"power\"] for r in design_results) / len(design_results)\n",
    "        target_power = self.design_configs[design_name].target_power\n",
    "\n",
    "        if avg_power > target_power * 1.2:\n",
    "            recommendations.append(OptimizationRecommendation(\n",
    "                category=\"power\",\n",
    "                priority=\"MEDIUM\",\n",
    "                description=\"Power consumption significantly exceeds target\",\n",
    "                expected_improvement={\"power\": -15.0},\n",
    "                implementation_effort=\"HIGH\",\n",
    "                risk_level=\"MEDIUM\"\n",
    "            ))\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "    def generate_enterprise_report(self, design_name: str, output_format: str = \"html\") -> str:\n",
    "        \"\"\"Generate comprehensive enterprise-quality report\"\"\"\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        report_file = f\"enterprise_report_{design_name}_{timestamp}.{output_format}\"\n",
    "\n",
    "        try:\n",
    "            design_results = [r for r in self.analysis_results if r.design_name == design_name]\n",
    "\n",
    "            if not design_results:\n",
    "                self.logger.error(f\"No results available for {design_name}\")\n",
    "                return \"\"\n",
    "\n",
    "            if output_format.lower() == \"html\":\n",
    "                content = self._generate_html_report(design_name, design_results)\n",
    "            else:\n",
    "                content = self._generate_text_report(design_name, design_results)\n",
    "\n",
    "            with open(f\"reports/{report_file}\", 'w') as f:\n",
    "                f.write(content)\n",
    "\n",
    "            self.logger.info(f\"Enterprise report generated: reports/{report_file}\")\n",
    "            return f\"reports/{report_file}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Report generation failed: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def _generate_html_report(self, design_name: str, results: List[AnalysisResults]) -> str:\n",
    "        \"\"\"Generate professional HTML report\"\"\"\n",
    "\n",
    "        html_content = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>VLSI Analysis Report - {design_name}</title>\n",
    "    <style>\n",
    "        body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
    "        .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 5px; }}\n",
    "        .section {{ margin: 20px 0; }}\n",
    "        .metric {{ background-color: #f9f9f9; padding: 10px; margin: 5px 0; }}\n",
    "        .violation {{ background-color: #ffe6e6; padding: 10px; margin: 5px 0; }}\n",
    "        table {{ border-collapse: collapse; width: 100%; }}\n",
    "        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
    "        th {{ background-color: #f2f2f2; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"header\">\n",
    "        <h1>VLSI Design Analysis Report</h1>\n",
    "        <h2>Design: {design_name}</h2>\n",
    "        <p>Generated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}</p>\n",
    "        <p>Framework Version: {self.framework_version}</p>\n",
    "    </div>\n",
    "\n",
    "    <div class=\"section\">\n",
    "        <h3>Executive Summary</h3>\n",
    "        <p>Total corners analyzed: {len(results)}</p>\n",
    "        <p>Analysis completion: {datetime.now().strftime(\"%Y-%m-%d\")}</p>\n",
    "    </div>\n",
    "\n",
    "    <div class=\"section\">\n",
    "        <h3>Corner Analysis Results</h3>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <th>Corner</th>\n",
    "                <th>Frequency (MHz)</th>\n",
    "                <th>Power (mW)</th>\n",
    "                <th>Worst Slack (ns)</th>\n",
    "                <th>Violations</th>\n",
    "            </tr>\n",
    "\"\"\"\n",
    "\n",
    "        for result in results:\n",
    "            violations_count = len(result.violations)\n",
    "            html_content += f\"\"\"\n",
    "            <tr>\n",
    "                <td>{result.corner}</td>\n",
    "                <td>{result.metrics['frequency']:.1f}</td>\n",
    "                <td>{result.metrics['power']:.1f}</td>\n",
    "                <td>{result.metrics['worst_slack']:.3f}</td>\n",
    "                <td>{violations_count}</td>\n",
    "            </tr>\n",
    "\"\"\"\n",
    "\n",
    "        html_content += \"\"\"\n",
    "        </table>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "        return html_content\n",
    "\n",
    "    def _generate_text_report(self, design_name: str, results: List[AnalysisResults]) -> str:\n",
    "        \"\"\"Generate text-based report\"\"\"\n",
    "\n",
    "        report = f\"\"\"\n",
    "VLSI DESIGN ANALYSIS REPORT\n",
    "===========================\n",
    "\n",
    "Design: {design_name}\n",
    "Generated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "Framework Version: {self.framework_version}\n",
    "\n",
    "EXECUTIVE SUMMARY\n",
    "-----------------\n",
    "Total corners analyzed: {len(results)}\n",
    "Framework session: {self.session_id}\n",
    "\n",
    "DETAILED RESULTS\n",
    "----------------\n",
    "\"\"\"\n",
    "\n",
    "        for result in results:\n",
    "            report += f\"\"\"\n",
    "Corner: {result.corner}\n",
    "  Frequency: {result.metrics['frequency']:.1f} MHz\n",
    "  Power: {result.metrics['power']:.1f} mW\n",
    "  Worst slack: {result.metrics['worst_slack']:.3f} ns\n",
    "  Violations: {len(result.violations)}\n",
    "  Execution time: {result.execution_time:.2f} seconds\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "        return report\n",
    "\n",
    "    def shutdown(self):\n",
    "        \"\"\"Graceful framework shutdown\"\"\"\n",
    "\n",
    "        self.logger.info(\"Initiating framework shutdown...\")\n",
    "\n",
    "        # Stop monitoring\n",
    "        if hasattr(self, 'monitoring_active'):\n",
    "            self.monitoring_active = False\n",
    "            if hasattr(self, 'monitoring_thread'):\n",
    "                self.monitoring_thread.join(timeout=5)\n",
    "\n",
    "        # Generate session summary\n",
    "        session_duration = datetime.now() - self.start_time\n",
    "\n",
    "        self.logger.info(f\"Framework session summary:\")\n",
    "        self.logger.info(f\"  Session duration: {session_duration}\")\n",
    "        self.logger.info(f\"  Designs processed: {len(self.design_configs)}\")\n",
    "        self.logger.info(f\"  Analysis results: {len(self.analysis_results)}\")\n",
    "\n",
    "        self.logger.info(\"Framework shutdown complete\")\n",
    "\n",
    "# =============================================================================\n",
    "# ENTERPRISE FRAMEWORK DEMONSTRATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ü§ñ ENTERPRISE VLSI AUTOMATION FRAMEWORK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize enterprise framework\n",
    "framework = VLSIAutomationFramework(\"enterprise_config.json\")\n",
    "\n",
    "print(f\"üèóÔ∏è Framework Version: {framework.framework_version}\")\n",
    "print(f\"üÜî Session ID: {framework.session_id}\")\n",
    "\n",
    "# Create sample design configuration\n",
    "design_config = DesignConfiguration(\n",
    "    design_name=\"next_gen_cpu_core\",\n",
    "    technology_node=\"5nm\",\n",
    "    target_frequency=3000.0,  # 3 GHz\n",
    "    target_power=2000.0,      # 2W\n",
    "    target_area=4.0,          # 4 mm¬≤\n",
    "    voltage_corners=[\"0.75V\", \"0.8V\", \"0.85V\"],\n",
    "    temperature_corners=[-40, 25, 125],\n",
    "    process_corners=[\"SS\", \"TT\", \"FF\"],\n",
    "    optimization_goals={\"frequency\": 1.0, \"power\": 0.8, \"area\": 0.6},\n",
    "    constraints_file=\"cpu_constraints.sdc\",\n",
    "    rtl_files=[\"cpu_core.v\", \"alu.v\", \"cache.v\"],\n",
    "    library_path=\"/design/libraries/5nm\"\n",
    ")\n",
    "\n",
    "print(f\"\\nüìã DESIGN REGISTRATION:\")\n",
    "success = framework.register_design(design_config)\n",
    "print(f\"   Registration: {'‚úÖ Success' if success else '‚ùå Failed'}\")\n",
    "\n",
    "if success:\n",
    "    print(f\"\\nüîç COMPREHENSIVE ANALYSIS:\")\n",
    "    analysis_success = framework.run_design_analysis(\"next_gen_cpu_core\", \"full\")\n",
    "\n",
    "    if analysis_success:\n",
    "        print(f\"   Analysis: ‚úÖ Complete\")\n",
    "\n",
    "        # Generate optimization recommendations\n",
    "        print(f\"\\nüí° OPTIMIZATION RECOMMENDATIONS:\")\n",
    "        recommendations = framework.generate_optimization_recommendations(\"next_gen_cpu_core\")\n",
    "\n",
    "        for i, rec in enumerate(recommendations, 1):\n",
    "            print(f\"   {i}. [{rec.priority}] {rec.category.upper()}: {rec.description}\")\n",
    "            print(f\"      Expected improvement: {rec.expected_improvement}\")\n",
    "            print(f\"      Implementation effort: {rec.implementation_effort}\")\n",
    "\n",
    "        # Generate enterprise report\n",
    "        print(f\"\\nüìä ENTERPRISE REPORT GENERATION:\")\n",
    "        report_file = framework.generate_enterprise_report(\"next_gen_cpu_core\", \"html\")\n",
    "        if report_file:\n",
    "            print(f\"   Report generated: {report_file}\")\n",
    "\n",
    "        text_report = framework.generate_enterprise_report(\"next_gen_cpu_core\", \"txt\")\n",
    "        if text_report:\n",
    "            print(f\"   Text report: {text_report}\")\n",
    "\n",
    "# Framework shutdown\n",
    "print(f\"\\nüîí FRAMEWORK SHUTDOWN:\")\n",
    "framework.shutdown()\n",
    "\n",
    "print(f\"\\nüèÜ ENTERPRISE FRAMEWORK BENEFITS:\")\n",
    "print(\"‚úÖ **Scalable Architecture**: Handle multiple designs and large corner matrices\")\n",
    "print(\"‚úÖ **Professional Logging**: Comprehensive audit trail and debugging\")\n",
    "print(\"‚úÖ **Parallel Processing**: Efficient multi-corner analysis\")\n",
    "print(\"‚úÖ **Configuration Management**: Flexible, validated configuration system\")\n",
    "print(\"‚úÖ **Enterprise Reporting**: Professional HTML/PDF reports with analytics\")\n",
    "print(\"‚úÖ **Error Recovery**: Robust operation in production environments\")\n",
    "print(\"‚úÖ **Team Integration**: Ready for version control and CI/CD workflows\")\n",
    "print(\"‚úÖ **Performance Monitoring**: Resource usage tracking and optimization\")\n",
    "\n",
    "# Cleanup demonstration files\n",
    "cleanup_files = [\"enterprise_config.json\"]\n",
    "for filename in cleanup_files:\n",
    "    if Path(filename).exists():\n",
    "        Path(filename).unlink()\n",
    "        print(f\"\\nüóëÔ∏è  Cleaned up: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c151150c",
   "metadata": {},
   "source": [
    "## üéì Chapter 4 Summary: Professional VLSI Automation Mastery\n",
    "\n",
    "Congratulations! You've just completed a comprehensive journey through professional-grade Python script development for VLSI automation. This chapter transformed you from a Python beginner into someone who understands production-quality automation development.\n",
    "\n",
    "### üìö **What You've Mastered:**\n",
    "\n",
    "#### üèóÔ∏è **Professional Script Architecture:**\n",
    "- **Complete Structure**: From shebang lines to main execution blocks\n",
    "- **Documentation Standards**: Comprehensive docstrings and inline comments\n",
    "- **Import Organization**: Standard library, third-party, and local imports\n",
    "- **Configuration Management**: Professional constants and parameter handling\n",
    "- **Modular Design**: Functions, classes, and reusable components\n",
    "\n",
    "#### üîã **Real-World Power Analysis Tool:**\n",
    "- **Multi-Corner Analysis**: SS, TT, FF process corners with temperature/voltage scaling\n",
    "- **Technology Scaling**: Support for 180nm to 5nm technology nodes\n",
    "- **Professional Modeling**: Dynamic power, leakage power, I/O power calculations\n",
    "- **Performance Metrics**: Power density, efficiency, and optimization recommendations\n",
    "- **Industry Standards**: Production-ready power estimation methodologies\n",
    "\n",
    "#### üìä **Advanced Timing Report Parser:**\n",
    "- **Multi-Tool Support**: Synopsys, Cadence, Mentor tool compatibility\n",
    "- **Robust Parsing**: Regex-based extraction with error recovery\n",
    "- **Large File Handling**: Streaming parser for massive timing reports\n",
    "- **Data Structures**: Professional timing path and constraint objects\n",
    "- **Statistical Analysis**: Comprehensive violation analysis and trending\n",
    "\n",
    "#### üìÅ **Enterprise File Operations:**\n",
    "- **Atomic Operations**: Prevent file corruption in automation flows\n",
    "- **Streaming Processing**: Handle GB-sized files efficiently\n",
    "- **Multi-Format Support**: JSON, CSV, YAML configuration management\n",
    "- **Backup Systems**: Automated backup with integrity verification\n",
    "- **Performance Optimization**: Memory-efficient I/O for large datasets\n",
    "\n",
    "#### üõ°Ô∏è **Production-Grade Error Handling:**\n",
    "- **Custom Exception Hierarchy**: VLSI-specific error types and severities\n",
    "- **Automatic Retry Mechanisms**: Exponential backoff for network/resource issues\n",
    "- **Comprehensive Logging**: Multi-level logging with context preservation\n",
    "- **Recovery Strategies**: Graceful degradation and fallback mechanisms\n",
    "- **User-Friendly Diagnostics**: Clear error messages with actionable solutions\n",
    "\n",
    "#### ü§ñ **Enterprise Automation Framework:**\n",
    "- **Scalable Architecture**: Multi-design, multi-corner automation platform\n",
    "- **Parallel Processing**: ThreadPoolExecutor for efficient corner analysis\n",
    "- **Professional Monitoring**: Resource usage tracking and performance metrics\n",
    "- **Enterprise Reporting**: HTML/PDF reports with executive summaries\n",
    "- **Team Integration**: Configuration management and audit trails\n",
    "\n",
    "### üéØ **Professional Skills Acquired:**\n",
    "\n",
    "| Skill Category | Python Concepts | VLSI Applications | Industry Value |\n",
    "|----------------|-----------------|-------------------|----------------|\n",
    "| **Architecture** | Classes, modules, packages | Tool integration flows | Maintainable automation |\n",
    "| **Data Processing** | Regex, streaming I/O | Report parsing, analysis | Handle massive datasets |\n",
    "| **Error Handling** | Custom exceptions, logging | Robust automation flows | Production reliability |\n",
    "| **Performance** | Threading, optimization | Multi-corner analysis | Scalable design flows |\n",
    "| **Professional** | Documentation, standards | Team collaboration | Enterprise deployment |\n",
    "\n",
    "### üí° **Key Professional Insights:**\n",
    "\n",
    "#### üîÑ **Python vs Traditional VLSI Scripting:**\n",
    "- **TCL/Perl**: Tool-specific, limited libraries, cryptic syntax\n",
    "- **Python**: Universal, rich ecosystem, readable and maintainable\n",
    "- **Advantage**: Python scripts become reusable assets across tools and projects\n",
    "\n",
    "#### üöÄ **Production Deployment Ready:**\n",
    "- **Version Control**: Professional structure for Git/SVN integration\n",
    "- **CI/CD Integration**: Automated testing and deployment capabilities\n",
    "- **Team Collaboration**: Self-documenting code that new engineers can understand\n",
    "- **Scalability**: Architecture that grows with project complexity\n",
    "\n",
    "#### üìà **Career Impact:**\n",
    "- **Efficiency Gains**: 10x faster development compared to traditional scripting\n",
    "- **Quality Improvement**: Robust error handling reduces production issues\n",
    "- **Innovation Enabler**: Python's ML/AI libraries open new optimization possibilities\n",
    "- **Market Value**: Python automation skills are highly sought after in VLSI industry\n",
    "\n",
    "### üõ£Ô∏è **What's Next? Your VLSI Python Journey Continues:**\n",
    "\n",
    "#### **Immediate Applications:**\n",
    "- **Adapt Power Tool**: Customize for your technology node and design constraints\n",
    "- **Extend Parser**: Add support for your specific EDA tool report formats\n",
    "- **Build Framework**: Create automation for your daily VLSI tasks\n",
    "- **Team Integration**: Share scripts and build collaborative workflows\n",
    "\n",
    "#### **Advanced Chapters Ahead:**\n",
    "- **Chapter 5**: Deep dive into Python data types for complex VLSI data\n",
    "- **Chapter 6**: Control flow for intelligent design decisions\n",
    "- **Chapter 7**: Loops for batch processing and automation\n",
    "- **Chapter 8**: Advanced functions and object-oriented VLSI design\n",
    "- **Chapter 9+**: File processing, databases, web interfaces, and ML integration\n",
    "\n",
    "### üèÜ **Professional Achievement Unlocked:**\n",
    "\n",
    "You now possess the fundamental skills to create **production-quality VLSI automation tools**. The scripts and frameworks you've learned in this chapter are not just educational examples‚Äîthey represent real patterns used in industry-leading semiconductor companies.\n",
    "\n",
    "#### **Your New Capabilities:**\n",
    "- ‚úÖ **Design professional Python scripts** with industry-standard structure\n",
    "- ‚úÖ **Handle complex VLSI data** with robust parsing and processing\n",
    "- ‚úÖ **Create scalable automation frameworks** for team deployment\n",
    "- ‚úÖ **Implement enterprise-grade error handling** for production reliability\n",
    "- ‚úÖ **Generate professional reports** for design reviews and sign-off\n",
    "- ‚úÖ **Integrate with existing VLSI flows** using standard interfaces\n",
    "\n",
    "### üéØ **The Bottom Line:**\n",
    "\n",
    "**Chapter 4 has transformed you from a Python learner into a VLSI automation developer.** You now understand not just Python syntax, but how to architect professional solutions that solve real VLSI challenges. The frameworks and patterns you've learned will serve as the foundation for increasingly sophisticated automation throughout your career.\n",
    "\n",
    "**Ready to dive deeper into Python data types and build even more powerful VLSI tools? Let's continue to Chapter 6! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
