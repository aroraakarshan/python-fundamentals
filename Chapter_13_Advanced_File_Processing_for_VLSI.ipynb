{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "553bc726",
   "metadata": {},
   "source": [
    "# Chapter 13: Advanced File Processing and Data Handling üìä\n",
    "\n",
    "## üéØ **Learning Objectives**\n",
    "Master advanced Python file processing for VLSI automation:\n",
    "\n",
    "### **Advanced File Operations**\n",
    "- Directory operations and batch processing\n",
    "- Large file handling and streaming\n",
    "- File monitoring and change detection\n",
    "- Backup and versioning strategies\n",
    "\n",
    "### **Data Processing Techniques**\n",
    "- **Log Analysis**: EDA tool log parsing and error extraction\n",
    "- **Report Aggregation**: Combining data from multiple sources\n",
    "- **Format Conversion**: Converting between tool formats\n",
    "- **Data Validation**: Content verification and integrity checking\n",
    "\n",
    "### **Automation Workflows**\n",
    "- Batch file processing pipelines\n",
    "- Automated report generation\n",
    "- File system monitoring\n",
    "- Configuration management\n",
    "\n",
    "---\n",
    "\n",
    "## üîß **Why Advanced File Processing Matters**\n",
    "Complex VLSI projects require sophisticated file handling:\n",
    "- **Scale**: Processing thousands of design files\n",
    "- **Integration**: Connecting multiple EDA tools\n",
    "- **Quality**: Ensuring data integrity across tool flows\n",
    "- **Efficiency**: Automating repetitive file operations\n",
    "- **Reliability**: Robust error handling and recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893bb1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIRECTORY OPERATIONS AND FILE MANAGEMENT\n",
    "# ========================================\n",
    "# Advanced directory handling and batch file operations\n",
    "\n",
    "print(\"üìÇ DIRECTORY OPERATIONS AND FILE MANAGEMENT\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "import shutil\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# =============================================================================\n",
    "# DIRECTORY OPERATIONS FOR VLSI PROJECTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüìÅ DIRECTORY OPERATIONS FOR VLSI PROJECTS:\")\n",
    "\n",
    "def create_vlsi_project_structure(project_path, project_name):\n",
    "    \"\"\"Create standard VLSI project directory structure.\"\"\"\n",
    "    base_path = Path(project_path) / project_name\n",
    "\n",
    "    # Define standard directory structure\n",
    "    directories = [\n",
    "        'src/rtl',\n",
    "        'src/constraints',\n",
    "        'src/testbench',\n",
    "        'scripts/synthesis',\n",
    "        'scripts/place_route',\n",
    "        'scripts/verification',\n",
    "        'results/synthesis',\n",
    "        'results/place_route',\n",
    "        'results/timing',\n",
    "        'results/power',\n",
    "        'reports',\n",
    "        'docs',\n",
    "        'libs',\n",
    "        'work'\n",
    "    ]\n",
    "\n",
    "    created_dirs = []\n",
    "    try:\n",
    "        for dir_path in directories:\n",
    "            full_path = base_path / dir_path\n",
    "            full_path.mkdir(parents=True, exist_ok=True)\n",
    "            created_dirs.append(str(full_path.relative_to(base_path)))\n",
    "\n",
    "        print(f\"     ‚úÖ Created project structure for '{project_name}'\")\n",
    "        print(f\"     üìÅ Base path: {base_path}\")\n",
    "        print(f\"     üìÇ Created {len(created_dirs)} directories\")\n",
    "        return str(base_path), created_dirs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"     ‚ùå Error creating project structure: {e}\")\n",
    "        return None, []\n",
    "\n",
    "def find_files_by_pattern(directory, pattern, recursive=True):\n",
    "    \"\"\"Find files matching pattern in directory.\"\"\"\n",
    "    search_path = Path(directory)\n",
    "\n",
    "    if not search_path.exists():\n",
    "        print(f\"     ‚ùå Directory not found: {directory}\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        if recursive:\n",
    "            # Use ** for recursive search\n",
    "            files = list(search_path.glob(f\"**/{pattern}\"))\n",
    "        else:\n",
    "            files = list(search_path.glob(pattern))\n",
    "\n",
    "        # Convert to relative paths for better display\n",
    "        relative_files = [f.relative_to(search_path) for f in files]\n",
    "\n",
    "        print(f\"     üîç Found {len(files)} files matching '{pattern}'\")\n",
    "        return [str(f) for f in relative_files]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"     ‚ùå Error searching files: {e}\")\n",
    "        return []\n",
    "\n",
    "def copy_files_by_type(source_dir, dest_dir, file_extensions):\n",
    "    \"\"\"Copy files with specific extensions from source to destination.\"\"\"\n",
    "    source_path = Path(source_dir)\n",
    "    dest_path = Path(dest_dir)\n",
    "\n",
    "    # Create destination directory\n",
    "    dest_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    copied_files = []\n",
    "\n",
    "    try:\n",
    "        for ext in file_extensions:\n",
    "            pattern = f\"*.{ext}\"\n",
    "            files = list(source_path.glob(f\"**/{pattern}\"))\n",
    "\n",
    "            for file_path in files:\n",
    "                # Maintain relative directory structure\n",
    "                relative_path = file_path.relative_to(source_path)\n",
    "                dest_file = dest_path / relative_path\n",
    "\n",
    "                # Create parent directories if needed\n",
    "                dest_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                # Copy file\n",
    "                shutil.copy2(file_path, dest_file)\n",
    "                copied_files.append(str(relative_path))\n",
    "\n",
    "        print(f\"     ‚úÖ Copied {len(copied_files)} files ({', '.join(file_extensions)})\")\n",
    "        return copied_files\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"     ‚ùå Error copying files: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test directory operations\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    temp_path = Path(temp_dir)\n",
    "\n",
    "    print(\"   Testing directory operations:\")\n",
    "\n",
    "    # Create project structure\n",
    "    project_base, dirs = create_vlsi_project_structure(temp_dir, \"cpu_design\")\n",
    "\n",
    "    if project_base:\n",
    "        project_path = Path(project_base)\n",
    "\n",
    "        # Create some sample files\n",
    "        sample_files = {\n",
    "            'src/rtl/cpu_core.v': 'module cpu_core(); endmodule',\n",
    "            'src/rtl/alu.v': 'module alu(); endmodule',\n",
    "            'src/constraints/timing.sdc': 'create_clock -period 10 clk',\n",
    "            'scripts/synthesis/syn_script.tcl': 'read_verilog cpu_core.v',\n",
    "            'results/synthesis/area.rpt': 'Area Report\\n============',\n",
    "            'docs/readme.txt': 'CPU Design Documentation'\n",
    "        }\n",
    "\n",
    "        for file_path, content in sample_files.items():\n",
    "            full_file_path = project_path / file_path\n",
    "            full_file_path.write_text(content)\n",
    "\n",
    "        print(f\"     üìù Created {len(sample_files)} sample files\")\n",
    "\n",
    "        # Test file finding\n",
    "        print(\"\\n   Testing file search:\")\n",
    "        verilog_files = find_files_by_pattern(project_path, \"*.v\")\n",
    "        sdc_files = find_files_by_pattern(project_path, \"*.sdc\")\n",
    "        report_files = find_files_by_pattern(project_path, \"*.rpt\")\n",
    "\n",
    "        for files, file_type in [(verilog_files, \"Verilog\"), (sdc_files, \"SDC\"), (report_files, \"Report\")]:\n",
    "            if files:\n",
    "                print(f\"     {file_type} files: {files}\")\n",
    "\n",
    "        # Test file copying\n",
    "        print(\"\\n   Testing file copying:\")\n",
    "        backup_dir = project_path / \"backup\"\n",
    "        copied = copy_files_by_type(project_path / \"src\", backup_dir, [\"v\", \"sdc\"])\n",
    "\n",
    "# =============================================================================\n",
    "# BATCH FILE PROCESSING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è BATCH FILE PROCESSING FUNCTIONS:\")\n",
    "\n",
    "def process_verilog_files(file_list, output_dir):\n",
    "    \"\"\"Process multiple Verilog files and extract module information.\"\"\"\n",
    "    results = []\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for file_path in file_list:\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                content = f.read()\n",
    "\n",
    "            # Extract module information\n",
    "            module_pattern = r'module\\s+(\\w+)\\s*\\('\n",
    "            modules = re.findall(module_pattern, content)\n",
    "\n",
    "            # Count various constructs\n",
    "            always_blocks = content.count('always')\n",
    "            wire_count = content.count('wire')\n",
    "            reg_count = content.count('reg')\n",
    "\n",
    "            file_info = {\n",
    "                'file': Path(file_path).name,\n",
    "                'modules': modules,\n",
    "                'module_count': len(modules),\n",
    "                'always_blocks': always_blocks,\n",
    "                'wire_declarations': wire_count,\n",
    "                'reg_declarations': reg_count,\n",
    "                'line_count': len(content.split('\\n')),\n",
    "                'char_count': len(content)\n",
    "            }\n",
    "\n",
    "            results.append(file_info)\n",
    "\n",
    "            # Create summary file\n",
    "            summary_file = output_path / f\"{Path(file_path).stem}_summary.txt\"\n",
    "            with open(summary_file, 'w') as f:\n",
    "                f.write(f\"Verilog File Analysis: {file_info['file']}\\n\")\n",
    "                f.write(f\"{'='*50}\\n\")\n",
    "                f.write(f\"Modules: {', '.join(file_info['modules'])}\\n\")\n",
    "                f.write(f\"Module count: {file_info['module_count']}\\n\")\n",
    "                f.write(f\"Always blocks: {file_info['always_blocks']}\\n\")\n",
    "                f.write(f\"Wire declarations: {file_info['wire_declarations']}\\n\")\n",
    "                f.write(f\"Reg declarations: {file_info['reg_declarations']}\\n\")\n",
    "                f.write(f\"Lines: {file_info['line_count']}\\n\")\n",
    "                f.write(f\"Characters: {file_info['char_count']}\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"     ‚ùå Error processing {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return results\n",
    "\n",
    "def generate_project_report(project_path, output_file):\n",
    "    \"\"\"Generate comprehensive project report.\"\"\"\n",
    "    project_path = Path(project_path)\n",
    "\n",
    "    if not project_path.exists():\n",
    "        print(f\"     ‚ùå Project path not found: {project_path}\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        # Collect file statistics\n",
    "        file_stats = {\n",
    "            'verilog': len(list(project_path.glob(\"**/*.v\"))),\n",
    "            'sdc': len(list(project_path.glob(\"**/*.sdc\"))),\n",
    "            'tcl': len(list(project_path.glob(\"**/*.tcl\"))),\n",
    "            'reports': len(list(project_path.glob(\"**/*.rpt\"))),\n",
    "            'logs': len(list(project_path.glob(\"**/*.log\"))),\n",
    "            'total': 0\n",
    "        }\n",
    "        file_stats['total'] = sum(file_stats.values()) - file_stats['total']\n",
    "\n",
    "        # Calculate total size\n",
    "        total_size = 0\n",
    "        all_files = list(project_path.glob(\"**/*\"))\n",
    "        for file_path in all_files:\n",
    "            if file_path.is_file():\n",
    "                total_size += file_path.stat().st_size\n",
    "\n",
    "        # Generate report\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(f\"VLSI Project Report\\n\")\n",
    "            f.write(f\"{'='*50}\\n\")\n",
    "            f.write(f\"Project Path: {project_path}\\n\")\n",
    "            f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "\n",
    "            f.write(f\"File Statistics:\\n\")\n",
    "            f.write(f\"{'='*20}\\n\")\n",
    "            for file_type, count in file_stats.items():\n",
    "                f.write(f\"{file_type.upper():15s}: {count:6d} files\\n\")\n",
    "\n",
    "            f.write(f\"\\nProject Size:\\n\")\n",
    "            f.write(f\"{'='*15}\\n\")\n",
    "            f.write(f\"Total size: {total_size:,} bytes ({total_size/1024:.1f} KB)\\n\")\n",
    "            f.write(f\"Total files: {len([f for f in all_files if f.is_file()])} files\\n\")\n",
    "            f.write(f\"Total directories: {len([f for f in all_files if f.is_dir()])} directories\\n\")\n",
    "\n",
    "        print(f\"     ‚úÖ Project report generated: {output_file}\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"     ‚ùå Error generating report: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test batch processing\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    temp_path = Path(temp_dir)\n",
    "\n",
    "    print(\"   Testing batch file processing:\")\n",
    "\n",
    "    # Create test Verilog files\n",
    "    test_verilog_files = {\n",
    "        'cpu.v': '''module cpu (\n",
    "    input wire clk,\n",
    "    input wire reset,\n",
    "    output reg [31:0] data\n",
    ");\n",
    "    reg [31:0] counter;\n",
    "    wire clock_enable;\n",
    "\n",
    "    always @(posedge clk) begin\n",
    "        if (reset)\n",
    "            counter <= 0;\n",
    "        else\n",
    "            counter <= counter + 1;\n",
    "    end\n",
    "endmodule''',\n",
    "\n",
    "        'memory.v': '''module memory (\n",
    "    input wire clk,\n",
    "    input wire [7:0] addr,\n",
    "    output reg [31:0] data\n",
    ");\n",
    "    reg [31:0] mem_array [0:255];\n",
    "\n",
    "    always @(posedge clk) begin\n",
    "        data <= mem_array[addr];\n",
    "    end\n",
    "endmodule'''\n",
    "    }\n",
    "\n",
    "    # Write test files\n",
    "    verilog_files = []\n",
    "    for filename, content in test_verilog_files.items():\n",
    "        file_path = temp_path / filename\n",
    "        file_path.write_text(content)\n",
    "        verilog_files.append(str(file_path))\n",
    "\n",
    "    # Process Verilog files\n",
    "    output_dir = temp_path / \"analysis\"\n",
    "    results = process_verilog_files(verilog_files, output_dir)\n",
    "\n",
    "    print(f\"     üìä Processed {len(results)} Verilog files\")\n",
    "    for result in results:\n",
    "        print(f\"       {result['file']}: {result['module_count']} modules, {result['line_count']} lines\")\n",
    "\n",
    "    # Generate project report\n",
    "    report_file = temp_path / \"project_report.txt\"\n",
    "    success = generate_project_report(temp_path, report_file)\n",
    "\n",
    "print(\"\\nüèÜ DIRECTORY AND BATCH OPERATION BENEFITS:\")\n",
    "print(\"‚úÖ **Project Organization**: Automated directory structure creation\")\n",
    "print(\"‚úÖ **File Discovery**: Pattern-based file searching and filtering\")\n",
    "print(\"‚úÖ **Batch Processing**: Efficient handling of multiple files\")\n",
    "print(\"‚úÖ **Reporting**: Automated project analysis and documentation\")\n",
    "print(\"‚úÖ **Maintenance**: File copying, backup, and cleanup operations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9b26de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOG ANALYSIS AND ERROR EXTRACTION\n",
    "# =================================\n",
    "# Processing EDA tool logs for insights\n",
    "\n",
    "print(\"üìä LOG ANALYSIS AND ERROR EXTRACTION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# =============================================================================\n",
    "# EDA TOOL LOG PARSING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüìù EDA TOOL LOG PARSING:\")\n",
    "\n",
    "# Sample EDA tool log content\n",
    "synthesis_log = \"\"\"\n",
    "Info: Starting synthesis flow\n",
    "Info: Reading design files...\n",
    "Info: Reading /home/project/src/cpu_core.v\n",
    "Info: Reading /home/project/src/alu.v\n",
    "Info: Reading /home/project/src/memory.v\n",
    "Warning: Implicit wire declaration for signal 'clk_enable' in module cpu_core\n",
    "Info: Elaborating design...\n",
    "Info: Elaborated module cpu_core with 1250 gates\n",
    "Info: Elaborated module alu with 456 gates\n",
    "Info: Elaborated module memory with 2048 gates\n",
    "Warning: Unconnected output port 'overflow' in module alu\n",
    "Info: Starting synthesis optimization...\n",
    "Info: Area optimization: 15% reduction\n",
    "Info: Timing optimization: setup slack improved by 0.5ns\n",
    "Error: Setup timing violation on path cpu_core/reg1 -> alu/out\n",
    "Error: Hold timing violation on path memory/data -> cpu_core/reg2\n",
    "Info: Power optimization: 8% reduction\n",
    "Warning: High fanout on signal 'clk' (fanout = 1024)\n",
    "Info: Synthesis completed\n",
    "Info: Final area: 1250.5 um^2\n",
    "Info: Final power: 825.3 mW\n",
    "Info: Critical path delay: 9.85 ns\n",
    "\"\"\"\n",
    "\n",
    "def parse_eda_log(log_content, tool_name=\"Unknown\"):\n",
    "    \"\"\"Parse EDA tool log and extract key information.\"\"\"\n",
    "    lines = log_content.strip().split('\\n')\n",
    "\n",
    "    log_data = {\n",
    "        'tool': tool_name,\n",
    "        'total_lines': len(lines),\n",
    "        'messages': {\n",
    "            'info': [],\n",
    "            'warning': [],\n",
    "            'error': []\n",
    "        },\n",
    "        'metrics': {},\n",
    "        'files_processed': [],\n",
    "        'summary': {}\n",
    "    }\n",
    "\n",
    "    for line_num, line in enumerate(lines, 1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Extract message type and content\n",
    "        if line.startswith('Info:'):\n",
    "            message = line[5:].strip()\n",
    "            log_data['messages']['info'].append({\n",
    "                'line': line_num,\n",
    "                'message': message\n",
    "            })\n",
    "\n",
    "            # Extract metrics from info messages\n",
    "            if 'Final area:' in message:\n",
    "                area_match = re.search(r'Final area:\\s*([\\d.]+)', message)\n",
    "                if area_match:\n",
    "                    log_data['metrics']['area'] = float(area_match.group(1))\n",
    "            elif 'Final power:' in message:\n",
    "                power_match = re.search(r'Final power:\\s*([\\d.]+)', message)\n",
    "                if power_match:\n",
    "                    log_data['metrics']['power'] = float(power_match.group(1))\n",
    "            elif 'Critical path delay:' in message:\n",
    "                delay_match = re.search(r'Critical path delay:\\s*([\\d.]+)', message)\n",
    "                if delay_match:\n",
    "                    log_data['metrics']['delay'] = float(delay_match.group(1))\n",
    "            elif 'Reading /' in message:\n",
    "                file_match = re.search(r'Reading\\s+(/[^\\s]+)', message)\n",
    "                if file_match:\n",
    "                    log_data['files_processed'].append(file_match.group(1))\n",
    "\n",
    "        elif line.startswith('Warning:'):\n",
    "            message = line[8:].strip()\n",
    "            log_data['messages']['warning'].append({\n",
    "                'line': line_num,\n",
    "                'message': message\n",
    "            })\n",
    "\n",
    "        elif line.startswith('Error:'):\n",
    "            message = line[6:].strip()\n",
    "            log_data['messages']['error'].append({\n",
    "                'line': line_num,\n",
    "                'message': message\n",
    "            })\n",
    "\n",
    "    # Generate summary\n",
    "    log_data['summary'] = {\n",
    "        'info_count': len(log_data['messages']['info']),\n",
    "        'warning_count': len(log_data['messages']['warning']),\n",
    "        'error_count': len(log_data['messages']['error']),\n",
    "        'files_count': len(log_data['files_processed']),\n",
    "        'has_errors': len(log_data['messages']['error']) > 0,\n",
    "        'status': 'FAIL' if len(log_data['messages']['error']) > 0 else 'PASS'\n",
    "    }\n",
    "\n",
    "    return log_data\n",
    "\n",
    "def categorize_log_messages(log_data):\n",
    "    \"\"\"Categorize log messages by type for better analysis.\"\"\"\n",
    "    categories = {\n",
    "        'timing_violations': [],\n",
    "        'unconnected_signals': [],\n",
    "        'implicit_declarations': [],\n",
    "        'optimization_results': [],\n",
    "        'file_operations': []\n",
    "    }\n",
    "\n",
    "    all_messages = (\n",
    "        log_data['messages']['info'] +\n",
    "        log_data['messages']['warning'] +\n",
    "        log_data['messages']['error']\n",
    "    )\n",
    "\n",
    "    for msg in all_messages:\n",
    "        message = msg['message'].lower()\n",
    "\n",
    "        if any(keyword in message for keyword in ['timing violation', 'slack']):\n",
    "            categories['timing_violations'].append(msg)\n",
    "        elif any(keyword in message for keyword in ['unconnected', 'floating']):\n",
    "            categories['unconnected_signals'].append(msg)\n",
    "        elif any(keyword in message for keyword in ['implicit', 'undeclared']):\n",
    "            categories['implicit_declarations'].append(msg)\n",
    "        elif any(keyword in message for keyword in ['optimization', 'reduction', 'improved']):\n",
    "            categories['optimization_results'].append(msg)\n",
    "        elif any(keyword in message for keyword in ['reading', 'writing', 'file']):\n",
    "            categories['file_operations'].append(msg)\n",
    "\n",
    "    return categories\n",
    "\n",
    "def generate_log_summary_report(log_data, output_file):\n",
    "    \"\"\"Generate comprehensive log summary report.\"\"\"\n",
    "    try:\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(f\"EDA Tool Log Analysis Report\\n\")\n",
    "            f.write(f\"{'='*50}\\n\")\n",
    "            f.write(f\"Tool: {log_data['tool']}\\n\")\n",
    "            f.write(f\"Total lines processed: {log_data['total_lines']}\\n\")\n",
    "            f.write(f\"Analysis status: {log_data['summary']['status']}\\n\\n\")\n",
    "\n",
    "            # Message summary\n",
    "            f.write(f\"Message Summary:\\n\")\n",
    "            f.write(f\"{'='*20}\\n\")\n",
    "            f.write(f\"Info messages: {log_data['summary']['info_count']}\\n\")\n",
    "            f.write(f\"Warnings: {log_data['summary']['warning_count']}\\n\")\n",
    "            f.write(f\"Errors: {log_data['summary']['error_count']}\\n\\n\")\n",
    "\n",
    "            # Metrics\n",
    "            if log_data['metrics']:\n",
    "                f.write(f\"Design Metrics:\\n\")\n",
    "                f.write(f\"{'='*15}\\n\")\n",
    "                for metric, value in log_data['metrics'].items():\n",
    "                    f.write(f\"{metric.capitalize()}: {value}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "            # Files processed\n",
    "            if log_data['files_processed']:\n",
    "                f.write(f\"Files Processed ({len(log_data['files_processed'])}):\\n\")\n",
    "                f.write(f\"{'='*25}\\n\")\n",
    "                for file_path in log_data['files_processed']:\n",
    "                    f.write(f\"  {file_path}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "            # Errors (if any)\n",
    "            if log_data['messages']['error']:\n",
    "                f.write(f\"Errors ({len(log_data['messages']['error'])}):\\n\")\n",
    "                f.write(f\"{'='*15}\\n\")\n",
    "                for error in log_data['messages']['error']:\n",
    "                    f.write(f\"  Line {error['line']}: {error['message']}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "            # Warnings (if any)\n",
    "            if log_data['messages']['warning']:\n",
    "                f.write(f\"Warnings ({len(log_data['messages']['warning'])}):\\n\")\n",
    "                f.write(f\"{'='*18}\\n\")\n",
    "                for warning in log_data['messages']['warning']:\n",
    "                    f.write(f\"  Line {warning['line']}: {warning['message']}\\n\")\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"     ‚ùå Error generating report: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test log analysis\n",
    "print(\"   Analyzing synthesis log:\")\n",
    "parsed_log = parse_eda_log(synthesis_log, \"Design Compiler\")\n",
    "\n",
    "print(f\"     ‚úÖ Parsed {parsed_log['total_lines']} log lines\")\n",
    "print(f\"     Status: {parsed_log['summary']['status']}\")\n",
    "print(f\"     Messages: {parsed_log['summary']['info_count']} info, {parsed_log['summary']['warning_count']} warnings, {parsed_log['summary']['error_count']} errors\")\n",
    "\n",
    "if parsed_log['metrics']:\n",
    "    print(f\"     Metrics extracted:\")\n",
    "    for metric, value in parsed_log['metrics'].items():\n",
    "        print(f\"       {metric.capitalize()}: {value}\")\n",
    "\n",
    "# Categorize messages\n",
    "categories = categorize_log_messages(parsed_log)\n",
    "print(f\"\\n   Message categorization:\")\n",
    "for category, messages in categories.items():\n",
    "    if messages:\n",
    "        print(f\"     {category.replace('_', ' ').title()}: {len(messages)} messages\")\n",
    "\n",
    "# Generate report\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    report_file = Path(temp_dir) / \"log_analysis.txt\"\n",
    "    success = generate_log_summary_report(parsed_log, report_file)\n",
    "    if success:\n",
    "        print(f\"     ‚úÖ Log analysis report generated\")\n",
    "\n",
    "# =============================================================================\n",
    "# MULTI-LOG AGGREGATION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nüìã MULTI-LOG AGGREGATION:\")\n",
    "\n",
    "def aggregate_multiple_logs(log_files_data):\n",
    "    \"\"\"Aggregate data from multiple log files.\"\"\"\n",
    "    aggregated = {\n",
    "        'total_logs': len(log_files_data),\n",
    "        'overall_status': 'PASS',\n",
    "        'combined_metrics': {},\n",
    "        'message_totals': {\n",
    "            'info': 0,\n",
    "            'warning': 0,\n",
    "            'error': 0\n",
    "        },\n",
    "        'tool_summary': {},\n",
    "        'error_patterns': defaultdict(int),\n",
    "        'warning_patterns': defaultdict(int)\n",
    "    }\n",
    "\n",
    "    for log_data in log_files_data:\n",
    "        # Aggregate message counts\n",
    "        for msg_type in ['info', 'warning', 'error']:\n",
    "            aggregated['message_totals'][msg_type] += len(log_data['messages'][msg_type])\n",
    "\n",
    "        # Check overall status\n",
    "        if log_data['summary']['status'] == 'FAIL':\n",
    "            aggregated['overall_status'] = 'FAIL'\n",
    "\n",
    "        # Aggregate metrics\n",
    "        for metric, value in log_data['metrics'].items():\n",
    "            if metric not in aggregated['combined_metrics']:\n",
    "                aggregated['combined_metrics'][metric] = []\n",
    "            aggregated['combined_metrics'][metric].append(value)\n",
    "\n",
    "        # Tool summary\n",
    "        tool = log_data['tool']\n",
    "        if tool not in aggregated['tool_summary']:\n",
    "            aggregated['tool_summary'][tool] = {\n",
    "                'logs': 0,\n",
    "                'errors': 0,\n",
    "                'warnings': 0\n",
    "            }\n",
    "        aggregated['tool_summary'][tool]['logs'] += 1\n",
    "        aggregated['tool_summary'][tool]['errors'] += len(log_data['messages']['error'])\n",
    "        aggregated['tool_summary'][tool]['warnings'] += len(log_data['messages']['warning'])\n",
    "\n",
    "        # Analyze error patterns\n",
    "        for error in log_data['messages']['error']:\n",
    "            # Extract key words from error message\n",
    "            words = error['message'].lower().split()\n",
    "            key_words = [w for w in words if len(w) > 3 and w not in ['this', 'that', 'with', 'from', 'module']]\n",
    "            if key_words:\n",
    "                pattern = key_words[0]  # Use first significant word as pattern\n",
    "                aggregated['error_patterns'][pattern] += 1\n",
    "\n",
    "        # Analyze warning patterns\n",
    "        for warning in log_data['messages']['warning']:\n",
    "            words = warning['message'].lower().split()\n",
    "            key_words = [w for w in words if len(w) > 3 and w not in ['this', 'that', 'with', 'from', 'module']]\n",
    "            if key_words:\n",
    "                pattern = key_words[0]\n",
    "                aggregated['warning_patterns'][pattern] += 1\n",
    "\n",
    "    return aggregated\n",
    "\n",
    "# Test multi-log aggregation\n",
    "sample_logs = [\n",
    "    parse_eda_log(synthesis_log, \"Design Compiler\"),\n",
    "    parse_eda_log(\"Info: Starting place and route\\nWarning: High congestion in region\\nInfo: Completed successfully\", \"Innovus\"),\n",
    "    parse_eda_log(\"Info: Starting timing analysis\\nError: Setup violation found\\nInfo: Analysis complete\", \"PrimeTime\")\n",
    "]\n",
    "\n",
    "aggregated_data = aggregate_multiple_logs(sample_logs)\n",
    "\n",
    "print(\"   Multi-log aggregation results:\")\n",
    "print(f\"     Total logs analyzed: {aggregated_data['total_logs']}\")\n",
    "print(f\"     Overall status: {aggregated_data['overall_status']}\")\n",
    "print(f\"     Combined messages: {aggregated_data['message_totals']['info']} info, {aggregated_data['message_totals']['warning']} warnings, {aggregated_data['message_totals']['error']} errors\")\n",
    "\n",
    "print(f\"\\n   Tool summary:\")\n",
    "for tool, summary in aggregated_data['tool_summary'].items():\n",
    "    print(f\"     {tool}: {summary['logs']} logs, {summary['errors']} errors, {summary['warnings']} warnings\")\n",
    "\n",
    "if aggregated_data['error_patterns']:\n",
    "    print(f\"\\n   Common error patterns:\")\n",
    "    for pattern, count in sorted(aggregated_data['error_patterns'].items(), key=lambda x: x[1], reverse=True)[:3]:\n",
    "        print(f\"     '{pattern}': {count} occurrences\")\n",
    "\n",
    "print(\"\\nüèÜ LOG ANALYSIS BENEFITS:\")\n",
    "print(\"‚úÖ **Automated Monitoring**: Parse tool logs without manual review\")\n",
    "print(\"‚úÖ **Error Detection**: Quickly identify and categorize issues\")\n",
    "print(\"‚úÖ **Trend Analysis**: Track error patterns across runs\")\n",
    "print(\"‚úÖ **Quality Metrics**: Extract performance data from logs\")\n",
    "print(\"‚úÖ **Reporting**: Generate comprehensive analysis reports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0ae5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LARGE FILE PROCESSING AND STREAMING\n",
    "# ===================================\n",
    "# Efficient handling of large VLSI files\n",
    "\n",
    "print(\"üíæ LARGE FILE PROCESSING AND STREAMING\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "import mmap\n",
    "import itertools\n",
    "\n",
    "# =============================================================================\n",
    "# STREAMING FILE PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüåä STREAMING FILE PROCESSING:\")\n",
    "\n",
    "def process_large_netlist_streaming(file_path, chunk_size=8192):\n",
    "    \"\"\"Process large netlist file using streaming to manage memory.\"\"\"\n",
    "    statistics = {\n",
    "        'total_lines': 0,\n",
    "        'module_count': 0,\n",
    "        'instance_count': 0,\n",
    "        'wire_count': 0,\n",
    "        'assign_count': 0,\n",
    "        'file_size': 0\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Get file size\n",
    "        file_path = Path(file_path)\n",
    "        statistics['file_size'] = file_path.stat().st_size\n",
    "\n",
    "        print(f\"     üìÅ Processing file: {file_path.name} ({statistics['file_size']:,} bytes)\")\n",
    "\n",
    "        # Process file in chunks\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            buffer = \"\"\n",
    "\n",
    "            while True:\n",
    "                chunk = f.read(chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "\n",
    "                buffer += chunk\n",
    "\n",
    "                # Process complete lines\n",
    "                while '\\n' in buffer:\n",
    "                    line, buffer = buffer.split('\\n', 1)\n",
    "                    line = line.strip()\n",
    "\n",
    "                    if line:\n",
    "                        statistics['total_lines'] += 1\n",
    "\n",
    "                        # Count different constructs\n",
    "                        if line.startswith('module '):\n",
    "                            statistics['module_count'] += 1\n",
    "                        elif 'wire ' in line:\n",
    "                            statistics['wire_count'] += 1\n",
    "                        elif 'assign ' in line:\n",
    "                            statistics['assign_count'] += 1\n",
    "                        elif re.match(r'\\s*\\w+\\s+\\w+\\s*\\(', line):  # Instance pattern\n",
    "                            statistics['instance_count'] += 1\n",
    "\n",
    "            # Process remaining buffer\n",
    "            if buffer.strip():\n",
    "                statistics['total_lines'] += 1\n",
    "\n",
    "        print(f\"     ‚úÖ Streaming processing completed\")\n",
    "        print(f\"       Lines: {statistics['total_lines']:,}\")\n",
    "        print(f\"       Modules: {statistics['module_count']}\")\n",
    "        print(f\"       Instances: {statistics['instance_count']}\")\n",
    "        print(f\"       Wires: {statistics['wire_count']}\")\n",
    "        print(f\"       Assigns: {statistics['assign_count']}\")\n",
    "\n",
    "        return statistics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"     ‚ùå Error in streaming processing: {e}\")\n",
    "        return None\n",
    "\n",
    "def search_large_file_patterns(file_path, patterns, max_matches=100):\n",
    "    \"\"\"Search for patterns in large files efficiently.\"\"\"\n",
    "    matches = {pattern: [] for pattern in patterns}\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                line = line.strip()\n",
    "\n",
    "                for pattern in patterns:\n",
    "                    if pattern in line and len(matches[pattern]) < max_matches:\n",
    "                        matches[pattern].append({\n",
    "                            'line_number': line_num,\n",
    "                            'content': line\n",
    "                        })\n",
    "\n",
    "                # Stop if all patterns have enough matches\n",
    "                if all(len(matches[p]) >= max_matches for p in patterns):\n",
    "                    break\n",
    "\n",
    "        print(f\"     üîç Pattern search completed:\")\n",
    "        for pattern, match_list in matches.items():\n",
    "            print(f\"       '{pattern}': {len(match_list)} matches\")\n",
    "\n",
    "        return matches\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"     ‚ùå Error in pattern search: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create sample large netlist for testing\n",
    "def create_sample_large_netlist(file_path, num_modules=100):\n",
    "    \"\"\"Create a sample large netlist file for testing.\"\"\"\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(\"// Large Netlist Example\\n\")\n",
    "        f.write(\"// Generated for testing large file processing\\n\\n\")\n",
    "\n",
    "        for i in range(num_modules):\n",
    "            f.write(f\"module test_module_{i} (\\n\")\n",
    "            f.write(f\"    input wire clk,\\n\")\n",
    "            f.write(f\"    input wire reset,\\n\")\n",
    "            f.write(f\"    input wire [31:0] data_in,\\n\")\n",
    "            f.write(f\"    output wire [31:0] data_out\\n\")\n",
    "            f.write(f\");\\n\\n\")\n",
    "\n",
    "            # Add some internal wires and instances\n",
    "            for j in range(5):\n",
    "                f.write(f\"    wire internal_sig_{j};\\n\")\n",
    "\n",
    "            for j in range(3):\n",
    "                f.write(f\"    sub_module_inst inst_{j} (\\n\")\n",
    "                f.write(f\"        .clk(clk),\\n\")\n",
    "                f.write(f\"        .reset(reset),\\n\")\n",
    "                f.write(f\"        .data(internal_sig_{j})\\n\")\n",
    "                f.write(f\"    );\\n\\n\")\n",
    "\n",
    "            # Add assign statements\n",
    "            for j in range(2):\n",
    "                f.write(f\"    assign internal_sig_{j} = data_in[{j*8+7}:{j*8}];\\n\")\n",
    "\n",
    "            f.write(f\"\\nendmodule\\n\\n\")\n",
    "\n",
    "# Test large file processing\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    temp_path = Path(temp_dir)\n",
    "    large_netlist = temp_path / \"large_design.v\"\n",
    "\n",
    "    print(\"   Creating sample large netlist:\")\n",
    "    create_sample_large_netlist(large_netlist, num_modules=50)\n",
    "\n",
    "    # Test streaming processing\n",
    "    print(\"\\n   Testing streaming processing:\")\n",
    "    stats = process_large_netlist_streaming(large_netlist)\n",
    "\n",
    "    # Test pattern searching\n",
    "    print(\"\\n   Testing pattern search:\")\n",
    "    patterns = ['module', 'wire', 'assign', 'input', 'output']\n",
    "    matches = search_large_file_patterns(large_netlist, patterns, max_matches=10)\n",
    "\n",
    "# =============================================================================\n",
    "# MEMORY-MAPPED FILE PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nüó∫Ô∏è MEMORY-MAPPED FILE PROCESSING:\")\n",
    "\n",
    "def process_with_memory_mapping(file_path):\n",
    "    \"\"\"Process file using memory mapping for efficiency.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "                # Convert to string for processing\n",
    "                content = mm.read().decode('utf-8')\n",
    "\n",
    "                # Quick statistics\n",
    "                line_count = content.count('\\n')\n",
    "                module_count = content.count('module ')\n",
    "                instance_count = len(re.findall(r'\\w+\\s+\\w+\\s*\\(', content))\n",
    "\n",
    "                print(f\"     üìä Memory-mapped processing results:\")\n",
    "                print(f\"       File size: {len(content):,} bytes\")\n",
    "                print(f\"       Lines: {line_count:,}\")\n",
    "                print(f\"       Modules: {module_count}\")\n",
    "                print(f\"       Instances: {instance_count}\")\n",
    "\n",
    "                return {\n",
    "                    'size': len(content),\n",
    "                    'lines': line_count,\n",
    "                    'modules': module_count,\n",
    "                    'instances': instance_count\n",
    "                }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"     ‚ùå Error in memory-mapped processing: {e}\")\n",
    "        return None\n",
    "\n",
    "# =============================================================================\n",
    "# CHUNKED FILE PROCESSING FOR REPORTS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nüìä CHUNKED FILE PROCESSING FOR REPORTS:\")\n",
    "\n",
    "def process_large_timing_report(file_path, chunk_lines=1000):\n",
    "    \"\"\"Process large timing report in chunks.\"\"\"\n",
    "    path_data = []\n",
    "    current_path = None\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            line_buffer = []\n",
    "\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                line_buffer.append(line.strip())\n",
    "\n",
    "                # Process in chunks\n",
    "                if len(line_buffer) >= chunk_lines:\n",
    "                    # Process this chunk\n",
    "                    chunk_paths = extract_timing_paths_from_chunk(line_buffer)\n",
    "                    path_data.extend(chunk_paths)\n",
    "\n",
    "                    # Clear buffer but keep some overlap for path boundaries\n",
    "                    line_buffer = line_buffer[-50:]  # Keep last 50 lines\n",
    "\n",
    "        # Process remaining lines\n",
    "        if line_buffer:\n",
    "            chunk_paths = extract_timing_paths_from_chunk(line_buffer)\n",
    "            path_data.extend(chunk_paths)\n",
    "\n",
    "        print(f\"     ‚úÖ Processed timing report with {len(path_data)} paths\")\n",
    "        return path_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"     ‚ùå Error processing timing report: {e}\")\n",
    "        return []\n",
    "\n",
    "def extract_timing_paths_from_chunk(lines):\n",
    "    \"\"\"Extract timing paths from a chunk of lines.\"\"\"\n",
    "    paths = []\n",
    "    current_path = None\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('Startpoint:'):\n",
    "            if current_path:\n",
    "                paths.append(current_path)\n",
    "\n",
    "            current_path = {\n",
    "                'startpoint': line.split(':')[1].strip().split()[0],\n",
    "                'endpoint': '',\n",
    "                'slack': 0.0\n",
    "            }\n",
    "        elif line.startswith('Endpoint:') and current_path:\n",
    "            current_path['endpoint'] = line.split(':')[1].strip().split()[0]\n",
    "        elif 'slack' in line and current_path:\n",
    "            slack_match = re.search(r'([\\d.-]+)\\s+slack', line)\n",
    "            if slack_match:\n",
    "                current_path['slack'] = float(slack_match.group(1))\n",
    "\n",
    "    if current_path:\n",
    "        paths.append(current_path)\n",
    "\n",
    "    return paths\n",
    "\n",
    "# Test memory-mapped processing\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    temp_path = Path(temp_dir)\n",
    "    test_file = temp_path / \"test_design.v\"\n",
    "\n",
    "    # Create test file\n",
    "    create_sample_large_netlist(test_file, num_modules=20)\n",
    "\n",
    "    print(\"   Testing memory-mapped processing:\")\n",
    "    mm_stats = process_with_memory_mapping(test_file)\n",
    "\n",
    "print(\"\\nüèÜ LARGE FILE PROCESSING BENEFITS:\")\n",
    "print(\"‚úÖ **Memory Efficiency**: Handle files larger than available RAM\")\n",
    "print(\"‚úÖ **Performance**: Fast processing through streaming and mapping\")\n",
    "print(\"‚úÖ **Scalability**: Process multi-gigabyte VLSI files\")\n",
    "print(\"‚úÖ **Pattern Search**: Efficient searching in large datasets\")\n",
    "print(\"‚úÖ **Chunk Processing**: Break large tasks into manageable pieces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6711cc",
   "metadata": {},
   "source": [
    "## üí™ **Practice Exercises: Advanced File Processing**\n",
    "\n",
    "### **üéØ Exercise 1: Complete EDA Log Analysis System**\n",
    "Create a comprehensive log analysis framework:\n",
    "- Parse logs from multiple EDA tools (synthesis, P&R, timing, power)\n",
    "- Extract and categorize errors, warnings, and performance metrics\n",
    "- Generate trend analysis across multiple design iterations\n",
    "- Create automated alerts for critical issues and quality gates\n",
    "\n",
    "### **üéØ Exercise 2: Large Design File Processor**\n",
    "Build a system for processing massive VLSI design files:\n",
    "- Stream process multi-gigabyte netlist files efficiently\n",
    "- Extract hierarchical design information and statistics\n",
    "- Implement parallel processing for multiple files\n",
    "- Generate comprehensive design analysis reports\n",
    "\n",
    "### **üéØ Exercise 3: Automated Report Aggregation Pipeline**\n",
    "Implement a complete report processing pipeline:\n",
    "- Collect timing, power, and area reports from multiple corners\n",
    "- Aggregate data across process variations and operating conditions\n",
    "- Generate executive summary reports with trend analysis\n",
    "- Create HTML dashboards with interactive charts\n",
    "\n",
    "### **üéØ Exercise 4: Configuration and Environment Manager**\n",
    "Create a robust configuration management system:\n",
    "- Handle complex tool configuration hierarchies\n",
    "- Support environment-specific overrides and templates\n",
    "- Implement configuration validation and dependency checking\n",
    "- Generate configuration diffs and change tracking\n",
    "\n",
    "---\n",
    "\n",
    "## üèÜ **Chapter Summary: Advanced File Processing Mastery**\n",
    "\n",
    "### **‚úÖ Directory and Batch Operations**\n",
    "- **Project Structure**: Automated VLSI project organization\n",
    "- **File Discovery**: Pattern-based searching and filtering\n",
    "- **Batch Processing**: Efficient handling of multiple design files\n",
    "- **Maintenance**: Copying, backup, and cleanup operations\n",
    "\n",
    "### **‚úÖ Log Analysis and Monitoring**\n",
    "- **EDA Tool Logs**: Automated parsing and error extraction\n",
    "- **Pattern Recognition**: Categorization of issues and metrics\n",
    "- **Multi-Log Aggregation**: Cross-tool analysis and reporting\n",
    "- **Quality Monitoring**: Automated error detection and alerting\n",
    "\n",
    "### **‚úÖ Large File Processing**\n",
    "- **Streaming**: Memory-efficient processing of huge files\n",
    "- **Memory Mapping**: Fast access to large datasets\n",
    "- **Chunked Processing**: Breaking large tasks into manageable pieces\n",
    "- **Performance**: Optimized algorithms for VLSI file sizes\n",
    "\n",
    "### **‚úÖ Professional Applications**\n",
    "- **Scalability**: Handle enterprise-scale VLSI projects\n",
    "- **Automation**: Reduce manual file processing tasks\n",
    "- **Integration**: Connect multiple EDA tools seamlessly\n",
    "- **Quality Assurance**: Comprehensive validation and monitoring\n",
    "\n",
    "**üöÄ Next**: Ready for advanced Python topics including testing, debugging, performance optimization, and deployment strategies for production VLSI automation systems!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
