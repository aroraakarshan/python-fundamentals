{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63102c8f",
   "metadata": {},
   "source": [
    "# Chapter 12: File I/O Operations for VLSI Automation üìÅ\n",
    "\n",
    "## üéØ **Learning Objectives**\n",
    "Master Python file operations for professional VLSI automation:\n",
    "\n",
    "### **Core File Operations**\n",
    "- Reading and writing text files (Verilog, SDC, reports)\n",
    "- Binary file handling and data processing\n",
    "- Path manipulation and directory operations\n",
    "- File system navigation and management\n",
    "\n",
    "### **VLSI File Processing**\n",
    "- **Design Files**: Verilog parsing and generation\n",
    "- **Constraint Files**: SDC reading and writing\n",
    "- **Report Processing**: Timing, power, area analysis\n",
    "- **Log Analysis**: Tool output parsing and metrics extraction\n",
    "\n",
    "### **Advanced Techniques**\n",
    "- Error handling and file validation\n",
    "- Large file processing strategies\n",
    "- Data transformation and format conversion\n",
    "- Backup and versioning workflows\n",
    "\n",
    "---\n",
    "\n",
    "## üîß **Why File I/O Matters in VLSI**\n",
    "File operations are fundamental to VLSI automation:\n",
    "- **Tool Integration**: Processing input/output files from EDA tools\n",
    "- **Data Exchange**: Converting between different file formats\n",
    "- **Report Analysis**: Extracting metrics from tool reports\n",
    "- **Script Automation**: Reading configurations and generating outputs\n",
    "- **Quality Control**: Validating design files and catching errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df765f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASIC FILE OPERATIONS FOR VLSI DESIGN\n",
    "# =====================================\n",
    "# Essential file reading and writing techniques\n",
    "\n",
    "print(\"üìÅ BASIC FILE OPERATIONS FOR VLSI DESIGN\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# =============================================================================\n",
    "# TEXT FILE READING AND WRITING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüìù TEXT FILE READING AND WRITING:\")\n",
    "\n",
    "# Sample VLSI file contents\n",
    "verilog_content = \"\"\"module cpu_core (\n",
    "    input wire clk,\n",
    "    input wire reset_n,\n",
    "    input wire [31:0] instruction,\n",
    "    output reg [31:0] result\n",
    ");\n",
    "\n",
    "reg [31:0] register_file [0:31];\n",
    "reg [31:0] pc;\n",
    "\n",
    "always @(posedge clk or negedge reset_n) begin\n",
    "    if (!reset_n) begin\n",
    "        pc <= 32'h0;\n",
    "        result <= 32'h0;\n",
    "    end else begin\n",
    "        case (instruction[31:26])\n",
    "            6'b000001: result <= register_file[instruction[25:21]] + register_file[instruction[20:16]];\n",
    "            6'b000010: result <= register_file[instruction[25:21]] - register_file[instruction[20:16]];\n",
    "            default: result <= 32'h0;\n",
    "        endcase\n",
    "        pc <= pc + 4;\n",
    "    end\n",
    "end\n",
    "\n",
    "endmodule\"\"\"\n",
    "\n",
    "sdc_content = \"\"\"# Timing constraints for cpu_core\n",
    "create_clock -name clk -period 10.0 [get_ports clk]\n",
    "set_input_delay -clock clk -max 2.0 [get_ports instruction]\n",
    "set_input_delay -clock clk -min 0.5 [get_ports instruction]\n",
    "set_output_delay -clock clk -max 3.0 [get_ports result]\n",
    "set_output_delay -clock clk -min 0.5 [get_ports result]\n",
    "set_max_area 1500.0\n",
    "set_max_dynamic_power 0.8\"\"\"\n",
    "\n",
    "# Create temporary directory for examples\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    temp_path = Path(temp_dir)\n",
    "\n",
    "    # =============================================================================\n",
    "    # WRITING FILES - DIFFERENT METHODS\n",
    "    # =============================================================================\n",
    "\n",
    "    print(\"   Writing VLSI files:\")\n",
    "\n",
    "    # Method 1: Basic write with open()\n",
    "    verilog_file = temp_path / \"cpu_core.v\"\n",
    "    with open(verilog_file, 'w') as f:\n",
    "        f.write(verilog_content)\n",
    "    print(f\"     ‚úÖ Verilog file: {verilog_file.name} ({len(verilog_content)} chars)\")\n",
    "\n",
    "    # Method 2: Using Path.write_text()\n",
    "    sdc_file = temp_path / \"constraints.sdc\"\n",
    "    sdc_file.write_text(sdc_content)\n",
    "    print(f\"     ‚úÖ SDC file: {sdc_file.name} ({len(sdc_content.splitlines())} lines)\")\n",
    "\n",
    "    # Method 3: Writing with encoding specification\n",
    "    report_file = temp_path / \"synthesis_report.txt\"\n",
    "    report_content = \"\"\"\n",
    "Synthesis Report for cpu_core\n",
    "============================\n",
    "Area: 1250.5 Œºm¬≤\n",
    "Power: 0.825 W\n",
    "Frequency: 1000 MHz\n",
    "Timing: MET (slack: +0.123ns)\n",
    "    \"\"\"\n",
    "    with open(report_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(report_content.strip())\n",
    "    print(f\"     ‚úÖ Report file: {report_file.name} ({report_file.stat().st_size} bytes)\")\n",
    "\n",
    "    # =============================================================================\n",
    "    # READING FILES - DIFFERENT METHODS\n",
    "    # =============================================================================\n",
    "\n",
    "    print(f\"\\nüìñ READING FILES - DIFFERENT METHODS:\")\n",
    "\n",
    "    # Method 1: Read entire file\n",
    "    print(\"   Method 1 - Read entire file:\")\n",
    "    with open(verilog_file, 'r') as f:\n",
    "        content = f.read()\n",
    "        module_count = content.count('module')\n",
    "        always_count = content.count('always')\n",
    "        print(f\"     Verilog analysis: {module_count} modules, {always_count} always blocks\")\n",
    "\n",
    "    # Method 2: Read line by line\n",
    "    print(\"\\n   Method 2 - Line by line processing:\")\n",
    "    constraint_count = 0\n",
    "    comment_count = 0\n",
    "    with open(sdc_file, 'r') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if line.startswith('#'):\n",
    "                comment_count += 1\n",
    "            elif line and not line.startswith('#'):\n",
    "                constraint_count += 1\n",
    "        print(f\"     SDC analysis: {constraint_count} constraints, {comment_count} comments in {line_num} lines\")\n",
    "\n",
    "    # Method 3: Read all lines into list\n",
    "    print(\"\\n   Method 3 - Read all lines:\")\n",
    "    with open(report_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Process report lines\n",
    "    metrics = {}\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if ':' in line and any(keyword in line for keyword in ['Area', 'Power', 'Frequency', 'Timing']):\n",
    "            parts = line.split(':')\n",
    "            if len(parts) == 2:\n",
    "                key = parts[0].strip()\n",
    "                value = parts[1].strip()\n",
    "                metrics[key] = value\n",
    "\n",
    "    print(f\"     Report metrics extracted:\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"       {key}: {value}\")\n",
    "\n",
    "    # =============================================================================\n",
    "    # FILE INFORMATION AND OPERATIONS\n",
    "    # =============================================================================\n",
    "\n",
    "    print(f\"\\nüìä FILE INFORMATION AND OPERATIONS:\")\n",
    "\n",
    "    for file_path in [verilog_file, sdc_file, report_file]:\n",
    "        stat = file_path.stat()\n",
    "        print(f\"   {file_path.name}:\")\n",
    "        print(f\"     Size: {stat.st_size} bytes\")\n",
    "        print(f\"     Exists: {file_path.exists()}\")\n",
    "        print(f\"     Is file: {file_path.is_file()}\")\n",
    "        print(f\"     Readable: {os.access(file_path, os.R_OK)}\")\n",
    "        print(f\"     Extension: {file_path.suffix}\")\n",
    "\n",
    "print(\"\\nüèÜ BASIC FILE I/O BENEFITS:\")\n",
    "print(\"‚úÖ **Design File Processing**: Read/write Verilog, SDC, reports\")\n",
    "print(\"‚úÖ **Automated Analysis**: Extract metrics from tool outputs\")\n",
    "print(\"‚úÖ **Cross-Platform**: Path operations work on all systems\")\n",
    "print(\"‚úÖ **Flexible Methods**: Multiple approaches for different needs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66d895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVANCED FILE PROCESSING TECHNIQUES\n",
    "# ===================================\n",
    "# CSV, JSON, and structured data handling\n",
    "\n",
    "print(\"üîß ADVANCED FILE PROCESSING TECHNIQUES\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# =============================================================================\n",
    "# CSV PROCESSING FOR DESIGN DATA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüìà CSV PROCESSING FOR DESIGN DATA:\")\n",
    "\n",
    "# Function to write design metrics to CSV\n",
    "def write_design_metrics_csv(filename, design_data):\n",
    "    \"\"\"Write design metrics to CSV file.\"\"\"\n",
    "    with open(filename, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        # Write header\n",
    "        writer.writerow(['Module', 'Area_um2', 'Power_mW', 'Frequency_MHz', 'Utilization', 'Timing_Slack'])\n",
    "        # Write data\n",
    "        for row in design_data:\n",
    "            writer.writerow(row)\n",
    "    return len(design_data)\n",
    "\n",
    "# Function to read and analyze CSV data\n",
    "def read_design_metrics_csv(filename):\n",
    "    \"\"\"Read and analyze design metrics from CSV file.\"\"\"\n",
    "    designs = []\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            design = {\n",
    "                'module': row['Module'],\n",
    "                'area': float(row['Area_um2']),\n",
    "                'power': float(row['Power_mW']),\n",
    "                'frequency': float(row['Frequency_MHz']),\n",
    "                'utilization': float(row['Utilization']),\n",
    "                'slack': float(row['Timing_Slack'])\n",
    "            }\n",
    "            designs.append(design)\n",
    "    return designs\n",
    "\n",
    "# Function to analyze design data\n",
    "def analyze_design_data(designs):\n",
    "    \"\"\"Analyze design metrics and return summary.\"\"\"\n",
    "    total_area = sum(d['area'] for d in designs)\n",
    "    total_power = sum(d['power'] for d in designs)\n",
    "    failed_timing = [d for d in designs if d['slack'] < 0]\n",
    "    avg_utilization = sum(d['utilization'] for d in designs) / len(designs)\n",
    "\n",
    "    return {\n",
    "        'total_modules': len(designs),\n",
    "        'total_area': total_area,\n",
    "        'total_power': total_power,\n",
    "        'failed_timing': len(failed_timing),\n",
    "        'failed_modules': [d['module'] for d in failed_timing],\n",
    "        'avg_utilization': avg_utilization\n",
    "    }\n",
    "\n",
    "# Sample design data\n",
    "design_data = [\n",
    "    ['cpu_core', 1250.5, 825.0, 1000, 0.75, 0.123],\n",
    "    ['memory_ctrl', 890.2, 456.0, 800, 0.68, 0.089],\n",
    "    ['io_interface', 234.8, 125.0, 100, 0.45, 0.567],\n",
    "    ['crypto_unit', 678.9, 234.0, 500, 0.82, -0.045],\n",
    "    ['dsp_block', 445.6, 178.0, 200, 0.59, 0.234]\n",
    "]\n",
    "\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    temp_path = Path(temp_dir)\n",
    "    csv_file = temp_path / \"design_metrics.csv\"\n",
    "\n",
    "    # Write and read CSV\n",
    "    print(\"   CSV processing workflow:\")\n",
    "    rows_written = write_design_metrics_csv(csv_file, design_data)\n",
    "    print(f\"     ‚úÖ Written {rows_written} design records to CSV\")\n",
    "\n",
    "    designs = read_design_metrics_csv(csv_file)\n",
    "    print(f\"     ‚úÖ Read {len(designs)} design records from CSV\")\n",
    "\n",
    "    # Analyze data\n",
    "    analysis = analyze_design_data(designs)\n",
    "    print(f\"\\n   Design analysis results:\")\n",
    "    print(f\"     Total modules: {analysis['total_modules']}\")\n",
    "    print(f\"     Total area: {analysis['total_area']:.1f} Œºm¬≤\")\n",
    "    print(f\"     Total power: {analysis['total_power']:.1f} mW\")\n",
    "    print(f\"     Average utilization: {analysis['avg_utilization']:.1%}\")\n",
    "    print(f\"     Timing violations: {analysis['failed_timing']} modules\")\n",
    "    if analysis['failed_modules']:\n",
    "        print(f\"     Failed modules: {analysis['failed_modules']}\")\n",
    "\n",
    "    # =============================================================================\n",
    "    # JSON PROCESSING FOR CONFIGURATION DATA\n",
    "    # =============================================================================\n",
    "\n",
    "    print(f\"\\nüîß JSON PROCESSING FOR CONFIGURATION DATA:\")\n",
    "\n",
    "    # Function to create tool configuration\n",
    "    def create_tool_config():\n",
    "        \"\"\"Create comprehensive tool configuration.\"\"\"\n",
    "        return {\n",
    "            \"project\": {\n",
    "                \"name\": \"cpu_design\",\n",
    "                \"version\": \"1.0\",\n",
    "                \"technology\": \"28nm\",\n",
    "                \"created\": datetime.now().isoformat()\n",
    "            },\n",
    "            \"synthesis\": {\n",
    "                \"tool\": \"Design Compiler\",\n",
    "                \"version\": \"2023.03\",\n",
    "                \"settings\": {\n",
    "                    \"optimization_level\": \"high\",\n",
    "                    \"area_effort\": \"medium\",\n",
    "                    \"timing_effort\": \"high\",\n",
    "                    \"power_effort\": \"low\"\n",
    "                },\n",
    "                \"libraries\": [\n",
    "                    \"/libs/tsmc28/timing.lib\",\n",
    "                    \"/libs/tsmc28/power.lib\"\n",
    "                ],\n",
    "                \"constraints\": {\n",
    "                    \"max_area\": 1500.0,\n",
    "                    \"target_frequency\": 1000,\n",
    "                    \"max_power\": 1.0\n",
    "                }\n",
    "            },\n",
    "            \"place_route\": {\n",
    "                \"tool\": \"Innovus\",\n",
    "                \"version\": \"21.1\",\n",
    "                \"settings\": {\n",
    "                    \"placement_effort\": \"standard\",\n",
    "                    \"routing_effort\": \"high\",\n",
    "                    \"optimization\": \"timing\"\n",
    "                },\n",
    "                \"technology\": {\n",
    "                    \"process\": \"28nm\",\n",
    "                    \"metal_layers\": 9,\n",
    "                    \"min_width\": 0.09\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    # Function to validate configuration\n",
    "    def validate_tool_config(config):\n",
    "        \"\"\"Validate tool configuration.\"\"\"\n",
    "        errors = []\n",
    "\n",
    "        # Check required fields\n",
    "        if 'project' not in config:\n",
    "            errors.append(\"Missing project information\")\n",
    "\n",
    "        if 'synthesis' in config:\n",
    "            syn_config = config['synthesis']\n",
    "            if 'constraints' in syn_config:\n",
    "                constraints = syn_config['constraints']\n",
    "                if constraints.get('target_frequency', 0) <= 0:\n",
    "                    errors.append(\"Invalid target frequency\")\n",
    "                if constraints.get('max_area', 0) <= 0:\n",
    "                    errors.append(\"Invalid max area constraint\")\n",
    "\n",
    "        return errors\n",
    "\n",
    "    # Function to extract tool summary\n",
    "    def extract_tool_summary(config):\n",
    "        \"\"\"Extract summary information from configuration.\"\"\"\n",
    "        summary = {}\n",
    "\n",
    "        if 'project' in config:\n",
    "            summary['project'] = config['project']['name']\n",
    "            summary['technology'] = config['project']['technology']\n",
    "\n",
    "        for tool_name in ['synthesis', 'place_route']:\n",
    "            if tool_name in config:\n",
    "                tool_config = config[tool_name]\n",
    "                summary[tool_name] = {\n",
    "                    'tool': tool_config.get('tool', 'Unknown'),\n",
    "                    'version': tool_config.get('version', 'Unknown')\n",
    "                }\n",
    "\n",
    "        return summary\n",
    "\n",
    "    json_file = temp_path / \"tool_config.json\"\n",
    "\n",
    "    # Create and save configuration\n",
    "    config = create_tool_config()\n",
    "    with open(json_file, 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    print(f\"     ‚úÖ Configuration saved to {json_file.name}\")\n",
    "\n",
    "    # Read and validate configuration\n",
    "    with open(json_file, 'r') as f:\n",
    "        loaded_config = json.load(f)\n",
    "\n",
    "    errors = validate_tool_config(loaded_config)\n",
    "    if errors:\n",
    "        print(f\"     ‚ùå Configuration errors: {errors}\")\n",
    "    else:\n",
    "        print(f\"     ‚úÖ Configuration validation passed\")\n",
    "\n",
    "    # Extract and display summary\n",
    "    summary = extract_tool_summary(loaded_config)\n",
    "    print(f\"\\n   Configuration summary:\")\n",
    "    print(f\"     Project: {summary.get('project', 'N/A')} ({summary.get('technology', 'N/A')})\")\n",
    "    for tool, info in summary.items():\n",
    "        if tool not in ['project', 'technology']:\n",
    "            print(f\"     {tool.title()}: {info['tool']} v{info['version']}\")\n",
    "\n",
    "print(\"\\nüèÜ ADVANCED FILE PROCESSING BENEFITS:\")\n",
    "print(\"‚úÖ **Structured Data**: CSV/JSON for design metrics and configurations\")\n",
    "print(\"‚úÖ **Data Validation**: Configuration and format checking\")\n",
    "print(\"‚úÖ **Tool Integration**: Standard formats for data exchange\")\n",
    "print(\"‚úÖ **Automation Ready**: Programmatic data processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30201709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VLSI REPORT PARSING AND ANALYSIS\n",
    "# ================================\n",
    "# Extracting metrics from EDA tool reports\n",
    "\n",
    "print(\"üìä VLSI REPORT PARSING AND ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# =============================================================================\n",
    "# TIMING REPORT PARSING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n‚è±Ô∏è TIMING REPORT PARSING:\")\n",
    "\n",
    "# Sample timing report content\n",
    "timing_report_content = \"\"\"\n",
    "****************************************\n",
    "Report : timing\n",
    "        -path full\n",
    "        -delay max\n",
    "        -max_paths 10\n",
    "Design : cpu_core\n",
    "Version: X-2021.09\n",
    "Date   : Sat Sep  7 10:30:00 2025\n",
    "****************************************\n",
    "\n",
    "Startpoint: instruction[0] (input port clocked by clk)\n",
    "Endpoint: result[31] (output port clocked by clk)\n",
    "Path Group: clk\n",
    "Path Type: max\n",
    "\n",
    "  Delay    Time   Description\n",
    "-----------------------------------------\n",
    "   0.00    0.00   clock clk (rise edge)\n",
    "   2.00    2.00   input external delay\n",
    "   0.15    2.15 ^ instruction[0] (in)\n",
    "   1.25    3.40 ^ U15/Y (INVX1)\n",
    "   0.85    4.25 v U23/Y (NAND2X1)\n",
    "   1.35    5.60 ^ U45/Y (NOR2X1)\n",
    "   0.95    6.55 v result[31] (out)\n",
    "   3.00    9.55   output external delay\n",
    "  10.00   10.00   clock clk (rise edge)\n",
    "           0.45   clock uncertainty\n",
    "  10.45   10.45   clock reconvergence pessimism\n",
    "-----------------------------------------\n",
    "  10.45          required time\n",
    "   9.55          arrival time\n",
    "-----------------------------------------\n",
    "   0.90          slack (MET)\n",
    "\n",
    "Startpoint: instruction[8] (input port clocked by clk)\n",
    "Endpoint: result[15] (output port clocked by clk)\n",
    "Path Group: clk\n",
    "Path Type: max\n",
    "\n",
    "  Delay    Time   Description\n",
    "-----------------------------------------\n",
    "   0.00    0.00   clock clk (rise edge)\n",
    "   2.00    2.00   input external delay\n",
    "   0.15    2.15 ^ instruction[8] (in)\n",
    "   1.45    3.60 ^ U25/Y (INVX2)\n",
    "   0.95    4.55 v U33/Y (NAND2X1)\n",
    "   1.15    5.70 ^ U55/Y (NOR2X1)\n",
    "   0.85    6.55 v result[15] (out)\n",
    "   3.00    9.55   output external delay\n",
    "  10.00   10.00   clock clk (rise edge)\n",
    "           0.45   clock uncertainty\n",
    "  10.45   10.45   clock reconvergence pessimism\n",
    "-----------------------------------------\n",
    "  10.45          required time\n",
    "   9.55          arrival time\n",
    "-----------------------------------------\n",
    "   0.90          slack (MET)\n",
    "\"\"\"\n",
    "\n",
    "def parse_timing_report(report_content):\n",
    "    \"\"\"Parse timing report and extract key metrics.\"\"\"\n",
    "    lines = report_content.strip().split('\\n')\n",
    "\n",
    "    # Extract header information\n",
    "    header_info = {}\n",
    "    design_match = re.search(r'Design\\s*:\\s*(\\w+)', report_content)\n",
    "    if design_match:\n",
    "        header_info['design'] = design_match.group(1)\n",
    "\n",
    "    # Extract timing paths\n",
    "    paths = []\n",
    "    current_path = None\n",
    "    in_path_section = False\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Detect start of new path\n",
    "        if line.startswith('Startpoint:'):\n",
    "            if current_path:\n",
    "                paths.append(current_path)\n",
    "            current_path = {\n",
    "                'startpoint': re.search(r'Startpoint:\\s*([^\\s]+)', line).group(1),\n",
    "                'endpoint': '',\n",
    "                'path_group': '',\n",
    "                'path_type': '',\n",
    "                'arrival_time': 0.0,\n",
    "                'required_time': 0.0,\n",
    "                'slack': 0.0,\n",
    "                'status': 'UNKNOWN'\n",
    "            }\n",
    "\n",
    "        elif line.startswith('Endpoint:') and current_path:\n",
    "            current_path['endpoint'] = re.search(r'Endpoint:\\s*([^\\s]+)', line).group(1)\n",
    "\n",
    "        elif line.startswith('Path Group:') and current_path:\n",
    "            current_path['path_group'] = re.search(r'Path Group:\\s*(\\w+)', line).group(1)\n",
    "\n",
    "        elif line.startswith('Path Type:') and current_path:\n",
    "            current_path['path_type'] = re.search(r'Path Type:\\s*(\\w+)', line).group(1)\n",
    "\n",
    "        elif 'arrival time' in line and current_path:\n",
    "            time_match = re.search(r'([\\d.-]+)\\s+arrival time', line)\n",
    "            if time_match:\n",
    "                current_path['arrival_time'] = float(time_match.group(1))\n",
    "\n",
    "        elif 'required time' in line and current_path:\n",
    "            time_match = re.search(r'([\\d.-]+)\\s+required time', line)\n",
    "            if time_match:\n",
    "                current_path['required_time'] = float(time_match.group(1))\n",
    "\n",
    "        elif 'slack' in line and current_path:\n",
    "            slack_match = re.search(r'([\\d.-]+)\\s+slack\\s*\\((\\w+)\\)', line)\n",
    "            if slack_match:\n",
    "                current_path['slack'] = float(slack_match.group(1))\n",
    "                current_path['status'] = slack_match.group(2)\n",
    "\n",
    "    # Add last path\n",
    "    if current_path:\n",
    "        paths.append(current_path)\n",
    "\n",
    "    return {\n",
    "        'header': header_info,\n",
    "        'paths': paths,\n",
    "        'total_paths': len(paths)\n",
    "    }\n",
    "\n",
    "def analyze_timing_paths(timing_data):\n",
    "    \"\"\"Analyze timing paths and generate summary.\"\"\"\n",
    "    paths = timing_data['paths']\n",
    "\n",
    "    if not paths:\n",
    "        return {'error': 'No timing paths found'}\n",
    "\n",
    "    # Calculate statistics\n",
    "    slacks = [path['slack'] for path in paths]\n",
    "    violations = [path for path in paths if path['slack'] < 0]\n",
    "\n",
    "    analysis = {\n",
    "        'total_paths': len(paths),\n",
    "        'violations': len(violations),\n",
    "        'worst_slack': min(slacks) if slacks else 0,\n",
    "        'best_slack': max(slacks) if slacks else 0,\n",
    "        'average_slack': sum(slacks) / len(slacks) if slacks else 0,\n",
    "        'timing_status': 'PASS' if len(violations) == 0 else 'FAIL'\n",
    "    }\n",
    "\n",
    "    # Find worst path\n",
    "    if slacks:\n",
    "        worst_idx = slacks.index(min(slacks))\n",
    "        analysis['worst_path'] = {\n",
    "            'startpoint': paths[worst_idx]['startpoint'],\n",
    "            'endpoint': paths[worst_idx]['endpoint'],\n",
    "            'slack': paths[worst_idx]['slack']\n",
    "        }\n",
    "\n",
    "    return analysis\n",
    "\n",
    "# Test timing report parsing\n",
    "print(\"   Parsing timing report:\")\n",
    "timing_data = parse_timing_report(timing_report_content)\n",
    "print(f\"     ‚úÖ Parsed report for design: {timing_data['header'].get('design', 'Unknown')}\")\n",
    "print(f\"     ‚úÖ Found {timing_data['total_paths']} timing paths\")\n",
    "\n",
    "analysis = analyze_timing_paths(timing_data)\n",
    "print(f\"\\n   Timing analysis results:\")\n",
    "print(f\"     Overall status: {analysis['timing_status']}\")\n",
    "print(f\"     Total violations: {analysis['violations']}\")\n",
    "print(f\"     Worst slack: {analysis['worst_slack']:.3f} ns\")\n",
    "print(f\"     Best slack: {analysis['best_slack']:.3f} ns\")\n",
    "print(f\"     Average slack: {analysis['average_slack']:.3f} ns\")\n",
    "\n",
    "if 'worst_path' in analysis:\n",
    "    worst = analysis['worst_path']\n",
    "    print(f\"     Worst path: {worst['startpoint']} -> {worst['endpoint']} ({worst['slack']:+.3f}ns)\")\n",
    "\n",
    "# =============================================================================\n",
    "# POWER REPORT PARSING\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n‚ö° POWER REPORT PARSING:\")\n",
    "\n",
    "# Sample power report content\n",
    "power_report_content = \"\"\"\n",
    "****************************************\n",
    "Report : power\n",
    "        -analysis_effort low\n",
    "Design : cpu_core\n",
    "Version: X-2021.09\n",
    "Date   : Sat Sep  7 10:30:00 2025\n",
    "****************************************\n",
    "\n",
    "                Internal         Switching           Leakage            Total\n",
    "Power Group    Power            Power               Power              Power    (   %  )  Attrs\n",
    "--------------------------------------------------------------------------------------------------\n",
    "clock_network  2.341e-02        1.234e-03           1.456e-05          2.465e-02 ( 29.89)\n",
    "register       1.567e-02        8.912e-04           2.345e-05          1.659e-02 ( 20.11)\n",
    "combinational  1.234e-02        2.345e-03           3.456e-05          1.472e-02 ( 17.84)\n",
    "memory         8.901e-03        1.567e-03           1.234e-05          1.048e-02 ( 12.71)\n",
    "io_pad         6.789e-03        3.456e-04           5.678e-06          7.141e-03 (  8.66)\n",
    "black_box      4.567e-03        6.789e-04           8.901e-06          5.255e-03 (  6.37)\n",
    "macro          3.456e-03        5.678e-04           1.234e-05          4.037e-03 (  4.89)\n",
    "--------------------------------------------------------------------------------------------------\n",
    "Total          7.456e-02        7.654e-03           1.234e-04          8.245e-02 (100.00)\n",
    "\n",
    "1\n",
    "\"\"\"\n",
    "\n",
    "def parse_power_report(report_content):\n",
    "    \"\"\"Parse power report and extract power breakdown.\"\"\"\n",
    "    lines = report_content.strip().split('\\n')\n",
    "\n",
    "    # Extract header information\n",
    "    header_info = {}\n",
    "    design_match = re.search(r'Design\\s*:\\s*(\\w+)', report_content)\n",
    "    if design_match:\n",
    "        header_info['design'] = design_match.group(1)\n",
    "\n",
    "    # Extract power data\n",
    "    power_groups = []\n",
    "    total_power = {}\n",
    "\n",
    "    # Look for data lines with power values\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Skip header and separator lines\n",
    "        if not line or line.startswith('-') or line.startswith('Power Group') or line.startswith('*'):\n",
    "            continue\n",
    "\n",
    "        # Parse data lines with scientific notation\n",
    "        power_match = re.match(r'(\\w+)\\s+([\\d.e-]+)\\s+([\\d.e-]+)\\s+([\\d.e-]+)\\s+([\\d.e-]+)\\s+\\(\\s*([\\d.]+)\\)', line)\n",
    "        if power_match:\n",
    "            group_name = power_match.group(1)\n",
    "            internal = float(power_match.group(2))\n",
    "            switching = float(power_match.group(3))\n",
    "            leakage = float(power_match.group(4))\n",
    "            total = float(power_match.group(5))\n",
    "            percentage = float(power_match.group(6))\n",
    "\n",
    "            if group_name.lower() == 'total':\n",
    "                total_power = {\n",
    "                    'internal': internal,\n",
    "                    'switching': switching,\n",
    "                    'leakage': leakage,\n",
    "                    'total': total\n",
    "                }\n",
    "            else:\n",
    "                power_groups.append({\n",
    "                    'group': group_name,\n",
    "                    'internal_power': internal,\n",
    "                    'switching_power': switching,\n",
    "                    'leakage_power': leakage,\n",
    "                    'total_power': total,\n",
    "                    'percentage': percentage\n",
    "                })\n",
    "\n",
    "    return {\n",
    "        'header': header_info,\n",
    "        'power_groups': power_groups,\n",
    "        'total_power': total_power\n",
    "    }\n",
    "\n",
    "def analyze_power_breakdown(power_data):\n",
    "    \"\"\"Analyze power breakdown and generate insights.\"\"\"\n",
    "    groups = power_data['power_groups']\n",
    "    total = power_data['total_power']\n",
    "\n",
    "    if not groups or not total:\n",
    "        return {'error': 'No power data found'}\n",
    "\n",
    "    # Find top power consumers\n",
    "    sorted_groups = sorted(groups, key=lambda x: x['total_power'], reverse=True)\n",
    "\n",
    "    # Calculate power type percentages\n",
    "    total_power_val = total['total']\n",
    "    internal_pct = (total['internal'] / total_power_val * 100) if total_power_val > 0 else 0\n",
    "    switching_pct = (total['switching'] / total_power_val * 100) if total_power_val > 0 else 0\n",
    "    leakage_pct = (total['leakage'] / total_power_val * 100) if total_power_val > 0 else 0\n",
    "\n",
    "    analysis = {\n",
    "        'total_power_mw': total_power_val * 1000,  # Convert to mW\n",
    "        'power_breakdown': {\n",
    "            'internal': {'value': total['internal'] * 1000, 'percentage': internal_pct},\n",
    "            'switching': {'value': total['switching'] * 1000, 'percentage': switching_pct},\n",
    "            'leakage': {'value': total['leakage'] * 1000, 'percentage': leakage_pct}\n",
    "        },\n",
    "        'top_consumers': sorted_groups[:3],\n",
    "        'group_count': len(groups)\n",
    "    }\n",
    "\n",
    "    return analysis\n",
    "\n",
    "# Test power report parsing\n",
    "print(\"   Parsing power report:\")\n",
    "power_data = parse_power_report(power_report_content)\n",
    "print(f\"     ‚úÖ Parsed report for design: {power_data['header'].get('design', 'Unknown')}\")\n",
    "print(f\"     ‚úÖ Found {len(power_data['power_groups'])} power groups\")\n",
    "\n",
    "power_analysis = analyze_power_breakdown(power_data)\n",
    "if 'error' not in power_analysis:\n",
    "    print(f\"\\n   Power analysis results:\")\n",
    "    print(f\"     Total power: {power_analysis['total_power_mw']:.3f} mW\")\n",
    "\n",
    "    breakdown = power_analysis['power_breakdown']\n",
    "    print(f\"     Power breakdown:\")\n",
    "    print(f\"       Internal: {breakdown['internal']['value']:.3f} mW ({breakdown['internal']['percentage']:.1f}%)\")\n",
    "    print(f\"       Switching: {breakdown['switching']['value']:.3f} mW ({breakdown['switching']['percentage']:.1f}%)\")\n",
    "    print(f\"       Leakage: {breakdown['leakage']['value']:.3f} mW ({breakdown['leakage']['percentage']:.1f}%)\")\n",
    "\n",
    "    print(f\"\\n     Top power consumers:\")\n",
    "    for i, group in enumerate(power_analysis['top_consumers'], 1):\n",
    "        print(f\"       {i}. {group['group']}: {group['total_power']*1000:.3f} mW ({group['percentage']:.1f}%)\")\n",
    "\n",
    "print(\"\\nüèÜ VLSI REPORT PARSING BENEFITS:\")\n",
    "print(\"‚úÖ **Automated Metrics**: Extract key data from tool reports\")\n",
    "print(\"‚úÖ **Trend Analysis**: Track metrics across design iterations\")\n",
    "print(\"‚úÖ **Quality Gates**: Automated pass/fail checking\")\n",
    "print(\"‚úÖ **Dashboard Ready**: Data suitable for visualization tools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7f8533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERROR HANDLING AND ROBUST FILE OPERATIONS\n",
    "# ==========================================\n",
    "# Professional file handling with error management\n",
    "\n",
    "print(\"üõ°Ô∏è ERROR HANDLING AND ROBUST FILE OPERATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# =============================================================================\n",
    "# SAFE FILE OPERATIONS WITH ERROR HANDLING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüîí SAFE FILE OPERATIONS WITH ERROR HANDLING:\")\n",
    "\n",
    "def safe_file_write(file_path, content, create_backup=True):\n",
    "    \"\"\"Safely write file with optional backup and error handling.\"\"\"\n",
    "    try:\n",
    "        file_path = Path(file_path)\n",
    "\n",
    "        # Create parent directories if needed\n",
    "        file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Create backup if file exists\n",
    "        if create_backup and file_path.exists():\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            backup_path = file_path.with_suffix(f\".backup_{timestamp}{file_path.suffix}\")\n",
    "            shutil.copy2(file_path, backup_path)\n",
    "            print(f\"     üìã Backup created: {backup_path.name}\")\n",
    "\n",
    "        # Write new content\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "\n",
    "        print(f\"     ‚úÖ File written: {file_path.name} ({len(content)} chars)\")\n",
    "        return True, None\n",
    "\n",
    "    except PermissionError as e:\n",
    "        error_msg = f\"Permission denied: {file_path}\"\n",
    "        print(f\"     ‚ùå {error_msg}\")\n",
    "        return False, error_msg\n",
    "    except OSError as e:\n",
    "        error_msg = f\"OS error writing {file_path}: {e}\"\n",
    "        print(f\"     ‚ùå {error_msg}\")\n",
    "        return False, error_msg\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Unexpected error: {e}\"\n",
    "        print(f\"     ‚ùå {error_msg}\")\n",
    "        return False, error_msg\n",
    "\n",
    "def safe_file_read(file_path, encoding='utf-8'):\n",
    "    \"\"\"Safely read file with comprehensive error handling.\"\"\"\n",
    "    try:\n",
    "        file_path = Path(file_path)\n",
    "\n",
    "        # Check if file exists\n",
    "        if not file_path.exists():\n",
    "            error_msg = f\"File not found: {file_path}\"\n",
    "            print(f\"     ‚ùå {error_msg}\")\n",
    "            return None, error_msg\n",
    "\n",
    "        # Check if file is readable\n",
    "        if not os.access(file_path, os.R_OK):\n",
    "            error_msg = f\"File not readable: {file_path}\"\n",
    "            print(f\"     ‚ùå {error_msg}\")\n",
    "            return None, error_msg\n",
    "\n",
    "        # Read file content\n",
    "        with open(file_path, 'r', encoding=encoding) as f:\n",
    "            content = f.read()\n",
    "\n",
    "        print(f\"     ‚úÖ File read: {file_path.name} ({len(content)} chars)\")\n",
    "        return content, None\n",
    "\n",
    "    except UnicodeDecodeError as e:\n",
    "        error_msg = f\"Encoding error reading {file_path}: {e}\"\n",
    "        print(f\"     ‚ùå {error_msg}\")\n",
    "        return None, error_msg\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error reading {file_path}: {e}\"\n",
    "        print(f\"     ‚ùå {error_msg}\")\n",
    "        return None, error_msg\n",
    "\n",
    "def validate_verilog_content(content):\n",
    "    \"\"\"Basic Verilog syntax validation.\"\"\"\n",
    "    errors = []\n",
    "    lines = content.split('\\n')\n",
    "\n",
    "    # Check module/endmodule balance\n",
    "    module_count = content.count('module')\n",
    "    endmodule_count = content.count('endmodule')\n",
    "\n",
    "    if module_count != endmodule_count:\n",
    "        errors.append(f\"Module/endmodule mismatch: {module_count} vs {endmodule_count}\")\n",
    "\n",
    "    # Check for basic syntax issues\n",
    "    for i, line in enumerate(lines, 1):\n",
    "        line_stripped = line.strip()\n",
    "        if line_stripped and not line_stripped.startswith('//'):\n",
    "            # Check for missing semicolons (simplified check)\n",
    "            if any(keyword in line_stripped for keyword in ['reg ', 'wire ', 'input ', 'output ']):\n",
    "                if not line_stripped.endswith(';') and not line_stripped.endswith(','):\n",
    "                    if not any(char in line_stripped for char in ['(', ')', '[', ']']):\n",
    "                        errors.append(f\"Line {i}: Possible missing semicolon\")\n",
    "\n",
    "    return errors\n",
    "\n",
    "def validate_sdc_content(content):\n",
    "    \"\"\"Basic SDC syntax validation.\"\"\"\n",
    "    errors = []\n",
    "    lines = content.split('\\n')\n",
    "\n",
    "    # Check for valid SDC commands\n",
    "    valid_commands = [\n",
    "        'create_clock', 'create_generated_clock', 'set_input_delay', 'set_output_delay',\n",
    "        'set_max_delay', 'set_min_delay', 'set_max_area', 'set_max_power',\n",
    "        'set_clock_uncertainty', 'set_clock_latency'\n",
    "    ]\n",
    "\n",
    "    for i, line in enumerate(lines, 1):\n",
    "        line_stripped = line.strip()\n",
    "        if line_stripped and not line_stripped.startswith('#'):\n",
    "            # Check if line starts with valid command\n",
    "            if not any(line_stripped.startswith(cmd) for cmd in valid_commands):\n",
    "                # Allow some common patterns\n",
    "                if not any(pattern in line_stripped for pattern in ['set_', 'get_', '[', ']']):\n",
    "                    errors.append(f\"Line {i}: Unknown SDC command pattern\")\n",
    "\n",
    "    return errors\n",
    "\n",
    "# Test safe file operations\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    temp_path = Path(temp_dir)\n",
    "\n",
    "    print(\"   Testing safe file operations:\")\n",
    "\n",
    "    # Test successful write and read\n",
    "    test_verilog = \"\"\"module test_counter (\n",
    "    input wire clk,\n",
    "    input wire reset_n,\n",
    "    output reg [7:0] count\n",
    ");\n",
    "\n",
    "always @(posedge clk or negedge reset_n) begin\n",
    "    if (!reset_n)\n",
    "        count <= 8'h00;\n",
    "    else\n",
    "        count <= count + 1;\n",
    "end\n",
    "\n",
    "endmodule\"\"\"\n",
    "\n",
    "    verilog_file = temp_path / \"test_counter.v\"\n",
    "    success, error = safe_file_write(verilog_file, test_verilog)\n",
    "\n",
    "    if success:\n",
    "        content, error = safe_file_read(verilog_file)\n",
    "        if content:\n",
    "            # Validate content\n",
    "            verilog_errors = validate_verilog_content(content)\n",
    "            if verilog_errors:\n",
    "                print(f\"     ‚ö†Ô∏è  Verilog validation errors: {verilog_errors}\")\n",
    "            else:\n",
    "                print(f\"     ‚úÖ Verilog validation passed\")\n",
    "\n",
    "        # Test backup functionality\n",
    "        modified_verilog = test_verilog.replace(\"count + 1\", \"count + 2\")\n",
    "        safe_file_write(verilog_file, modified_verilog, create_backup=True)\n",
    "\n",
    "    # Test error handling - try to read non-existent file\n",
    "    safe_file_read(temp_path / \"nonexistent.v\")\n",
    "\n",
    "# =============================================================================\n",
    "# FILE PROCESSING WITH LOGGING\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nüìù FILE PROCESSING WITH LOGGING:\")\n",
    "\n",
    "def setup_file_logger(name, log_file=None):\n",
    "    \"\"\"Set up logger for file operations.\"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Remove existing handlers\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)\n",
    "\n",
    "    # Create formatter\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    # Console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "    # File handler if specified\n",
    "    if log_file:\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setFormatter(formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "def process_design_files_with_logging(file_list, output_dir):\n",
    "    \"\"\"Process multiple design files with comprehensive logging.\"\"\"\n",
    "    logger = setup_file_logger(\"VLSIFileProcessor\")\n",
    "\n",
    "    logger.info(f\"Starting batch processing of {len(file_list)} files\")\n",
    "\n",
    "    results = {\n",
    "        'processed': 0,\n",
    "        'errors': 0,\n",
    "        'warnings': 0,\n",
    "        'files': []\n",
    "    }\n",
    "\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for file_info in file_list:\n",
    "        file_name = file_info['name']\n",
    "        file_content = file_info['content']\n",
    "        file_type = file_info['type']\n",
    "\n",
    "        logger.info(f\"Processing {file_type} file: {file_name}\")\n",
    "\n",
    "        try:\n",
    "            # Validate content based on type\n",
    "            validation_errors = []\n",
    "            if file_type == 'verilog':\n",
    "                validation_errors = validate_verilog_content(file_content)\n",
    "            elif file_type == 'sdc':\n",
    "                validation_errors = validate_sdc_content(file_content)\n",
    "\n",
    "            if validation_errors:\n",
    "                logger.warning(f\"Validation warnings for {file_name}: {validation_errors}\")\n",
    "                results['warnings'] += len(validation_errors)\n",
    "\n",
    "            # Write file\n",
    "            output_file = output_path / file_name\n",
    "            success, error = safe_file_write(output_file, file_content, create_backup=False)\n",
    "\n",
    "            if success:\n",
    "                logger.info(f\"Successfully processed {file_name}\")\n",
    "                results['processed'] += 1\n",
    "                results['files'].append({\n",
    "                    'name': file_name,\n",
    "                    'status': 'success',\n",
    "                    'size': len(file_content),\n",
    "                    'warnings': len(validation_errors)\n",
    "                })\n",
    "            else:\n",
    "                logger.error(f\"Failed to process {file_name}: {error}\")\n",
    "                results['errors'] += 1\n",
    "                results['files'].append({\n",
    "                    'name': file_name,\n",
    "                    'status': 'error',\n",
    "                    'error': error\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error processing {file_name}: {e}\")\n",
    "            results['errors'] += 1\n",
    "\n",
    "    logger.info(f\"Batch processing complete: {results['processed']} success, {results['errors']} errors, {results['warnings']} warnings\")\n",
    "    return results\n",
    "\n",
    "# Test file processing with logging\n",
    "test_files = [\n",
    "    {\n",
    "        'name': 'cpu.v',\n",
    "        'type': 'verilog',\n",
    "        'content': 'module cpu(); endmodule'\n",
    "    },\n",
    "    {\n",
    "        'name': 'memory.v',\n",
    "        'type': 'verilog',\n",
    "        'content': 'module memory(); reg [31:0] data; endmodule'\n",
    "    },\n",
    "    {\n",
    "        'name': 'constraints.sdc',\n",
    "        'type': 'sdc',\n",
    "        'content': 'create_clock -period 10 clk\\nset_input_delay 2 data'\n",
    "    }\n",
    "]\n",
    "\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    print(\"   Processing files with logging:\")\n",
    "    results = process_design_files_with_logging(test_files, temp_dir)\n",
    "\n",
    "    print(f\"\\n   Processing summary:\")\n",
    "    print(f\"     Successful: {results['processed']}\")\n",
    "    print(f\"     Errors: {results['errors']}\")\n",
    "    print(f\"     Warnings: {results['warnings']}\")\n",
    "\n",
    "print(\"\\nüèÜ ROBUST FILE OPERATION BENEFITS:\")\n",
    "print(\"‚úÖ **Error Recovery**: Graceful handling of file system errors\")\n",
    "print(\"‚úÖ **Data Safety**: Automatic backups and validation\")\n",
    "print(\"‚úÖ **Audit Trail**: Comprehensive logging of operations\")\n",
    "print(\"‚úÖ **Production Ready**: Enterprise-grade file handling\")\n",
    "print(\"‚úÖ **Quality Assurance**: Built-in validation and error checking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e7d6f9",
   "metadata": {},
   "source": [
    "## üí™ **Practice Exercises: File I/O Mastery**\n",
    "\n",
    "### **üéØ Exercise 1: VLSI File Format Converter**\n",
    "Create a comprehensive file format conversion system:\n",
    "- Convert between Verilog and SPICE netlists\n",
    "- Transform SDC constraints to different tool formats\n",
    "- Handle different encoding and line ending formats\n",
    "- Validate converted files for syntax correctness\n",
    "\n",
    "### **üéØ Exercise 2: EDA Tool Log Analyzer**\n",
    "Build a robust log analysis system:\n",
    "- Parse multiple EDA tool log formats (synthesis, P&R, timing)\n",
    "- Extract errors, warnings, and performance metrics\n",
    "- Generate trend analysis from historical log data\n",
    "- Create automated alerts for critical issues\n",
    "\n",
    "### **üéØ Exercise 3: Design Metrics Dashboard Generator**\n",
    "Implement a metrics processing and reporting system:\n",
    "- Read timing, power, and area reports from multiple tools\n",
    "- Aggregate data across different design corners\n",
    "- Generate HTML/CSV reports with charts and tables\n",
    "- Track metrics over design iterations\n",
    "\n",
    "### **üéØ Exercise 4: Configuration Management Framework**\n",
    "Create a robust configuration handling system:\n",
    "- Validate JSON/YAML configuration files against schemas\n",
    "- Support configuration inheritance and environment overrides\n",
    "- Implement version control for configuration changes\n",
    "- Generate configuration diffs and change reports\n",
    "\n",
    "---\n",
    "\n",
    "## üèÜ **Chapter Summary: File I/O Mastery Achieved**\n",
    "\n",
    "### **‚úÖ File Operation Fundamentals**\n",
    "- **Text Processing**: Reading/writing Verilog, SDC, and report files\n",
    "- **Structured Data**: CSV and JSON for metrics and configurations\n",
    "- **Path Operations**: Cross-platform file and directory handling\n",
    "- **File Validation**: Content checking and syntax validation\n",
    "\n",
    "### **‚úÖ Advanced Processing Techniques**\n",
    "- **Report Parsing**: Automated extraction from timing and power reports\n",
    "- **Error Handling**: Robust operations with comprehensive error management\n",
    "- **Data Transformation**: Format conversion and content manipulation\n",
    "- **Logging**: Professional audit trails and debugging support\n",
    "\n",
    "### **‚úÖ VLSI-Specific Applications**\n",
    "- **Design Files**: Verilog module parsing and generation\n",
    "- **Tool Integration**: Processing EDA tool inputs and outputs\n",
    "- **Metrics Extraction**: Automated analysis of tool reports\n",
    "- **Configuration Management**: Tool settings and design parameters\n",
    "\n",
    "### **‚úÖ Professional Features**\n",
    "- **Backup Systems**: Automatic versioning and data safety\n",
    "- **Validation**: Content verification and syntax checking\n",
    "- **Performance**: Efficient processing of large files\n",
    "- **Maintainability**: Clean, organized code with proper error handling\n",
    "\n",
    "**üöÄ Next**: Ready for Chapter 14: Advanced File Processing for Large-Scale VLSI Automation!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
